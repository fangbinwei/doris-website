"use strict";(self.webpackChunkselectdb_portal=self.webpackChunkselectdb_portal||[]).push([[94282],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>N});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=d(a),u=r,N=c["".concat(s,".").concat(u)]||c[u]||m[u]||i;return a?n.createElement(N,o(o({ref:t},p),{},{components:a})):n.createElement(N,o({ref:t},p))}));function N(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:r,o[1]=l;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},27324:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var n=a(87462),r=(a(67294),a(3905));const i={title:"Doris On Hive",language:"en"},o=void 0,l={unversionedId:"ecosystem/external-table/hive-of-doris",id:"ecosystem/external-table/hive-of-doris",title:"Doris On Hive",description:"\x3c!--",source:"@site/docs/ecosystem/external-table/hive-of-doris.md",sourceDirName:"ecosystem/external-table",slug:"/ecosystem/external-table/hive-of-doris",permalink:"/docs/ecosystem/external-table/hive-of-doris",draft:!1,tags:[],version:"current",frontMatter:{title:"Doris On Hive",language:"en"},sidebar:"docs",previous:{title:"Doris On ODBC",permalink:"/docs/ecosystem/external-table/odbc-of-doris"},next:{title:"Doris On Iceberg",permalink:"/docs/ecosystem/external-table/iceberg-of-doris"}},s={},d=[{value:"Glossary",id:"glossary",level:2},{value:"Noun in Doris",id:"noun-in-doris",level:3},{value:"How To Use",id:"how-to-use",level:2},{value:"Create Hive External Table",id:"create-hive-external-table",level:3},{value:"Parameter Description",id:"parameter-description",level:4},{value:"Data Type Matching",id:"data-type-matching",level:2},{value:"Query Usage",id:"query-usage",level:3}],p={toc:d};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"hive-external-table-of-doris"},"Hive External Table of Doris"),(0,r.kt)("p",null,"Hive External Table of Doris provides Doris with direct access to Hive external tables, which eliminates the need for cumbersome data import and solves the problem of analyzing Hive tables with the help of Doris' OLAP capabilities: "),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"support for Hive data sources to access Doris"),(0,r.kt)("li",{parentName:"ol"},"Support joint queries between Doris and Hive data sources to perform more complex analysis operations"),(0,r.kt)("li",{parentName:"ol"},"Support access to kerberos-enabled Hive data sources")),(0,r.kt)("p",null,"This document introduces how to use this feature and the considerations."),(0,r.kt)("h2",{id:"glossary"},"Glossary"),(0,r.kt)("h3",{id:"noun-in-doris"},"Noun in Doris"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"FE: Frontend, the front-end node of Doris, responsible for metadata management and request access."),(0,r.kt)("li",{parentName:"ul"},"BE: Backend, the backend node of Doris, responsible for query execution and data storage")),(0,r.kt)("h2",{id:"how-to-use"},"How To Use"),(0,r.kt)("h3",{id:"create-hive-external-table"},"Create Hive External Table"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"-- Syntax\nCREATE [EXTERNAL] TABLE table_name (\n  col_name col_type [NULL | NOT NULL] [COMMENT \"comment\"]\n) ENGINE=HIVE\n[COMMENT \"comment\"] )\nPROPERTIES (\n  'property_name'='property_value',\n  ...\n);\n\n-- Example 1: Create the hive_table table under hive_db in a Hive cluster\nCREATE TABLE `t_hive` (\n  `k1` int NOT NULL COMMENT \"\",\n  `k2` char(10) NOT NULL COMMENT \"\",\n  `k3` datetime NOT NULL COMMENT \"\",\n  `k5` varchar(20) NOT NULL COMMENT \"\",\n  `k6` double NOT NULL COMMENT \"\"\n) ENGINE=HIVE\nCOMMENT \"HIVE\"\nPROPERTIES (\n'hive.metastore.uris' = 'thrift://192.168.0.1:9083',\n'database' = 'hive_db',\n'table' = 'hive_table'\n);\n\n-- Example 2: Create the hive_table table under hive_db in a Hive cluster with HDFS HA configuration.\nCREATE TABLE `t_hive` (\n  `k1` int NOT NULL COMMENT \"\",\n  `k2` char(10) NOT NULL COMMENT \"\",\n  `k3` datetime NOT NULL COMMENT \"\",\n  `k5` varchar(20) NOT NULL COMMENT \"\",\n  `k6` double NOT NULL COMMENT \"\"\n) ENGINE=HIVE\nCOMMENT \"HIVE\"\nPROPERTIES (\n'hive.metastore.uris' = 'thrift://192.168.0.1:9083',\n'database' = 'hive_db',\n'table' = 'hive_table',\n'dfs.nameservices'='hacluster',\n'dfs.ha.namenodes.hacluster'='n1,n2',\n'dfs.namenode.rpc-address.hacluster.n1'='192.168.0.1:8020',\n'dfs.namenode.rpc-address.hacluster.n2'='192.168.0.2:8020',\n'dfs.client.failover.proxy.provider.hacluster'='org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider'\n);\n\n-- Example 3: Create the hive external table under hive_db in Hive cluster with HDFS HA and enable kerberos authentication. \nCREATE TABLE `t_hive` (\n  `k1` int NOT NULL COMMENT \"\",\n  `k2` char(10) NOT NULL COMMENT \"\",\n  `k3` datetime NOT NULL COMMENT \"\",\n  `k5` varchar(20) NOT NULL COMMENT \"\",\n  `k6` double NOT NULL COMMENT \"\"\n) ENGINE=HIVE\nCOMMENT \"HIVE\"\nPROPERTIES (\n'hive.metastore.uris' = 'thrift://192.168.0.1:9083',\n'database' = 'hive_db',\n'table' = 'hive_table',\n'dfs.nameservices'='hacluster',\n'dfs.ha.namenodes.hacluster'='n1,n2',\n'dfs.namenode.rpc-address.hacluster.n1'='192.168.0.1:8020',\n'dfs.namenode.rpc-address.hacluster.n2'='192.168.0.2:8020',\n'dfs.client.failover.proxy.provider.hacluster'='org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider',\n'hadoop.security.authentication'='kerberos',\n'dfs.namenode.kerberos.principal'='hadoop/_HOST@REALM.COM'\n'hadoop.kerberos.principal'='doris_test@REALM.COM',\n'hadoop.kerberos.keytab'='/path/to/doris_test.keytab'\n);\n\n-- Example 4: Create the hive_table table under hive_db in a Hive cluster with data stored on S3\nCREATE TABLE `t_hive` (\n  `k1` int NOT NULL COMMENT \"\",\n  `k2` char(10) NOT NULL COMMENT \"\",\n  `k3` datetime NOT NULL COMMENT \"\",\n  `k5` varchar(20) NOT NULL COMMENT \"\",\n  `k6` double NOT NULL COMMENT \"\"\n) ENGINE=HIVE\nCOMMENT \"HIVE\"\nPROPERTIES (\n'hive.metastore.uris' = 'thrift://192.168.0.1:9083',\n'database' = 'hive_db',\n'table' = 'hive_table',\n'AWS_ACCESS_KEY'='your_access_key',\n'AWS_SECRET_KEY'='your_secret_key',\n'AWS_ENDPOINT'='s3.us-east-1.amazonaws.com',\n'AWS_REGION'='us-east-1'\n);\n\n")),(0,r.kt)("h4",{id:"parameter-description"},"Parameter Description"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"External Table Columns",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Column names should correspond to the Hive table"),(0,r.kt)("li",{parentName:"ul"},"The order of the columns should be the same as the Hive table"),(0,r.kt)("li",{parentName:"ul"},"Must contain all the columns in the Hive table"),(0,r.kt)("li",{parentName:"ul"},"Hive table partition columns do not need to be specified, they can be defined as normal columns."))),(0,r.kt)("li",{parentName:"ul"},"ENGINE should be specified as HIVE"),(0,r.kt)("li",{parentName:"ul"},"PROPERTIES attribute.",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hive.metastore.uris"),": Hive Metastore service address"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"database"),": the name of the database to which Hive is mounted"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"table"),": the name of the table to which Hive is mounted"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dfs.nameservices"),": the logical name for this new nameservice. See hdfs-site.xml"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dfs.ha.namenodes.[nameservice ID]"),"\uff1aunique identifiers for each NameNode in the nameservice. See hdfs-site.xml"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dfs.namenode.rpc-address.[nameservice ID].[name node ID]"),"\uff1athe fully-qualified RPC address for each NameNode to listen on. See hdfs-site.xml"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dfs.client.failover.proxy.provider.[nameservice ID]"),"\uff1athe Java class that HDFS clients use to contact the Active NameNode, usually it is org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"))),(0,r.kt)("li",{parentName:"ul"},"For a kerberos enabled Hive datasource, additional properties need to be set:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"dfs.namenode.kerberos.principal"),": HDFS namenode service principal"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hadoop.security.authentication"),": HDFS authentication type please set kerberos, default simple"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hadoop.kerberos.principal"),": The Kerberos pincipal that Doris will use when connectiong to HDFS."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hadoop.kerberos.keytab"),": HDFS client keytab location."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_ACCESS_KEY"),": AWS access key id."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_SECRET_KEY"),": AWS secret access key."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_ENDPOINT"),": S3 endpoint. e.g. s3.us-east-1.amazonaws.com"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_REGION"),": AWS region. e.g. us-east-1")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"To enable Doris to access the hadoop cluster with kerberos authentication enabled, you need to deploy the Kerberos client kinit on the Doris all FE and BE nodes, configure krb5.conf, and fill in the KDC service information."),(0,r.kt)("li",{parentName:"ul"},"The value of the PROPERTIES property ",(0,r.kt)("inlineCode",{parentName:"li"},"hadoop.kerberos.keytab")," needs to specify the absolute path of the keytab local file and allow the Doris process to access it.")),(0,r.kt)("h2",{id:"data-type-matching"},"Data Type Matching"),(0,r.kt)("p",null,"The supported Hive column types correspond to Doris in the following table."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"center"},"Hive"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Doris"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"BOOLEAN"),(0,r.kt)("td",{parentName:"tr",align:"center"},"BOOLEAN"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"CHAR"),(0,r.kt)("td",{parentName:"tr",align:"center"},"CHAR"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Only UTF8 encoding is supported")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"VARCHAR"),(0,r.kt)("td",{parentName:"tr",align:"center"},"VARCHAR"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Only UTF8 encoding is supported")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"TINYINT"),(0,r.kt)("td",{parentName:"tr",align:"center"},"TINYINT"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"SMALLINT"),(0,r.kt)("td",{parentName:"tr",align:"center"},"SMALLINT"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"INT"),(0,r.kt)("td",{parentName:"tr",align:"center"},"INT"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"BIGINT"),(0,r.kt)("td",{parentName:"tr",align:"center"},"BIGINT"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"FLOAT"),(0,r.kt)("td",{parentName:"tr",align:"center"},"FLOAT"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"DOUBLE"),(0,r.kt)("td",{parentName:"tr",align:"center"},"DOUBLE"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"DECIMAL"),(0,r.kt)("td",{parentName:"tr",align:"center"},"DECIMAL"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"DATE"),(0,r.kt)("td",{parentName:"tr",align:"center"},"DATE"),(0,r.kt)("td",{parentName:"tr",align:"center"})),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"center"},"TIMESTAMP"),(0,r.kt)("td",{parentName:"tr",align:"center"},"DATETIME"),(0,r.kt)("td",{parentName:"tr",align:"center"},"Timestamp to Datetime will lose precision")))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:")," "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Hive table Schema changes ",(0,r.kt)("strong",{parentName:"li"},"are not automatically synchronized")," and require rebuilding the Hive external table in Doris."),(0,r.kt)("li",{parentName:"ul"},"The current Hive storage format only supports Text, Parquet and ORC types"),(0,r.kt)("li",{parentName:"ul"},"The Hive version currently supported by default is ",(0,r.kt)("inlineCode",{parentName:"li"},"2.3.7\u30013.1.2"),", which has not been tested in other versions. More versions will be supported in the future.")),(0,r.kt)("h3",{id:"query-usage"},"Query Usage"),(0,r.kt)("p",null,"After you finish building the Hive external table in Doris, it is no different from a normal Doris OLAP table except that you cannot use the data model in Doris (rollup, preaggregation, materialized view, etc.)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"select * from t_hive where k1 > 1000 and k3 = 'term' or k4 like '%doris';\n")))}c.isMDXComponent=!0}}]);