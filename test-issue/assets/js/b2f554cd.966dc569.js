"use strict";(self.webpackChunkselectdb_portal=self.webpackChunkselectdb_portal||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/LY","metadata":{"permalink":"/blog/LY","source":"@site/blog/LY.md","title":"Best Practice: The Efficiency of the Data Warehouse Greatly Improved in LY Digital","description":"\x3c!--","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Xing Wang"}],"frontMatter":{"title":"Best Practice: The Efficiency of the Data Warehouse Greatly Improved in LY Digital","language":"en","summary":"Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. ","date":"2022-12-19","author":"Xing Wang","tags":["Best Practice"]},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.5","permalink":"/blog/release-1.1.5"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Guide: Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. I hope you like it.\\n\\n> Author: XingWang, Lead Developer of LY Digital\\n\\n![kv](/images/LY/en/kv.png)\\n\\n# 1. Background\\n\\n## 1.1 About LY Digital\\n\\nLY Digital is a tourism financial service platform under LY. Com. Formally established in 2015, LY Digital takes \\"Digital technology empowers the tourism industry.\\" as its vision.\\nAt present, LY Digital\'s business covers financial services, consumer financial services, financial technology and digital technology. So far, more than 10 million users and 76 cities have enjoyed our services.\\n\\n## 1.2 Requirements for Data Warehouse\\n\\n- Dashboard: Needs dashboard for T+1 business, etc.\\n- Early Warning System: Needs risk control, anomaly capital management and traffic monitoring, etc.\\n- Business Analysis: Needs timely data query analysis and temporary data retrieval, etc.\\n- Finance: Needs liquidation and payment reconciliation.\\n\\n# 2. Previous Data Warehouse\\n\\n## 2.1 Architecture\\n\\n![page_1](/images/LY/en/page_1.png)\\n\\nOur previous data warehouse adopted the combination of SteamSets and Apache Kudu, which was very popular in the past few years. In this architecture, Binlog is ingested into Apache Kudu after passing through StreamSets in real-time, and is finally queried and used through Apache Impala and visualization tools.\\n\\n### 2.1.2 Downside\\n\\n- The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs. \\n- The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs.\\n- Apache Kudu\'s performance in wide tables Join is not so good.\\n- SLA is not fully guaranteed because tenant isolation is not provided.\\n- Although SteamSets are equipped with early warning capabilities, job recovery capabilities are still poor. When configuring multiple tasks, the JVM consumes a lot, resulting in slow recovery.\\n\\n# 3. New Data Warehouse\\n\\n## 3.1 Research of Popular Data Warehouses\\n\\nDue to so many shortcomings, we had to give up the previous data warehouse. In 2020, we conducted an in-depth research on the popular data warehouses in the market.\\n\\nDuring the research, we focused on comparing Clickhouse and Apache Doris. ClickHouse has a high utilization rate of CPU, so it performs well in single-table query. But it does not perform well in multitable Joins and high QPS. On the other hand, Doris can not only support thousands of QPS per node. Thanks to the function of partitioning, it can also support high-concurrency queries at the QPS level of 10,000. Moreover, the horiziontal scaling in and out of ClickHouse are complex, which cannot be done automatically at present. Doris supports online dynamic scaling, and can be expanded horizontally according to the development of the business.\\n\\nIn the research, Apache Doris stood out. Doris\'s high-concurrency query capability is very attractive. Its dynamic scaling capabilities are also suitable for our flexible advertising business. So we chose Apache Doris for sure.\\n\\n![page_2](/images/LY/en/page_2.png)\\n\\nAfter introducing Apache Doris, we upgraded the entire data warehouse:\\n- We collect MySQL Binlog through Canal and then it is ingested into Kafka. Because Apache Doris is highly capatible with Kafka, we can easily use Routine Load to load and import data.\\n- We have made minor adjustments to the batch processing. For data stored in Hive, Apache Doris can ingest data from Hive through Broker Load. In this way, the data in batch processing can be directly ingested into Doris.\\n\\n## 3.2 Why We Choose Doris\\n\\n![page_3](/images/LY/en/page_3.png)\\n\\nThe overall performance of Apache Doris is impressive:\\n- Data access: It provides rich data import methods and can support the access of many types of data sources;\\n- Data connection: Doris supports JDBC and ODBC connections. And it can easily connect with BI tools. In addition, Doris uses the MySQL protocol for communication. Users can directly access Doris through various Client tools;\\n- SQL syntax: Doris adopts MySQL protocol and it is highly compatible with MySQL syntax, supporting standard SQL, and is low in learning costs for developers;\\n- MPP parallel computing: Doris provides excellent parallel computing capabilities and has obvious advantages in complex Join and wide table Join;\\n- Fully-completed documentation: Doris official documentation is very profound, which is friendly for new users. \\n\\n\\n## 3.3  Architecture of Real-time Processing \\n\\n![page_4](/images/LY/en/page_4.png)\\n\\n- Data source: In real-time processing, data sources come from business branches such as industrial finance, consumer finance, and risk control. They are all collected through Canal and API.\\n- Data collection: After data collection through Canal-Admin, Canal sends the data to Kafka message queue. After that, the data is ingested into the Doris through Routine Load.\\n- Inside Doris: The Doris cluster constitutes a  three-level layer of the data warehouse, namely: the DWD layer with the Unique model, the DWS layer with the Aggregation model, and the ADS application layer.\\n- Data application: The data is applied in three aspects: real-time dashboard, data timeliness analysis and data service.\\n\\n## 3.4 New Features\\n\\nThe data import method is simple and adopts 3 different import methods according to different scenarios:\\n- Routine Load: When we submit the Rountine Load task, there will be a process within Doris that consumes Kafka in real time, continuously reads data from Kafka and ingestes it into Doris.\\n- Broker Load: Offline data such as dim-tables and historical data are ingested into Doris in an orderly manner.\\n- Insert Into: Used for batch processing tasks, Insert into is responsible for processing data in the DWD layer\\n\\nDoris\' data model improves our development efficiency:\\n- The Unique model is used when accessing the DWD layer, which can effectively prevent repeated consumption of data.\\n-  In Doris, aggregation supports 4 models, such as Sum, Replace, Min, and Max. In this way, it may reduce a large amount of SQL code,  and no longer allow us to manually write Sum, Min, Max and other codes.\\n\\nDoris query is efficient:\\n- It supports materialized view and Rollup materialized index. The bottom layer of the materialized view is similar to the concept of Cube and the precomputation process. As a way of exchanging space for time, special tables are generated at the bottom layer. In the query, materialized view maps to the tables and responds quickly.\\n\\n# 4. Benefits of the New Data Warehouse\\n\\n- Data access: In the previous architecture, the Kudu table needs to be created manually during the imports through SteamSets. Lack of tools, the entire process of creating tables and tasks takes 20-30 minutes. Nowadays, fast data access can be realized through the platform. The access process of each table has been shortened from the previous 20-30 minutes to the current 3-5 minutes, which is to say that the performance has been improved by 5-6 times.\\n- Data development: After using Doris, we can directly use the data models, such as Unique and Aggregation.  The Duplicate model can well support logs, greatly speeding up the development process in ETL.\\n- Query analysis: The bottom layer of Doris has functions such as materialized view and Rollup materialized index. Moreover, Doris has made many optimizations for wide table associations, such as Runtime Filter and other Joins. Compared with Doris, Apache Kudu requires more complex optimization to be better used.\\n- Data report: It took 1-2 minutes to complete the rendering when we used Kudu to query before, but Doris responded in seconds or even milliseconds.\\n- Easy maintenance: Doris is not as complex as Hadoop. In March, our IDC was relocated, and 12 Doris virtual machines were all migrated within three days. The overall operation is relatively simple. In addition to physically moving the machine, FE\'s scaling only requires simple commands such as Add and Drop, which do not take a long time to do.\\n\\n# 5. Look ahead\\n\\n- Realize data access based on Flink CDC: At present, Flink CDC is not introduced, but Kafka through Canal instead. The development efficiency can be even faster if we use Flink CDC. Flink CDC still needs us to write a certain amount of code, which is not friendly for data analysts to use directly. We hope that data analysts only need to write simple SQL or directly operate. In the future planning, we plan to introduce Flink CDC.\\n- Keep up with the latest release: Now the latest version Apache Doris V1.2.0 has made great achievements in vectorization, multi-catalog, and light schema change. We will keep up with the community to upgrade the cluster and make full use of new features.\\n- Strengthen the construction of related systems: Our current index system management, such as report metadata, business metadata, and other management levels still need to be improved. Although we have data quality monitoring functions, it still needs to be strengthened and improved in automation."},{"id":"/release-1.1.5","metadata":{"permalink":"/blog/release-1.1.5","source":"@site/blog/release-1.1.5.md","title":"Apache Doris announced the official release of version 1.1.5","description":"\x3c!--","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.5","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1.5 on December 19, 2022","date":"2022-12-19","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Best Practice: The Efficiency of the Data Warehouse Greatly Improved in LY Digital","permalink":"/blog/LY"},"nextItem":{"title":"Best Practice in Kwai: Apache Doris on Elasticsearch","permalink":"/blog/BestPractice_Kwai"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn this release, Doris Team has fixed about 36 issues or performance improvement since 1.1.4. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n# Behavior Changes\\n\\nWhen alias name is same as the original column name like \\"select year(birthday) as birthday\\" and use it in group by, order by , having clause, doris\'s behavior is different from MySQL in the past. In this release, we make it follow MySQL\'s behavior. Group by and having clause will use original column at first and order by will use alias first. It maybe a litter confuse here so there is a simple advice here, you\'d better not use an alias the same as original column name.\\n\\n# Features\\n\\nAdd support of murmur_hash3_64. [#14636](https://github.com/apache/doris/pull/14636)\\n\\n# Improvements\\n\\nAdd timezone cache for convert_tz to improve performance. [#14616](https://github.com/apache/doris/pull/14616)\\n\\nSort result by tablename when call show clause. [#14492](https://github.com/apache/doris/pull/14492)\\n\\n# Bug Fix\\n\\nFix coredump when there is a if constant expr in select clause.  [#14858](https://github.com/apache/doris/pull/14858)\\n\\nColumnVector::insert_date_column may crashed. [#14839](https://github.com/apache/doris/pull/14839)\\n\\nUpdate high_priority_flush_thread_num_per_store default value to 6 and it will improve the load performance. [#14775](https://github.com/apache/doris/pull/14775)\\n\\nFix quick compaction core.  [#14731](https://github.com/apache/doris/pull/14731)\\n\\nPartition column is not duplicate key, spark load will throw IndexOutOfBounds error. [#14661](https://github.com/apache/doris/pull/14661)\\n\\nFix a memory leak problem in VCollectorIterator. [#14549](https://github.com/apache/doris/pull/14549)\\n\\nFix create table like when having sequence column. [#14511](https://github.com/apache/doris/pull/14511)\\n\\nUsing avg rowset to calculate batch size instead of using total_bytes since it costs a lot of cpu. [#14273](https://github.com/apache/doris/pull/14273)\\n\\nFix right outer join core with conjunct. [#14821](https://github.com/apache/doris/pull/14821)\\n\\nOptimize policy of tcmalloc gc.  [#14777](https://github.com/apache/doris/pull/14777) [#14738](https://github.com/apache/doris/pull/14738) [#14374](https://github.com/apache/doris/pull/14374)"},{"id":"/BestPractice_Kwai","metadata":{"permalink":"/blog/BestPractice_Kwai","source":"@site/blog/BestPractice_Kwai.md","title":"Best Practice in Kwai: Apache Doris on Elasticsearch","description":"\x3c!--","date":"2022-12-14T00:00:00.000Z","formattedDate":"December 14, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Xiang He"}],"frontMatter":{"title":"Best Practice in Kwai: Apache Doris on Elasticsearch","language":"en","summary":"This article mainly focuses on the practice of Apache Doris on Elasticsearch (DOE) in Kwai\'s business.Kwai\u2019s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai\'s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.","date":"2022-12-14","author":"Xiang He","tags":["Best Practice"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1.5","permalink":"/blog/release-1.1.5"},"nextItem":{"title":"Practice and Optimization of Apache Doris in Xiaomi","permalink":"/blog/xiaomi_vector"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Author: Xiang He, Head Developer of Big Data, Commercialization Team of Kwai\\n\\n![kv](/images/Kwai/en/kv.png)\\n\\n# 1 About Kwai\\n\\n## 1.1 Kwai\\nKwai(HKG:1024) is a social network for short videos and trends. Discover funny short videos, contribute to the virtual community with recordings, videos of your life, playing daily challenges or likes the best memes and videos. Share your life with short videos and choose from dozens of magical effects and filters for them.\\n\\n## 1.2 Kwai\'s Commercial Report Engine\\nKwai\u2019s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases.\\n\\n# 2 Previous Architecture\\n\\n## 2.1 Background\\nTraditional OLAP engines deal with multi-dimensional analysis in a more pre-modeled way, by building a data cube (Cube) to perform operations such as Drill-down, Roll-up, Slice, and Dice and Pivot. Modern OLAP analysis introduces the idea of \u200b\u200ba relational model, representing data in two-dimensional relational tables. In the modeling process, usally there are two modeling methods. One is to ingest the data of multiple tables into one wide table through Join; the other is to use the star schema, divide the data into fact table and dim-table.  And then Join them when querying. \\nBoth options have some pros and cons:\\n\\nWide table:\\n\\nTaking the idea of \u200b\u200bexchanging space for time. The primary key of the dim-table is the unique ID to fill all dimensions, and multiple dimension data is stored in redundant storage. Its advantage is that it is convenient to query, unnecessary to associate additional dim-tables, which is way better. The disadvantage is that if there is a change in dimension data, the entire table needs to be refreshed, which is bad for high-frequency Update.\\n\\nStar Schema:\\n\\nDimension data is completely separated from fact data. Dimension data is often stored in a dedicated engine (such as MySQL, Elasticsearch, etc.). When querying, dimension data is associated with the primary key. The advantage is that changes in dimension data do not affect fact data, which can support high-frequency Update operations. The disadvantage is that the query logic is relatively more complex, and multi-table Join may lead to performance loss.\\n\\n## 2.2 Requirement for an OLAP Engine\\n\\nIn Kwai\u2019s business, the commercial reports engine supports the real-time query of the advertising effect for advertisers. When building the report engine, we expect to meet the following requirements:\\n- Immersive data: the original data of a single table increases by ten billion every day\\n- High QPS in Query: thousand-level QPS on average\\n- High stability requirements: SLA level of 99.9999 %\\n\\nMost importantly, due to frequent changes in dimension data, dim-tables need to support Update operations up to thousand-level QPS and further support requirements such as fuzzy matching and word segmentation retrieval.\\nBased on the above requirements, we chose star schema and built a report engine architecture with Apache Druid and Elasticsearch.\\n\\n## 2.3 Previous Architecture: Based on Apache Druid\\n\\nWe chose the combination of Elasticsearch and Apache Druid. In data import, we use Flink to pre-aggregate the data at minute-evel, and use Kafka to pre-aggregate the data at hour-level. In data query, the application initiates a query request through RE Front API, and Re Query initiates queries to the dim-table engine (Elasticsearch and MySQL) and the extension engine respectively.\\n\\nDruid is a timing-based query engine that supports real-time data ingestion and is used to store and query large amounts of fact data. We adopt Elasticseach based on those concerns:\\n- High update frequency, QPS is around 1000\\n- Support word segmentation and fuzzy search, which is suitable for Kwai\\n- Supports high-level dim-table data, which can be directly qualified without adopting sub-database and sub-table just like MySQL database\\n- Supports data synchronization monitoring, and has check and recovery services as well\\n\\n## 2.4 Engine of the Reports\\n\\nThe report engine can be divided into two layers: REFront and REQuery. REMeta is an independent metadata management module. The report engine implements MEMJoin inside REQuery. It supports associative query between fact data in Druid and dimension data in Elasticsearch. And it also provides virtual cube query for upper-layer business, avoiding the exposion of complex cross-engine management and query logic.\\n\\n![page_1](/images/Kwai/en/page_1.png)\\n\\n# 3 New Architecture Based on Apache Doris\\n\\n## 3.1 Problems Remained \\nFirst, we came across a problem when we build the report engine. Mem Join is single-machine with serial execution. When the amount of data pulled from Elasticsearch exceeds 100,000 at a single time, the response time is close to 10s, and the user experience is poor. Moreover, using a single node to execute large-scale data Join will consume a lot of memory, causing Full GC.\\n\\nSecond, Druid\'s Lookup Join function is not so perfect, which is a big problem, and it cannot fully meet our business needs.\\n\\n## 3.2 Database Research\\n\\nSo we conducted a survey on popular OLAP databases in the industry, the most representative of which are Apache Doris and Clickhouse. We found out that Apache Doris is more capable of Join between large and wide tables. ClickHouse can support Broadcast memory-based Join, but the performance  is not good for the Join between large and wide tables with a large data volume. Both Doris and Clickhouse support detailed data storage, but the capability for concurrency of Clickhouse is low. On the contrary, Doris supports high-concurrency and low-latency query services, and a single machine supports up to thousands of QPS. When the concurrency increases, horizontal expansion of FE and BE can be supported. However, Clickhouse\'s data import is not able to support Transaction SQL, which cannot realize Exactly-once semantics and has limited ablility for standard SQL. In contrast, Doris provides Transaction SQL and atomicity for data import. Doris itself can ensure that messages in Kafka are not lost or re-subscribed, which is to say, Exactly-Once semantics is supported. ClickHouse has high learning cost, high operation and maintenance costs, and weak in distribution. The fact that it requires more customization and deeper technical strength is another problem. Doris is different. There are only two core components, FE and BE, and there are fewer external dependencies. We also found that because Doris is closer to the MySQL protocol, it is more convenient than Clickhouse and the cost of migration is not so large. In terms of horizontal expansion, Doris\' expansion and contraction can also achieve self-balancing, which is much better than that of Clickhouse.\\n\\nFrom this point of view, Doris can better improve the performance of Join and is much better in other aspects such as migration cost, horizontal expansion, and concurrency. However, Elasticsearch has inherent advantages in high-frequency Update.\\n\\nIt would be an ideal solution to deal with high-frequency Upate and Join performance at the same time by building engines through Doris on Elasticsearch.\\n\\n## 3.3 Good Choice: Doris on Elasticsearch\\n\\nWhat is the query performance of Doris on Elasticsearch?\\n\\nFirst of all, Apache Doris is a real-time analytical database based on MPP architecture, with strong performance and strong horizontal expansion capability. Doris on Elasticsearch takes advantage on this capability and does a lot of query optimization. Secondly, after integrating Elasticsearch, we have also made a lot of optimizations to the query:\\n- Shard-level concurrency\\n- Automatic adaptation of row and column scanning, priority to column scanning\\n- Sequential read, terminated early\\n- Two-phase query becomes one-phase query\\n- Broadcast Join is especially friendly for small batch data\\n\\n![page_2](/images/Kwai/en/page_2.png)\\n\\n## 3.4 Doris on Elasticsearch\\n\\n### 3.4.1 Data Link Upgrade\\n\\nThe upgrade of the data link is relatively simple. In the first step, in Doris we build a new Olap table and configure the materialized view. Second, the routine load is initiated based on the Kafka topic of the previous fact data, and then real-time data is ingested. The third step is to ingest offline data from Hive\'s broker load. The last step is to create an Elasticsearch external table through Doris.\\n\\n![page_3](/images/Kwai/en/page_3.png)\\n\\n### 3.4.2 Upgrades of the Report Engine\\n\\n![page_4](/images/Kwai/en/page_4.png)\\n\\nNote: The MySQL dim-table associated above is based on future planning. Currently, Elasticsearch is mainly used as the dim-table engine\\n\\nReport Engine Adaptation\\n- Generate virtual cube table based on Doris\'s star schema\\n- Adapt to cube table query analysis, intelligent Push-down\\n- Gray Release\\n\\n# 4  Online Performance\\n\\n## 4.1 Fact Table Query Performance Comparison\\n\\nDruid\\n\\n![page_5](/images/Kwai/en/page_5.png)\\n\\nDoris\\n\\n![page_6](/images/Kwai/en/page_6.png)\\n\\n99th percentile of response time: \\nDruid: 270ms, Doris: 150ms and which is reduced by 45%\\n\\n## 4.2 Comparison of Cube Table Query Performance in Join\\n\\nDruid\\n\\n![page_7](/images/Kwai/en/page_7.png)\\n\\nDoris\\n\\n![page_8](/images/Kwai/en/page_8.png)\\n\\n99th percentile of response time: \\nDruid: 660ms, Doris: 440ms and which is reduced by 33%\\n\\n## 4.3 Benefits\\n\\n- The overall time consumption of 99 percentile is reduced by about 35%\\n- Resource saving about 50%\\n- Remove the complex logic of MemJoin from the report engine; Realize through DO(in the case of large query: dim-table results exceed 100,000, performance improvement exceeds 10 times, 10s to 1s)\\n- Richer query semantics (Mem Join is relatively simple and does not support complex queries)\\n\\n# 5  Summary and Plans\\n\\nIn Kwai\'s commercial business, Join queries between dimension data and fact data is very common. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai\'s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.\\n\\nAlthough the Doris on Elasticsearch has fully meet our requirement, Elasticsearch external table still requires manual creation. However, Apache Doris recently released the latest version V1.2.0. The new version has added Multi-Catlog, which provides the ability to seamlessly access external table sources such as Hive, Elasticsearch, Hudi, and Iceberg. Users can connect to external tables through the CREATE CATALOG command, and Doris will automatically map the library and table information of the external dable. In this way, we don\'t need to manually create the Elasticsearch external tables to complete the mapping in the future, which greatly saves us time and cost of development and improves the efficiency of research and development. The power of other new functions such as Vectorization and Ligt Schema Change also gives us new expectations for Apache Doris. Bless Apache Doris!\\n\\n\\n# Contact Us\\n\\nApache Doris Website\uff1ahttp://doris.apache.org\\n\\nGithub\uff1ahttps://github.com/apache/doris\\n\\nDev Email\uff1adev@doris.apache.org"},{"id":"/xiaomi_vector","metadata":{"permalink":"/blog/xiaomi_vector","source":"@site/blog/xiaomi_vector.md","title":"Practice and Optimization of Apache Doris in Xiaomi","description":"\x3c!--","date":"2022-12-08T00:00:00.000Z","formattedDate":"December 8, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"ZuoWei"}],"frontMatter":{"title":"Practice and Optimization of Apache Doris in Xiaomi","summary":"Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.","date":"2022-12-08","author":"ZuoWei","tags":["Best Practice"]},"prevItem":{"title":"Best Practice in Kwai: Apache Doris on Elasticsearch","permalink":"/blog/BestPractice_Kwai"},"nextItem":{"title":"Apache Doris announced the official release of version 1.2.0","permalink":"/blog/release-1.2.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n> Guide: Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.\\n\\n> Author: ZuoWei, OLAP Engineer, Xiaomi\\n\\n![kv](/images/xiaomi/en/kv.png)\\n\\n# About Xiaomi\\n[Xiaomi Corporation](https://www.mi.com/global) (\u201cXiaomi\u201d or the \u201cGroup\u201d; HKG:1810), a consumer electronics and smart manufacturing company with smartphones and smart hardware connected by an Internet of Things (IoT) platform.  In 2021, Xiaomi\'s total revenue amounted to RMB328.3 billion(USD472,231,316,200), an increase of 33.5% year-over-year; Adjusted net profit was RMB22.0 billion(USD3,164,510,800), an increase of 69.5% year-over-year.\\n\\nDue to the growing need of data analysis, Xiaomi Group introduced Apache Doris in 2019. As one of the earliest users of Apache Doris, Xiaomi Group has been deeply involved in the open-source community. After three years of development, Apache Doris has been widely used in dozens of business departments within Xiaomi, such as Advertising, New Retail, Growth Analysis, Dashboards, UserPortraits, [AISTAR](https://airstar.com/home), [Xiaomi Youpin](https://www.xiaomiyoupin.com). Within Xiaomi, a data ecosystem has been built around Apache Doris. \\n\\n![page_1](/images/xiaomi/en/page_1.jpg)\\n\\nAt present, Apache Doris already has dozens of clusters in Xiaomi, with an overall scale of hundreds of virtual machines . Among them, the largest single cluster reaches nearly 100 nodes, with dozens of real-time data synchronization tasks. And the largest daily increment of a single table rocket to 12 billion, supporting PB-level storage. And a single cluster can support more than 20,000 multi-dimensional analysis queries per day.\\n\\n# Architecture Evolution\\nThe original intention of Xiaomi to introduce Apache Doris is to solve the problems encountered in user behavior analysis. With the development of Xiaomi\'s Internet business, the demand for growth analysis using user behavior data is becoming stronger and stronger. If each business branch builds its own growth analysis system, it will not only be costly, but also inefficient. Therefore, if there is a product that can help them stop worrying about underlying complex technical details, it would be great to have relevant business personnel focus on their own technical work. In this way, it can greatly improve work efficiency. Therefore, Xiaomi Big Data and the cloud platform jointly developed the growth analysis system called Growing Analytics (referred to as GA), which aims to provide a flexible multi-dimensional real-time query and analysis platform, which can manage data access and query solutions in a unified way, and help business branches to refine operation.\\n\\n## Previous Architecture\\nThe growth analysis platform project was established in mid-2018. At that time, based on the consideration of development time and cost, Xiaomi reused various existing big data basic components (HDFS, Kudu, SparkSQL, etc.) to build a growth analysis query system based on Lambda architecture. The architecture of the first version of the GA system is shown in the figure below, including the following aspects:\\n\\n- Data Source: The data source is the front-end embedded data and user behavior data.\\n- Data Access: The event tracking data is uniformly cleaned and ingested into Xiaomi\'s internal self-developed message queue, and the data is imported into Kudu through Spark Streaming.\\n- Storage: Separate hot and cold data in the storage layer. Hot data is stored in Kudu, and cold data is stored in HDFS. At the same time, partitioning is carried out in the storage layer. When the partition unit is day, part of the data will be cooled and stored on HDFS every night.\\n- Compute and Query: In the query layer, use SparkSQL to perform federated queries on the data on Kudu and HDFS, and finally display the query results on the front-end page.\\n\\n![page_2](/images/xiaomi/en/page_2.jpg)\\n\\nAt that time, the first version of the growth analysis platform helped us solve a series of problems in the user operation process, but there were also two problems:\\n\\n### Problem No.1: Scattered components\\nSince the historical architecture is based on the combination of SparkSQL + Kudu + HDFS, too many dependent components lead to high operation and maintenance costs. The original design is that each component uses the resources of the public cluster, but in practice, it is found that during the execution of the query job, the query performance is easily affected by other jobs in the public cluster, and query jitter is prone to occur, especially when reading data from the HDFS public cluster , sometimes slower.\\n\\n### Problem No.2: High resource consumption\\nWhen querying through SparkSQL, the latency is relatively high. SparkSQL is a query engine designed based on a batch processing system. In the process of exchanging data shuffle between each stage, it still needs to be placed on the disk, and the delay in completing the SQL query is relatively high. In order to ensure that SQL queries are not affected by resources, we ensure query performance by adding machines. However, in practice, we find that there is limited room for performance improvement. This solution cannot make full use of machine resources to achieve efficient queries. A certain waste of resources.\\n\\nIn response to the above two problems, our goal is to seek an MPP database that integrates computing and storage to replace our current storage and computing layer components. After technical selection, we finally decided to use Apache Doris to replace the older generation of historical architecture.\\n\\n## New Choice\\nPopular MPP-based query engines such as Impala and Presto, can efficiently support SQL queries, but they still need to rely on Kudu, HDFS, Hive Metastore and other storage system, which increase the operation and maintenance costs. At the same time, due to the separation of storage and compute, the query engine cannot easily find the data changes in the storage layer, resulting in bad performance in detailed query optimization. If you want to cache at the SQL layer, you cannot guarantee that the query results are up-to-date.\\n\\nApache Doris is a top-level project of the Apache Foundation. It is mainly positioned as a high-performance, real-time analytical database, and is mainly used to solve reports and multi-dimensional analysis. It integrates Google Mesa and Cloudera Impala technologies. We conducted an in-depth performance tests on Doris and communicated with the community many times. And finally, we determined to replace the previous computing and storage components with Doris. \\n\\n## New Architecture Based on Apache Doris\\nThe new architecture obtains event tracking data from the data source. Then data is ingested  into Apache Doris. Query results can be directly displayed in the applications. In this way, Doris has truly realized the unification of computing, storage, and resource management tools.\\n\\n![page_3](/images/xiaomi/en/page_3.jpg)\\n\\nWe chose Doris because:\\n- Doris has excellent query performance and can meet our business needs.\\n- Doris supports standard SQL, and the learning cost is low.\\n- Doris does not depend on other external components and is easy to operate and maintain.\\n- The Apache Doris community is very active and friendly, crowded with contributors. It is easier for further versions upgrades and convenient for maintenance.\\n\\n## Query Performance Comparision between Apache Doris & Spark SQL\\nNote: The comparison is based on Apache Doris V0.13\\n\\n![page_4](/images/xiaomi/en/page_4.jpg)\\n\\nWe selected a business model with an average daily data volume of about 1 billion, and conducted performance tests on Doris in different scenarios, including 6 event analysis scenarios, 3 retention analysis scenarios, and 3 funnel analysis scenarios. After comparing it with the previous architecture(SparkSQL+Kudu+HDFS), we found out:\\n- In the event analysis scenario, the average query time was reduced by 85%.\\n- In the scenarios of retention analysis and funnel analysis, the average query time was reduced by 50%.\\n\\n# Real Practice\\nBelow we will introduce our experience of data import, data query, A/B test in the business application of Apache Doris.\\n\\n## Data Import\\n\\n![page_5](/images/xiaomi/en/page_5.jpg)\\n\\nXiaomi writes data into Doris mainly through Stream Load, Broker Load and a small amount of data by Insert. Usually data is generally ingested into the message queue first, which is divided into real-time and offline data.\\n\\n### How to write real-time data into Apache Doris: \\nAfter part of real-time data processed by Flink, they will be ingested into Doris through  Flink-Doris-Connector provided by Apache Doris. The rest of the data is ingested through Spark Streaming. The bottom layer of these two writing approaches both rely on the Stream Load provided by Apache Doris.\\n\\n### How to write offline data into Apache Doris: \\nAfter offline data is partially ingested into Hive, they will be ingested into Doris through Xiaomi\'s data import tool. Users can directly submit Broker Load tasks to the Xiaomi\'s data import tool and import data directly into Doris, or import data through Spark SQL, which relies on the Spark-Doris-Connector provided by Apache Doris. Spark Doris Connector is actually the encapsulation of Stream Load.\\n\\n## Data Qurey\\n\\n![page_6](/images/xiaomi/en/page_6.jpg)\\n\\nUsers can query after data import is done. Inside Xiaomi, we query through our data platform. Users can perform visual queries on Doris through Xiaomi\'s data platform, and conduct user behavior analysis and user portrait analysis. In order to help our teams conduct event analysis, retention analysis, funnel analysis, path analysis and other behavioral analysis, we have added corresponding UDF (User Defined Function) and UDAF (User Defined Aggregate Function) to Doris.\\n\\nIn the upcoming version 1.2, Apache Doris adds the function of synchronizing metadata through external table, such as Hive/Hudi/Iceberg and Multi Catalog tool. External table query improves performance, and the ability to access external tables greatly increases ease of use. In the future, we will consider querying Hive and Iceberg data directly through Doris, which builds an architecture of datalake.\\n\\n## A/B Test\\nIn real business, the A/B test is a method of comparing two versions of strategies against each other to determine which one performs better. A/B test is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis. It is popular approach used to determine which variation performs better for a given conversion goal. Xiaomi\'s A/B test platform is an operation tool product that conducts the A/B test with experimental grouping, traffic splitting, and scientific evaluation to assist in decision making. Xiaomi\'s A/B test platform has several query applications: user deduplication, indicator summation, covariance calculation, etc. The query types will involve Count (distinct), Bitmap, Like, etc.\\n\\nApache Doris also provides services to Xiaomi\'s A/B test platform. Everyday, Xiaomi\'s A/B test platform needs to process a temendous amount of data with billions of queries. That\'s why Xiaomi\'s A/B test platform is eager to improve the query performance. \\n\\nApache Doris V1.1 released just in time and has fully supported vectorization in the processing and storage. Compared with the non-vectorized version, the query performance has been significantly improved. It is time to update Xiaomi\'s Doris cluster to the latest version. That\'s why we first launched the latest vectorized version of Doris on Xiaomi\'s A/B test platform.\\n\\n## Test before Launch\\nNote: The following tests are based on Apache Doris V1.1.2\\n\\nWe built a test cluster for Apache Doris V1.1.2, which is as big as that of the Xiaomi online Apache Doris V0.13 version, to test before the vectorization version goes online. The test is divided into two aspects: single SQL parrellel query test and batch SQL concurrent query test.\\n\\nThe configurations of the two clusters are exactly the same, and the specific configuration information is as follows:\\n- Scale: 3 FEs + 89 virtual machines\\n- CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz 16 cores 32 threads \xd7 2\\n- Memory: 256GB\\n- Disk: 7.3TB \xd7 12 HDD\\n\\n### Single SQL Parrellel Query Test\\nWe choose 7 classic queries in the Xiaomi A/B test. For each query, we limited the time range to 1 day, 7 days, and 20 days for testing, where the daily partition data size is about 3.1 billion (the data volume is about 2 TB). The test results are shown in the figures:\\n\\n![page_7](/images/xiaomi/en/page_7.jpg)\\n\\n![page_8](/images/xiaomi/en/page_8.jpg)\\n\\n![page_9](/images/xiaomi/en/page_9.jpg)\\n\\nThe Apache Doris V1.1.2 has at least 3~5 times performance improvement compared to the Xiaomi online Doris V0.13, which is remarkable.\\n\\n## Optimization\\nNote: The following tests are based on Apache Doris V1.1.2\\n\\nBased on Xiaomi\'s A/B test business data, we tuned Apache Doris V1.1.2 and conducted concurrent query tests on the tuned Doris V1.1.2 and Xiaomi\'s online Doris V0.13. The test results are as follows.\\n\\n### Optimization in Test 1\\nWe choose user deduplication, index summation, and covariance calculation query(the total number of SQL is 3245) in the A/B test to conduct concurrent query tests on the two versions. The single-day partition data of the table is about 3.1 billion (the amount of data is about 2 TB) and the query will be based on the latest week\'s data. The test results are shown in the figures:\\n\\n![page_10](/images/xiaomi/en/page_10.jpg)\\n\\nCompared with Apache Doris V0.13, the overall average latency of Doris V1.1.2 is reduced by about 48%, and the P95 latency is reduced by about 49%. In this test, the query performance of Doris V1.1.2 was nearly doubled compared to Doris V0.13.\\n\\n### Optimization in Test 2\\nWe choose 7 A/B test reports to test the two versions. Each A/B test report is corresponded to two modules in Xiaomi A/B test platform and each module represents thousands of SQL query. Each report submits query tasks to the cluster where the two versions reside at the same concurrency. The test results are shown in the figure:\\n\\n![page_11](/images/xiaomi/en/page_11.jpg)\\n\\nCompared with Doris V0.13, Doris V1.1.2 reduces the overall average latency by around 52%. In the test, the query performance of Doris V1.1.2 version was more than 1 time higher than that of Doris V0.13. \\n\\n### Optimization in Test 3\\nTo verify the performance of the tuned Apache Doris V1.1.2 in other cases, we choose the Xiaomi user behavior analysis to conduct concurrent query performance tests of Doris V1.1.2 and Doris V0.13. We choose behavior analysis query for 4 days on October 24, 25, 26 and 27, 2022. The test results are shown in the figures:\\n\\n![page_12](/images/xiaomi/en/page_12.jpg)\\n\\nCompared with Doris V0.13, the overall average latency of Doris V1.1.2 has been reduced by about 77%, and the P95 latency has been reduced by about 83%. In this test, the query performance of Doris V1.1.2 version is 4~6 times higher than that of Doris V0.13.\\n\\n# Conclusion\\nSince we adopted Apache Doris in 2019, Apache Doris has currently served dozens of businesses and sub-brands within Xiaomi, with dozens of clusters and hundreds of nodes. It completes more than 10,000 user online analysis queries every day and is responsible for most of the online analysis in Xiaomi.\\n\\nAfter performance test and tuning, Apache Doris V1.1.2 has met the launch requirements of the Xiaomi A/B test platform and does well in query performance and stability. In some cases, it even exceeds our expectations, such as the overall average latency being reduced by about 77% in our tuned version.\\n\\nMeanwhile, some functions have in the above been released in Apache Doris V1.0 or V1.1,  some PRs have been merged into the community Master Fork and should be released soon. Recently the activity of the community has been greatly enhanceed. We are glad to see that Apache Doris has become more and more mature, and stepped forward to an integrated datalake. We truly believe that in the future, more data analysis will be explored and realized within Apache Doris.\\n\\n\\n# Contact Us\\nApache Doris Website\uff1ahttp://doris.apache.org\\n\\nGithub Homepage\uff1ahttps://github.com/apache/doris\\n\\nEmail to DEV\uff1adev@doris.apache.org"},{"id":"/release-1.2.0","metadata":{"permalink":"/blog/release-1.2.0","source":"@site/blog/release-1.2.0.md","title":"Apache Doris announced the official release of version 1.2.0","description":"\x3c!--","date":"2022-12-07T00:00:00.000Z","formattedDate":"December 7, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.2.0","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.2.0 on December 7, 2022","date":"2022-12-7","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Practice and Optimization of Apache Doris in Xiaomi","permalink":"/blog/xiaomi_vector"},"nextItem":{"title":"JD.com\'s Exploration and Practice with Apache Doris in Realtime OLAP","permalink":"/blog/JD_OLAP"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\n# Feature\\n## Highlight\\n\\n1. Full Vectorizied-Engine support, greatly improved performance\\n\\n\\tIn the standard ssb-100-flat benchmark, the performance of 1.2 is 2 times faster than that of 1.1; in complex TPCH 100 benchmark, the performance of 1.2 is 3 times faster than that of 1.1.\\n\\n2. Merge-on-Write Unique Key\\n\\n\\tSupport Merge-On-Write on Unique Key Model. This mode marks the data that needs to be deleted or updated when the data is written, thereby avoiding the overhead of Merge-On-Read when querying, and greatly improving the reading efficiency on the updateable data model.\\n\\n3. Multi Catalog\\n\\n\\tThe multi-catalog feature provides Doris with the ability to quickly access external data sources for access. Users can connect to external data sources through the `CREATE CATALOG` command. Doris will automatically map the library and table information of external data sources. After that, users can access the data in these external data sources just like accessing ordinary tables. It avoids the complicated operation that the user needs to manually establish external mapping for each table.\\n   \\n    Currently this feature supports the following data sources:\\n   \\n    1. Hive Metastore: You can access data tables including Hive, Iceberg, and Hudi. It can also be connected to data sources compatible with Hive Metastore, such as Alibaba Cloud\'s DataLake Formation. Supports data access on both HDFS and object storage.\\n    2. Elasticsearch: Access ES data sources.\\n    3. JDBC: Access MySQL through the JDBC protocol.\\n   \\n    Documentation: https://doris.apache.org/zh-CN/docs/dev/ecosystem/external-table/multi-catalog)\\n\\n    > Note: The corresponding permission level will also be changed automatically, see the \\"Upgrade Notes\\" section for details.\\n   \\n4. Light table structure changes\\n\\nIn the new version, it is no longer necessary to change the data file synchronously for the operation of adding and subtracting columns to the data table, and only need to update the metadata in FE, thus realizing the millisecond-level Schema Change operation. Through this function, the DDL synchronization capability of upstream CDC data can be realized. For example, users can use Flink CDC to realize DML and DDL synchronization from upstream database to Doris.\\n\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\nWhen creating a table, set `\\"light_schema_change\\"=\\"true\\"` in properties.\\n\\n5. JDBC facade\\n\\n\\tUsers can connect to external data sources through JDBC. Currently supported:\\n\\n\\t  - MySQL\\n\\t  - PostgreSQL\\n\\t  - Oracle\\n\\t  - SQL Server\\n\\t  - Clickhouse\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/ecosystem/external-table/jdbc-of-doris/\\n\\n\\t> Note: The ODBC feature will be removed in a later version, please try to switch to the JDBC.\\n\\n6. JAVA UDF\\n\\n\\tSupports writing UDF/UDAF in Java, which is convenient for users to use custom functions in the Java ecosystem. At the same time, through technologies such as off-heap memory and Zero Copy, the efficiency of cross-language data access has been greatly improved.\\n\\n\\tDocument: https://doris.apache.org/zh-CN/docs/dev/ecosystem/udf/java-user-defined-function\\n\\n\\tExample: https://github.com/apache/doris/tree/master/samples/doris-demo\\n\\t\\n7. Remote UDF\\n\\n\\tSupports accessing remote user-defined function services through RPC, thus completely eliminating language restrictions for users to write UDFs. Users can use any programming language to implement custom functions to complete complex data analysis work.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/ecosystem/udf/remote-user-defined-function\\n\\n\\tExample: https://github.com/apache/doris/tree/master/samples/doris-demo\\n   \\n8. More data types support\\n\\n\\t- Array type\\n\\n\\t\\tArray types are supported. It also supports nested array types. In some scenarios such as user portraits and tags, the Array type can be used to better adapt to business scenarios. At the same time, in the new version, we have also implemented a large number of data-related functions to better support the application of data types in actual scenarios.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Types/ARRAY\\n\\n\\tRelated functions: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/array-functions/array_max\\n        \\n\\t- Jsonb type\\n\\n\\t\\tSupport binary Json data type: Jsonb. This type provides a more compact json encoding format, and at the same time provides data access in the encoding format. Compared with json data stored in strings, it is several times newer and can be improved.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Types/JSONB\\n\\n\\tRelated functions: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/json-functions/jsonb_parse\\n          \\n\\t- Date V2\\n\\t\\n\\t\\tSphere of influence:\\n\\n\\t\\t1. The user needs to specify datev2 and datetimev2 when creating the table, and the date and datetime of the original table will not be affected.\\n\\t\\t2. When datev2 and datetimev2 are calculated with the original date and datetime (for example, equivalent connection), the original type will be cast into a new type for calculation\\n\\t\\t3. The example is in the documentation\\n\\n\\t\\tDocumentation: https://doris.apache.org/docs/dev/sql-manual/sql-reference/Data-Types/DATEV2\\n\\t\\n\\t \\n## More\\n\\n1. A new memory management framework\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/maint-monitor/memory-management/memory-tracker\\n\\n2. Table Valued Function\\n\\n\\tDoris implements a set of Table Valued Function (TVF). TVF can be regarded as an ordinary table, which can appear in all places where \\"table\\" can appear in SQL.\\n\\n\\tFor example, we can use S3 TVF to implement data import on object storage:\\n\\n\\t```\\n\\tinsert into tbl select * from s3(\\"s3://bucket/file.*\\", \\"ak\\" = \\"xx\\", \\"sk\\" = \\"xxx\\") where c1 > 2;\\n\\t```\\n\\n\\tOr directly query data files on HDFS:\\n\\t\\n\\t```\\n\\tinsert into tbl select * from hdfs(\\"hdfs://bucket/file.*\\") where c1 > 2;\\n\\t```\\n\\t\\n\\tTVF can help users make full use of the rich expressiveness of SQL and flexibly process various data.\\n\\n    Documentation:\\n   \\n    https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/table-functions/s3\\n   \\n    https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functions/table-functions/hdfs\\n   \\n3. A more convenient way to create partitions\\n\\n\\tSupport for creating multiple partitions within a time range via the `FROM TO` command.\\n\\n4. Column renaming\\n\\n\\tFor tables with Light Schema Change enabled, column renaming is supported.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Alter/ALTER-TABLE-RENAME\\n\\t\\n5. Richer permission management\\n\\n\\t- Support row-level permissions\\n\\t\\n\\t\\tRow-level permissions can be created with the `CREATE ROW POLICY` command.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-POLICY\\n\\t\\n\\t- Support specifying password strength, expiration time, etc.\\n\\t\\n\\t- Support for locking accounts after multiple failed logins.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Account-Management-Statements/ALTER-USER\\n\\n6. Import\\n\\n\\t- CSV import supports csv files with header.\\n\\t\\n\\t\\tSearch for `csv_with_names` in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD/\\n\\t\\n\\t- Stream Load adds `hidden_columns`, which can explicitly specify the delete flag column and sequence column.\\n\\t\\n\\t\\tSearch for `hidden_columns` in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/STREAM-LOAD\\n\\t\\n\\t- Spark Load supports Parquet and ORC file import.\\n\\t\\n\\t- Support for cleaning completed imported Labels\\n\\t  \\n\\t  Documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CLEAN-LABEL\\n\\t\\n\\t- Support batch cancellation of import jobs by status\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Load/CANCEL-LOAD\\n\\t\\n\\t- Added support for Alibaba Cloud oss, Tencent Cloud cos/chdfs and Huawei Cloud obs in broker load.\\n\\t\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/advanced/broker\\n\\t\\n\\t- Support access to hdfs through hive-site.xml file configuration.\\n\\t\\n\\t\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/config/config-dir\\n\\n7. Support viewing the contents of the catalog recycle bin through `SHOW CATALOG RECYCLE BIN` function.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Show-Statements/SHOW-CATALOG-RECYCLE-BIN\\n\\n8. Support `SELECT * EXCEPT` syntax.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/data-table/basic-usage\\n\\n9. OUTFILE supports ORC format export. And supports multi-byte delimiters.\\n   \\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/OUTFILE\\n\\n10. Support to modify the number of Query Profiles that can be saved through configuration.\\n\\n\\tDocument search FE configuration item: max_query_profile_num\\n\\t\\n11. The DELETE statement supports IN predicate conditions. And it supports partition pruning.\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Manipulation-Statements/Manipulation/DELETE\\n\\n12. The default value of the time column supports using `CURRENT_TIMESTAMP`\\n\\n\\tSearch for \\"CURRENT_TIMESTAMP\\" in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\n13. Add two system tables: backends, rowsets\\n\\n\\tDocumentation:\\n\\n\\thttps://doris.apache.org/zh-CN/docs/dev/admin-manual/system-table/backends\\n\\n\\thttps://doris.apache.org/zh-CN/docs/dev/admin-manual/system-table/rowsets\\n\\n14. Backup and restore\\n\\n\\t- The Restore job supports the `reserve_replica` parameter, so that the number of replicas of the restored table is the same as that of the backup.\\n\\t\\n\\t- The Restore job supports `reserve_dynamic_partition_enable` parameter, so that the restored table keeps the dynamic partition enabled.\\n\\t\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/RESTORE\\n\\t\\n\\t- Support backup and restore operations through the built-in libhdfs, no longer rely on broker.\\n\\t\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Backup-and-Restore/CREATE-REPOSITORY\\n\\n15. Support data balance between multiple disks on the same machine\\n\\n\\tDocumentation:\\n\\t\\n\\thttps://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-REBALANCE-DISK\\n\\t\\n\\thttps://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-CANCEL-REBALANCE-DISK\\n\\n16. Routine Load supports subscribing to Kerberos-authenticated Kafka services.\\n\\n\\tSearch for kerberos in the documentation: https://doris.apache.org/zh-CN/docs/dev/data-operate/import/import-way/routine-load-manual\\n\\n17. New built-in-function\\n\\n\\tAdded the following built-in functions:\\n\\t\\n\\t- `cbrt`\\n\\t- `sequence_match/sequence_count`\\n\\t- `mask/mask_first_n/mask_last_n`\\n\\t- `elt`\\n\\t- `any/any_value`\\n\\t- `group_bitmap_xor`\\n\\t- `ntile`\\n\\t- `nvl`\\n\\t- `uuid`\\n\\t- `initcap`\\n\\t- `regexp_replace_one/regexp_extract_all`\\n\\t- `multi_search_all_positions/multi_match_any`\\n\\t- `domain/domain_without_www/protocol`\\n\\t- `running_difference`\\n\\t- `bitmap_hash64`\\n\\t- `murmur_hash3_64`\\n\\t- `to_monday`\\n\\t- `not_null_or_empty`\\n\\t- `window_funnel`\\n\\t- `group_bit_and/group_bit_or/group_bit_xor`\\n\\t- `outer combine`\\n\\t- and all array functions\\n\\n# Upgrade Notice\\n\\n## Known Issues\\n\\n- Use JDK11 will cause BE crash, please use JDK8 instead.\\n\\n## Behavior Changed\\n\\n- Permission level changes\\n\\n\\tBecause the catalog level is introduced, the corresponding user permission level will also be changed automatically. The rules are as follows:\\n\\t\\n\\t- GlobalPrivs and ResourcePrivs remain unchanged\\n\\t- Added CatalogPrivs level.\\n\\t- The original DatabasePrivs level is added with the internal prefix (indicating the db in the internal catalog)\\n\\t- Add the internal prefix to the original TablePrivs level (representing tbl in the internal catalog)\\n\\n- In GroupBy and Having clauses, match on column names in preference to aliases. (#14408)\\n\\n- Creating columns starting with `mv_` is no longer supported. `mv_` is a reserved keyword in materialized views (#14361)\\n\\n- Removed the default limit of 65535 rows added by the order by statement, and added the session variable `default_order_by_limit` to configure this limit. (#12478)\\n\\n- In the table generated by \\"Create Table As Select\\", all string columns use the string type uniformly, and no longer distinguish varchar/char/string (#14382)\\n\\n- In the audit log, remove the word `default_cluster` before the db and user names. (#13499) (#11408)\\n\\n- Add sql digest field in audit log (#8919)\\n\\n- The union clause always changes the order by logic. In the new version, the order by clause will be executed after the union is executed, unless explicitly associated by parentheses. (#9745)\\n\\n- During the decommission operation, the tablet in the recycle bin will be ignored to ensure that the decomission can be completed. (#14028)\\n\\n- The returned result of Decimal will be displayed according to the precision declared in the original column, or according to the precision specified in the cast function. (#13437)\\n\\n- Changed column name length limit from 64 to 256 (#14671)\\n\\n- Changes to FE configuration items\\n\\n  - The `enable_vectorized_load` parameter is enabled by default. (#11833)\\n\\n  - Increased `create_table_timeout` value. The default timeout for table creation operations will be increased. (#13520)\\n\\n  - Modify `stream_load_default_timeout_second` default value to 3 days.\\n\\n  - Modify the default value of `alter_table_timeout_second` to one month.\\n\\n  - Increase the parameter `max_replica_count_when_schema_change` to limit the number of replicas involved in the alter job, the default is 100000. (#12850)\\n\\n  - Add `disable_iceberg_hudi_table`. The iceberg and hudi appearances are disabled by default, and the multi catalog function is recommended. (#13932)\\n\\n- Changes to BE configuration items\\n\\n  - Removed `disable_stream_load_2pc` parameter. 2PC\'s stream load can be used directly. (#13520)\\n\\n  - Modify `tablet_rowset_stale_sweep_time_sec` from 1800 seconds to 300 seconds.\\n\\n  - Redesigned configuration item name about compaction (#13495)\\n\\n  - Revisited parameter about memory optimization (#13781)\\n\\n- Session variable changes\\n\\n   - Modify the variable `enable_insert_strict` to true by default. This will cause some insert operations that could be executed before, but inserted illegal values, to no longer be executed. (11866)\\n\\n   - Modified variable `enable_local_exchange` to default to true (#13292)\\n\\n   - Default data transmission via lz4 compression, controlled by variable `fragment_transmission_compression_codec` (#11955)\\n\\n   - Add `skip_storage_engine_merge` variable for debugging unique or agg model data (#11952)\\n   \\n     Documentation: https://doris.apache.org/zh-CN/docs/dev/advanced/variables\\n\\n- The BE startup script will check whether the value is greater than 200W through `/proc/sys/vm/max_map_count`. Otherwise, the startup fails. (#11052)\\n\\n- Removed mini load interface (#10520)\\n\\n- FE Metadata Version\\n\\n\\tFE Meta Version changed from 107 to 114, and cannot be rolled back after upgrading.\\n\\t\\n## During Upgrade\\n\\n1. Upgrade preparation\\n  \\n   - Need to replace: lib, bin directory (start/stop scripts have been modified)\\n   \\n   - BE also needs to configure JAVA_HOME, and already supports JDBC Table and Java UDF.\\n   \\n   - The default JVM Xmx parameter in fe.conf is changed to 8GB.\\n\\n2. Possible errors during the upgrade process\\n  \\n   - The repeat function cannot be used and an error is reported: `vectorized repeat function cannot be executed`, you can turn off the vectorized execution engine before upgrading. (#13868)\\n   \\n   - schema change fails with error: `desc_tbl is not set. Maybe the FE version is not equal to the BE` (#13822)\\n   \\n   - Vectorized hash join cannot be used and an error will be reported. `vectorized hash join cannot be executed`. You can turn off the vectorized execution engine before upgrading. (#13753)\\n\\n\\tThe above errors will return to normal after a full upgrade.\\n\\t\\n## Performance Impact\\n\\n- By default, JeMalloc is used as the memory allocator of the new version BE, replacing TcMalloc (#13367)\\n\\n- The batch size in the tablet sink is modified to be at least 8K. (#13912)\\n\\n- Disable chunk allocator by default (#13285)\\n\\n## Api change\\n\\n- BE\'s http api error return information changed from `{\\"status\\": \\"Fail\\", \\"msg\\": \\"xxx\\"}` to more specific ``{\\"status\\": \\"Not found\\", \\"msg\\": \\"Tablet not found. tablet_id=1202\\"}``(#9771)\\n\\n- In `SHOW CREATE TABLE`, the content of comment is changed from double quotes to single quotes (#10327)\\n\\n- Support ordinary users to obtain query profile through http command. (#14016)\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/manager/query-profile-action\\n\\n- Optimized the way to specify the sequence column, you can directly specify the column name. (#13872)\\nDocumentation: https://doris.apache.org/zh-CN/docs/dev/data-operate/update-delete/sequence-column-manual\\n\\n- Increase the space usage of remote storage in the results returned by `show backends` and `show tablets` (#11450)\\n\\n- Removed Num-Based Compaction related code (#13409)\\n\\n- Refactored BE\'s error code mechanism, some returned error messages will change (#8855)\\nother\\n\\n- Support Docker official image.\\n\\n- Support compiling Doris on MacOS(x86/M1) and ubuntu-22.04\\n  Documentation: https://doris.apache.org/zh-CN/docs/dev/install/source-install/compilation-mac/\\n\\n- Support for image file verification.\\n\\n  Documentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/maint-monitor/metadata-operation/\\n\\n- script related\\n\\n  - The stop scripts of FE and BE support exiting FE and BE via the `--grace` parameter (use kill -15 signal instead of kill -9)\\n\\n  - FE start script supports checking the current FE version via --version (#11563)\\n\\n - Support to get the data and related table creation statement of a tablet through the `ADMIN COPY TABLET` command, for local problem debugging (#12176)\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Database-Administration-Statements/ADMIN-COPY-TABLET\\n\\n- Support to obtain a table creation statement related to a SQL statement through the http api for local problem reproduction (#11979)\\n\\n\\tDocumentation: https://doris.apache.org/zh-CN/docs/dev/admin-manual/http-actions/fe/query-schema-action\\n\\n- Support to close the compaction function of this table when creating a table, for testing (#11743)\\n\\n\\tSearch for \\"disble_auto_compaction\\" in the documentation: https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-TABLE\\n\\t\\n# Big Thanks\\n\\nThanks to ALL who contributed to this release! (alphabetically)\\n```\\n@924060929\\n@a19920714liou\\n@adonis0147\\n@Aiden-Dong\\n@aiwenmo\\n@AshinGau\\n@b19mud\\n@BePPPower\\n@BiteTheDDDDt\\n@bridgeDream\\n@ByteYue\\n@caiconghui\\n@CalvinKirs\\n@cambyzju\\n@caoliang-web\\n@carlvinhust2012\\n@catpineapple\\n@ccoffline\\n@chenlinzhong\\n@chovy-3012\\n@coderjiang\\n@cxzl25\\n@dataalive\\n@dataroaring\\n@dependabot[bot]\\n@dinggege1024\\n@DongLiang-0\\n@Doris-Extras\\n@eldenmoon\\n@EmmyMiao87\\n@englefly\\n@FreeOnePlus\\n@Gabriel39\\n@gaodayue\\n@geniusjoe\\n@gj-zhang\\n@gnehil\\n@GoGoWen\\n@HappenLee\\n@hello-stephen\\n@Henry2SS\\n@hf200012\\n@huyuanfeng2018\\n@jacktengg\\n@jackwener\\n@jeffreys-cat\\n@Jibing-Li\\n@JNSimba\\n@Kikyou1997\\n@Lchangliang\\n@LemonLiTree\\n@lexoning\\n@liaoxin01\\n@lide-reed\\n@link3280\\n@liutang123\\n@liuyaolin\\n@LOVEGISER\\n@lsy3993\\n@luozenglin\\n@luzhijing\\n@madongz\\n@morningman\\n@morningman-cmy\\n@morrySnow\\n@mrhhsg\\n@Myasuka\\n@myfjdthink\\n@nextdreamblue\\n@pan3793\\n@pangzhili\\n@pengxiangyu\\n@platoneko\\n@qidaye\\n@qzsee\\n@SaintBacchus\\n@SeekingYang\\n@smallhibiscus\\n@sohardforaname\\n@song7788q\\n@spaces-X\\n@ssusieee\\n@stalary\\n@starocean999\\n@SWJTU-ZhangLei\\n@TaoZex\\n@timelxy\\n@Wahno\\n@wangbo\\n@wangshuo128\\n@wangyf0555\\n@weizhengte\\n@weizuo93\\n@wsjz\\n@wunan1210\\n@xhmz\\n@xiaokang\\n@xiaokangguo\\n@xinyiZzz\\n@xy720\\n@yangzhg\\n@Yankee24\\n@yeyudefeng\\n@yiguolei\\n@yinzhijian\\n@yixiutt\\n@yuanyuan8983\\n@zbtzbtzbt\\n@zenoyang\\n@zhangboya1\\n@zhangstar333\\n@zhannngchen\\n@ZHbamboo\\n@zhengshiJ\\n@zhenhb\\n@zhqu1148980644\\n@zuochunwei\\n@zy-kkk\\n```"},{"id":"/JD_OLAP","metadata":{"permalink":"/blog/JD_OLAP","source":"@site/blog/JD_OLAP.md","title":"JD.com\'s Exploration and Practice with Apache Doris in Realtime OLAP","description":"\x3c!--","date":"2022-12-02T00:00:00.000Z","formattedDate":"December 2, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Li Zhe"}],"frontMatter":{"title":"JD.com\'s Exploration and Practice with Apache Doris in Realtime OLAP","summary":"This article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com\'s practice in  real-time OLAP and stream computing may give you some inspiration.","date":"2022-12-02","author":"Li Zhe","tags":["Best Practice"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.2.0","permalink":"/blog/release-1.2.0"},"nextItem":{"title":"Apache Doris Helped Netease Create a Refined Operation DMP System","permalink":"/blog/Netease"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n![kv](/images/jd/en/kv.png)\\n\\n> Guide:\\nThis article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com\'s practice in  real-time OLAP and stream computing may give you some inspiration.\\n\\n>Author: Li Zhe, data engineer of JD.com, who focused on offline data, stream computing and application development.\\n\\n## About JD.com\\nJD.com (NASDAQ: JD), a leading e-commerce company in China, had a net income of RMB 951.6 billion in 2021. JD Group owns JD Retail, JD Global, JD Technology, JD Logistics, JD Cloud, etc. Jingdong Group was officially listed on the NASDAQ Stock Exchange in May 2014.\\n\\n## JD Search Box\'s Requirement: Real-time Data Analysis\\nJD search box, as the entrance of the e-commerce platform, provides a link betwee merchants and users. Users can express their needs through the search box. In order to better understand user intentions and quickly improve the conversion rate, multiple A/B tests are running online at the same time, which apply to multiple products. The category, organization, and brand all need to be monitored online for better conversion. At present, JD search box demands real-time data in application mainly includes three parts:\\n\\n1. The overall data of JD search box.\\n2. Real-time monitoring of the A/B test.\\n3. Top list of hot search words to reflect changes in public opinion. Words trending can reflect what users care\\n\\nThe analysis mentioned above needs to refine the data to the SKU-level. At the same time, we also undertake the task of building a real-time data platform to show our business analysists different real-time stream computing data.\\n\\nAlthough different business analysists care about different data granularity, time frequency, and dimensions, we are hoping to establish a unified real-time OLAP data warehouse and provide a set of safe, reliable and flexible real-time data services.\\n\\nAt present, the newly generated exposure logs every day reach hundreds of millions. The logs willl increase by 10 times if they are stored as SKU. And they would grow to billions of records if based on A/B test. Aggregation queries cross multi-dimension require second-level response time. \\n\\nSuch an amount of data also brings huge challenges to the team: 2 billion rows have been created daily; Up to 60 million rows need to be imported per minute; Data latency should be limited to 1 minute; MDX query needs to be executed within 3 seconds; QPS has reached above 20. Yet a new reliable OLAP database with high stability should be able to respond to priority 0 emergency.\\n\\n## The Evolution of the Real-time Architecture\\nOur previous architecture is based on Apache Storm for a point-to-point data processing. This approach can quickly meet the needs of real-time reports during the stage of rapid business growth in the early days. However, with the continuous development of business, disadvantages gradually appear. For example, poor flexibility, poor data consistency, low development efficiency and increased resource costs.\\n\\n![page_2](/images/jd/en/page_2.png)\\n\\nIn order to solve the problems of the previous architecture, we first upgraded the architecture and replaced Apache Storm with Apache Flink to achieve high throughput. At the same time, according to the characteristics of the search data, the real-time data is processed hierarchically, which means the PV data flow, the SKU data flow and the A/B test data flow are created. It is expected to build the upper real-time OLAP layer based on the real-time flow.\\n\\nWhen selecting OLAP database, the following points need to be considered:\\n\\n1. The data latency is at minute-level and the query response time is at second-level\\n2. Suppots standard SQL, which reduces the cost of use\\n3. Supports JOIN to facilitate adding dimension\\n4. Traffic data can be deduplicated approximately, but order data must be exact deduplicated \\n5. High throughput with tens of millions of records per minute and tens of billions of new records every day\\n6. Query concurrency needs to be high because Front-end may need it\\n\\nBy comparing the OLAP engines that support real-time import , we made an in-depth comparison among Apache Druid, Elasticsearch, Clickhouse and Apache Doris:\\n\\n![page_3](/images/jd/en/page_3.png)\\n\\n\\nWe found out that Doris and Clickhouse can meet our needs. But the concurrency of Clickhouse is low for us, which is a potential risk. Moreover, the data import of Clickhouse has no TRANSACTION and cannot achieve Exactly-once semantics. Clickhouse is not fully supportive of SQL.\\n\\n\\nFinally, we chose Apache Doris as our real-time OLAP database. For user behavior log data, we use Aggregation Key data table; As for E-commerce orders data, we use Unique Key data table. Moreover, we split the previous tasks and reuse the logic we tried before. Therefore, when Flink is processing, there will be new topic flow and real-time flow of different granularities generated in DWD. The new architecture is as follows:\\n\\n![page_4](/images/jd/en/page_4.png)\\n\\nIn the current technical architecture, flink task is very light. Based on the production data detail layer, we directly use Doris to act as the aggregation layer function.  And we ask Doris to complete window calculation which previously belongs to Flink. We also take advantage of the routine load to consume real-time data. Although the data is fine-grained before importing, based on the Aggregation Key, asynchronous aggregation will be automatically performed. The degree of aggregation is completely determined by the number of dimensions. By creating Rollup on the base table, double-write or multi-write and pre-aggregate operations are performed during import, which is similar to the function of materialized view, which can highly aggregate data to improve query performance.\\n\\nAnother advantage of using Kafka to directly connect to Doris at the detail layer is that it naturally supports data backtracking. Data backtracking means that when real-time data is out of order, the \\"late\\" data can be recalculated and the previous results can be updated. This is because delayed data can be written to the table whenever it arrives. The final solution is as follows:\\n\\n![page_5](/images/jd/en/page_5.png)\\n\\n## Optimization during the Promotion\\nAs mentioned above, we have established Aggregation Key of different granularities in Doris, including PV, SKU, and A/B test granularity. Here we take the exposure A/B test model with the largest amount of daily production data as an example to explain how to support the query of tens of billions of records per day during the big promotion period.\\n\\nStrategy we used:\\n- Monitoring: 10, 30, 60 minutes A/B test with indicators, such as exposure PV, UV, exposure SKU pieces, click PV, click UV and CTR.\\n- Data Modeling: Use exposed real-time data to establish Aggregation Key; And perform HyperLogLog approximate calculation with UV and PV\\n\\nClusters we had:\\n- 30+ virtual machines with storage of NVMe SSD\\n- 40+ partitions exposed by A/B test\\n- Tens of billions of new data are created every day\\n- 2 Rollups\\n\\nBenefits overall:\\n- Bucket Field can quickly locate tablet partition when querying\\n- Import 600 million records in 10 minutes\\n- 2 Rollups have relatively low IO, which meet the requirement of the query\\n\\n## Look Ahead\\nJD search box introduced Apache Doris in May 2020, with a scale of 30+ BEs, 10+ routine load tasks running online at the same time. Replacing Flink\'s window computing with Doris can not only improve development efficiency, adapt to dimension changes, but also reduce computing resources. Apache Doris provides unified interface services ensuring data consistency and security.\\nWe are also pushing the upgrade of JD search box\'s OLAP platform to the latest version. After upgrading, we plan to use the bitmap function to support accurate deduplication operations of UV and other indicators. In addition, we also plan to use the appropriate Flink window to develop the real-time stream computing of the aggregation layer to increase the richness and completeness of the data."},{"id":"/Netease","metadata":{"permalink":"/blog/Netease","source":"@site/blog/Netease.md","title":"Apache Doris Helped Netease Create a Refined Operation DMP System","description":"\x3c!--","date":"2022-11-30T00:00:00.000Z","formattedDate":"November 30, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Xiaodong Liu"}],"frontMatter":{"title":"Apache Doris Helped Netease Create a Refined Operation DMP System","summary":"Better data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.","date":"2022-11-30","author":"Xiaodong Liu","tags":["Best Practice"]},"prevItem":{"title":"JD.com\'s Exploration and Practice with Apache Doris in Realtime OLAP","permalink":"/blog/JD_OLAP"},"nextItem":{"title":"The Application of Apache Doris in NIO","permalink":"/blog/NIO"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris Helped Netease Create a Refined Operation DMP System\\n\\n![1280X1280](/images/netease/kv.png)\\n\\n> Guide: Refined operation is a trend of the future Internet, which requires excellent data analysis. In this article, you will get knowledge of: the construction of Netease Lifease\'s DMP system and the application of Apache Doris.\\n\\n> Author | Xiaodong Liu, Lead Developer, Netease\\n\\n\\nBetter data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.\\n\\n## About Netease and Lifease\\nNetEase (NASDAQ: NTES) is a leading Internet technology company in China, providing users with free emails, gaming, search engine services, news and entertainment, sports, e-commerce and other services.\\n\\nLifease is Netease\'s self-operated home furnishing e-commerce brand. Its products cover 8 categories in total: home life, apparel, food and beverages, personal care and cleaning, baby products, outdoor sport, digital home appliances, and Lifease\'s Special. In Q1 of 2022, Lifease launches \\"Pro \\" membership and other multiple memberships for different users. The number of Pro members has increased by 65% \u200b\u200bcompared with the previous year.\\n\\n## About the DMP System\\nDMP system plays an important role in Lifease\'s data analysis. \\nThe data sources of DMP mainly include:\\n- Business logs of APPs, H5s, PCs and other terminals\\n- Basic data constructed within NetEase Group\\n- Data from products sold by third-party such as JD.com, Alibaba, and Bytedance\\nThrough data collection and data cleaning, the above data is ingested into data assets. Based on these data, DMP has created a system of functions, such as tag creation, grouping and portrait analysis, which supports the business including: intelligent product matching, user engagement, and user insight. In general, the DMP system concentrates on building a data-centric tagging system and portrait system to assist the business.\\n\\nYou can get basic knowledge of the DMP system starting from the concepts below:\\n- Tagging: Tagging is one of the user monitoring abilities to uniquely identify individual users across different browsers, devices, and user sessions. This approach to user tagging works by capturing available data in your application\'s page source: age, address, preference and other variables. \\n- Targeting: Target audience may be dictated by age, gender, income, location, interests or a myriad of other factors.\\n- User Portrait Analysis: User portrait analysis is to develop user profiles, actions and attributes after targeting audience. For instance, check the behavior paths and consumption models of users whose portraits are \\"City: Hangzhou, Gender: Female\\" on Lifease APP.\\n\\n![1280X1280](/images/netease/1__core_capability.png)\\n\\nLlifease\'s tagging system mainly provides two core capabilities: \\n1. Tag Query: the ability to query the specified tag of a specific entity, which is often used to display basic information. \\n2. Targeting Audience: for both real-time and offline targets. Result after targeting is mainly used for:\\n- As Grouping Criteria: It can be used to tell if the user is in one or more specified groups. This occasionally occurs in scenarios such as advertising and contact marketing. \\n- Resultset Pull: Extract specified data to business system for customized development.\\n- Portrait Analysis: Analyze the behavioral and consumption models in specific groups of people for more refined operations.\\n\\nThe overall business process is as follows:\\n\\n![1280X1280](/images/netease/2__business_process.png)\\n\\n- First define the rules for tags and grouping;\\n- After defining the DSL, the task can be submitted to Spark for processing;\\n- After the processing is done, the results can be stored in Hive and Doris;\\n- Data from Hive or Doris can be queried and used according to the actual business needs.\\n\\n![1280X1280](/images/netease/3__dmp_architecture.png)\\n\\nThe DMP platform is divided into four modules: Processing&storage layer, scheduling layer, service layer, and metadata management.\\nAll tag meta-information is stored in the source data table; The scheduling layer schedules tasks for the entire business process: Data processing and aggregation are converted into basic tags, and the data in the basic tags and source tables are converted into something that can be used for data query through SQL; The scheduling layer dispatches tasks to Spark to process, and then stores results in both Hive and Doris. The service layer consists of four parts: tag service, entity grouping service, basic tag data service, and portrait analysis service.\\n\\n![1280X1280](/images/netease/4__tag_lifecycle.png)\\n\\nThe lifecycle of tag consists of 5 phases:\\n- Tag requirements: At this stage, the operation team demands and the product manager team evaluates the rationality and urgency of the requirements.\\n- Scheduling production: Developers first sort out the data from ODS to DWD, which is the entire link of DM layer. Secondly, they build a model based on data, and at the same time, monitor the production process.\\n- Targeting Audience: After the tag is produced, group the audience by those tags.\\n- Precision marketing: Carry out precision marketing strategy to people grouped by.\\n- Effect evaluation: In the end, tage usage rate and use effect need to be evaluated for future optimization.\\n\\n## Production of Tags\\n\\n![1280X1280](/images/netease/5__production_of_tags.png)\\n\\nTag data layering:\\n- The bottom layer is the ODS layer, including user login logs, event tracking records, transaction data, and Binlog data of various databases\\n- The data processed by the ODS layer, such as user login table, user activity table and order information table reaches the DWD detail layer\\n- The DWD layer data is aggregated to the DM layer and the tags are all implemented based on the DM layer data.\\nAt present, we have fully automated the data output from the original database to the ODS layer. And we also realized partial automation from the ODS layer to the DWD layer. And there are a small number of automated operations from the DWD to the DM layer, which will be our focus in the future.\\n\\n![1280X1280](/images/netease/6__type_of__tags.png)\\n\\nTags are devided based on timeliness: offline tags, quasi-real-time tags and real-time tags. According to the scale of data, it is divided into: aggregation tags and detail tags. In other cases, tags can also be divided into: account attribute tags, consumption behavior tags, active behavior tags, user preference tags, asset information tags, etc. \\n\\n![1280X1280](/images/netease/7__tags_settings.png)\\n\\nIt is inconvenient to use the data of the DM layer directly because the basic data is relatively primitive. The abstraction level is lacking and it is not easy to use. By combining basic data with AND, OR, and NOT, business tags are formed for further use, which can reduce the cost of understanding operations and make it easier to use.\\n\\n![1280X1280](/images/netease/8__target_audience.png)\\n\\nAfter the tags are merged, it is necessary to apply the tags to specific business scenarios, such as grouping. The configuration is shown on the left side of the figure above, which supports offline crowd packages and real-time behaviors (need to be configured separately). After configuration, generate the DSL rules shown on the right side of the figure above, expressed in Json format, which is more friendly to FE, and can also be converted into query statements of the datebase engine.\\n\\n![1280X1280](/images/netease/9__target_audience-mapping.png)\\n\\n![1280X1280](/images/netease/10__automation.png)\\n\\nTagging is partially automated. The degree of automation in grouping is relatively high. For example, group refresh can be done regularly every day; Advanced processing, such as intersection/merge/difference between groups; Data cleaning means timely cleaning up expired and invalid data.\\n\\n## Tags Storage\\nLifease\'s DMP labeling system needs to carry relatively large customer end traffic, and has relatively high requirements for real-time performance. Our storage requirements include:\\n- Need support high-performance query to deal with large-scale customer end traffic\\n- Need support SQL to facilitate data analysis scenarios\\n- Need support data update mechanism\\n- Can store large amount of data\\n- Need support for extension functions to handle custom data structures\\n- Closely integrated with big data ecology\\n\\nIn the field of big data, multiple engines vary in different applicable scenarios. We used the popular engines in the chart below to optimize our database architecture for 2 times.\\n\\n![1280X1280](/images/netease/11__comparision.png)\\n\\nOur architecture V1.0 is shown below:\\n\\n![1280X1280](/images/netease/12__architecture_v1_0.png)\\n\\nMost of the offline data is stored in Hive while a small part is stored in Hbase (mainly used for querying basic tags). Part of the real-time data is stored in Hbase for basic tags query and the rest is double-written into KUDU and Elasticsearch for real-time grouping and data query. The data offline is processed by Impala and cached in Redis. \\nDisadvantages :\\n- Too many database engines.\\n- Double writing has hidden problems with data quality. One side may succeed while the other side fails, resulting in data inconsistency.\\n- The project is complex and maintainability is poor.\\nIn order to reduce the usage of engine and storage, we improved and implemented version 2.0 :\\n\\n![1280X1280](/images/netease/13__architecture_v2_0.png)\\n\\nIn storage architecture V2.0, Apache Doris is adopted. Offline data is mainly stored in Hive. At the same time, basic tags are imported into Doris, and real-time data as well. The query federation of Hive and Doris is performed based on Spark, and the results are stored in Redis. After this improvement, an storage engine which can manages offline and real-time data has been created. We are currently use Apache Doris 1.0, which enables : 1. The query performance can be controlled within 20ms at 99% 2.  The query performance can be controlled within 50ms at 99.9%.  Now the architecture is simplified, which greatly reduces operation and maintenance costs.\\n\\n## Advantages of Apache Doris in Practice\\n\\n![1280X1280](/images/netease/14__advantages_in_practice.png)\\n\\nLifeuse has adopted Apache Doris to check, batch query, path analyse and grouping. The advantages are as follows:\\n- The query federation performance of  key query and a small number of tables exceeds 10,000 QPS, with RT99<50MS.\\n- The horizontal expansion capability is relatively strong and maintenance cost is relatively low.\\n- The offlin and real-time data are unified to reduce the complexity of the tags model.\\n\\nThe downside is that importing a large amount of small data takes up more resources. But this problem has been optimized in Doris 1.1. Apache Doris has greatly enhanced the data compaction capability in version 1.1, and can quickly complete aggregation of new data, avoiding the -235 error caused by too many versions of sharded data and the low query efficiency problems.\\n\\n## Future Plan\\n\\n![1280X1280](/images/netease/15__future_plan.png)\\n\\nHive and Spark are gradually turning into Apache Doris. \\nOptimize the tagging system:\\n- Establish a rich and accurate tag evaluation system\\n- Improve tag quality and output speed\\n- Improve tag coverage\\nMore precision operation:\\n- Build a rich user analysis model\\n- Improve the user insight model evaluation system based on the frequency of use and user value\\n- Establish general image analysis capabilities to assist intelligent decision-making in operations"},{"id":"/NIO","metadata":{"permalink":"/blog/NIO","source":"@site/blog/NIO.md","title":"The Application of Apache Doris in NIO","description":"\x3c!--","date":"2022-11-28T00:00:00.000Z","formattedDate":"November 28, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Huaidong Tang"}],"frontMatter":{"title":"The Application of Apache Doris in NIO","summary":"NIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries. Recently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people..","date":"2022-11-28","author":"Huaidong Tang","tags":["Best Practice"]},"prevItem":{"title":"Apache Doris Helped Netease Create a Refined Operation DMP System","permalink":"/blog/Netease"},"nextItem":{"title":"How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario","permalink":"/blog/scenario"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n# The Application of Apache Doris in NIO\\n\\n![NIO](/images/NIO_kv.png)\\n\\n>Guide: The topic of this sharing is the application of Apache Doris in NIO, which mainly includes the following topics:\\n>1. Introduction about NIO\\n>2. The Development of OLAP in NIO\\n>3. Apache Doris-the Unified OLAP Data warehouse\\n>4. Best Practice of Apache Doris on CDP Architecture\\n>5. Summery and Benefits\\n\\nAuthor\uff1aHuaidong Tang, Data Team Leader, NIO INC\\n\\n## About NIO\\n\\nNIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries.\\n\\nRecently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people.\\n\\n## The Architecture Evolution of OLAP in NIO\\n\\nThe architectural evolution of OLAP in NIO took several steps for years.\\n\\n### 1. Introduced Apache Druid\\n\\nAt that time, there were not so many OLAP storage and query engines to choose from. The more common ones were Apache Druid and Apache Kylin. There are 2 reasons why we didn\'t choose Kylin.\\n\\n- The most suitable and optimal storage at the bottom of Kylin is HBase and adding it would increase the cost of operation and maintenance.\\n\\n- Kylin\'s precalculation involves various dimensions and indicators. Too many dimensions and indicators would cause great pressure on storage.\\n\\nWe prefer Druid because we used to be users and are familiar with it. Apache Druid has obvious advantages. It supports real-time and offline data import, columnar storage, high concurrency, and high query efficiency. But it has downsides as well:\\n\\n- Standard protocols such as JDBC are not used\\n\\n- The capability of JOIN is weak\\n\\n- Significant performance downhill when performing dedeplication\\n\\n- High in operation and maintenance costs, different components have separate installation methods and different dependencies; Data import needs extra integration with Hadoop and the dependencies of JAR packages\\n\\n### 2. Introduced TiDB\\n\\n**TiDB is a mature datawarehouse focused on OLTP+OLAP, which also has distinctive advantages and disadvantages:**\\n\\nAdvantage:\\n\\n- OLTP database, can be updated friendly\\n\\n- Supports detailed and aggregated query, which can handle dashboard statistical reports or query of detailed data at the same time\\n\\n- Supports standard SQL, which has low cost of use\\n\\n- Low operation and maintenance cost\\n\\nDisadvantages:\\n\\n- It is not an independent OLAP. TiFlash relies on OLTP and will increase storage. Its OLAP ability is insufficient\\n\\n- The overall performance should be measured separately by each scene\\n\\n### 3. Introduced Apache Doris\\n\\nSince 2021, we have officially introduced Apache Doris. In the process of selection, we are most concerned about various factors such as product performance, SQL protocol, system compatibility, learning and operation and maintenance costs. After deep research and detailed comparison of the following systems, we came to the following conclusions:\\n\\n**Apache Doris, whose advantages fully meet our demands:**\\n\\n- Supports high concurrent query (what we concerned most)\\n\\n- Supports both real-time and offline data\\n\\n- Supports detailed and aggregated query\\n\\n- UNIQ model can be updated\\n\\n- The ability of Materialized View can greatly speed up query efficiency\\n\\n- Fully compatible with the MySQL protocol and the cost of development is relatively low\\n\\n- The performance fully meets our requirements\\n\\n- Lower operation and maintenance costs\\n\\n**Moreover, there is another competitor, Clickhouse. Its stand-alone performance is extremely strong, but its disadvantages are hard to accept:**\\n\\n- In some cases, its multi-table JOIN is weak\\n\\n- Relatively low in concurrency\\n\\n- High operation and maintenance costs\\n\\nWith multiple good performances, Apache Doris outstands Druid and TiDB. Meanwhile Clickhouse did not fit well in our business, which lead us to Apache Doris.\\n\\n## Apache Doris-the Unified OLAP Datawarehouse\\n![NIO](/images/olap.png)\\n\\nThis diagram basically describes our OLAP Architecuture, including data source, data import, data processing, data warehouse, data service and application.\\n\\n### 1. Data Source\\n\\nIn NIO, the data source not only refers to database, but also event tracking data, device data, vehicle data, etc. The data will be ingested into the big data platform. \\n### 2. Data Import\\n\\nFor business data, you can trigger CDC and convert it into a data stream, store it in Kafka, and then perform stream processing. Some data that can only be passed in batches will directly enter our distributed storage.\\n\\n### 3. Data Processing\\n\\nWe took the Lambda architecture rather than stream-batch integration.\\n\\nOur own business determines that our Lambda architecture should be divided into two paths: offline and real-time:\\n\\n- Some data is streamed.\\n\\n- Some data can be stored in the data stream, and some historical data will not be stored in Kafka.\\n\\n- Some data requires high precision in some circumstances. In order to ensure the accuracy of the data, an offline pipeline will recalculate and refresh the entire data.\\n\\n### 4. Data Warehouse\\n\\nFrom data processing to the data warehouse, we did not adopt Flink or Spark Doris Connector. We use Routine Load to connect Apache Doris and Flink, and Broker Load to connect Doris and Spark. The data generated in batches by Spark will be backed up to Hive for further use in other scenarios. In this way, each calculation is used for multiple scenarios at the same time, which greatly improves the efficiency. It also works for Flink.\\n\\n### 5. Data Service\\n\\nWhat behind Doris is One Service. By registering the data source or flexible configuration, the API with flow and authority control is automatically generated, which greatly improves flexibility. And with the k8s serverless solution, the entire service is much more flexible.\\n\\n### 6. Application\\n\\nIn the application layer, we mainly deploy some reporting applications and other services.\\n\\nWe mainly have two types of scenarios:\\n\\n- **User-oriented** , which is similar to the Internet, contains a data dashboard and data indicators.\\n\\n- **Car-oriented** , car data enters Doris in this way. After certain aggregation, the volume of Doris data is about billions. But the overall performance can still meet our requirements.\\n\\n## Best Practice of Apache Doris on CDP Architecture\\n\\n### 1. CDP Architecture\\n\\n![NIO](/images/cdp.png)\\n\\nNext, let me introduce Doris\' practice on the operating platform. This is what happens in our real business. Nowadays, Internet companies will make their own CDP, which includes several modules:\\n\\n- **Tags** , which is the most basic part.\\n\\n- **Target** , based on tags, select people according to some certain logic.\\n\\n- **Insight** , aiming at a group of people, clarify the distribution and characteristics of the group.\\n\\n- **Touch** , use methods such as text messages, phone calls, voices, APP notifications, IM, etc. to reach users, and cooperate with flow control.\\n\\n- **Effect analysis,** to improve the integrity of the operation platform, with action, effect and feedback.\\n\\nDoris plays the most important role here, including: tags storage, groups storage, and effect analysis.\\n\\nTags are divided into basic tags and basic data of user behavior. We can flexibly customize other tags based on those facts. From the perspective of time effectiveness, tags are also divided into real-time tags and offline tags.\\n\\n### 2. Considerations for CDP Storage Selection\\n\\nWe took five dimensions into account when we select CDP storage.\\n\\n**(1) Unification of Offline and Real-time**\\n\\nAs mentioned earlier, there are offline tags and real-time tags. Currently we are close to quasi-real-time. For some data, quasi-real-time is good enough to meet our needs. A large number of tags are still offline tags. The methods used are Doris\'s Routine Load and Broker Load.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Real-time tags | Real-time data updates | Routine Load |\\n| Offline tags | Highly efficient batch import | Broker Load |\\n| Unification of offline and real-time | Unification of offline and real-time data storage | Routine Load and Broker Load update different columns of the same table |\\n\\nIn addition, on the same table, the update frequency of different columns is also different. For example, we need to update the user\'s identity in real time because the user\'s identity changes all the time. T+1\'s update does not meet our needs. Some tags are offline, such as the user\'s gender, age and other basic tags, T+1 update is sufficient to meet our standards. The maintenance cost caused by putting the tags of basic users on the same table is very low. When customizing tags later, the number of tables will be greatly reduced, which benefits the overall performance.\\n\\n**(2) Efficient Targets**\\n\\nWhen users tags are done, is time to target right group of people. The target is to filter out all the people who meet the conditions according to different combinations of tags. At this time, there will be queries with different combinations of tag conditions. There was an obvious improvement when Apache Doris upgraded to vectorization.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Complex Condition Targets | Highly efficient combination of tags | Optimization of SIMD |\\n\\n**(3) Efficient Polymerization**\\n\\nThe user insights and effect analysis statistics mentioned above require statistical analysis of the data, which is not a simple thing of obtaining tags by user ID. The amount of data read and query efficiency have a great impact on the distribution of our tags, the distribution of groups, and the statistics of effect analysis. Apache Doris helps a lot:\\n\\n- Data Partition. We shard the data by time order and the analysis and statistics will greatly reduce the amount of data, which can greatly speed up the efficiency of query and analysis.\\n\\n- Node aggregation. Then we collect them for unified aggregation.\\n\\n- Vectorization. The vectorization execution engine has significant performance improvement.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Distribution of Tags Values | The distribution values \u200b\u200bof all tags need to be updated every day. Fast and efficient statistics are required  | Data partition lessens data transfer and calculation |\\n| Distribution of Groups | Same as Above | Unified storage and calculation, each node aggregates first |\\n| Statistics for Performance Analysis | Same as Above | Speed up SIMD\\n\\n\\n**(4) Multi-table Association**\\n\\nOur CDP might be different from common CDP scenarios in the industry, because common CDP tags in some scenarios are estimated in advance and no custom tags, which leaves the flexibility to users who use CDP to customize tags themselves. The underlying data is scattered in different database tables. If you want to create a custom tag, you must associate the tables.\\n\\nA very important reason we chose Doris is the ability to associate multiple tables. Through performance tests, Apache Doris is able to meet our requirements. And Doris provides users with powerful capabilities because tags are dynamic.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Distributed Characteristics of the Population | The distribution of statistical groups under a certain characteristic | Table Association |\\n| Single Tag | Display tags |\\n\\n**(5) Query Federation**\\n\\nWhether the user is successfully reached or not will be recorded in TiDB. Notifications during operations may only affect user experience. If a transaction is involved, such as gift cards or coupons, the task execution must be done without repetition. TiDB is more suitable for this OLTP scenario.\\n\\nBut for effect analysis, it is necessary to understand the extent to which the operation plan is implemented, whether the goal is achieved and its distribution. It is necessary to combine task execution and group selection for analysis, which requires the query association between Doris and TiDB.\\n\\nThe size of the tag is probably small, so we would like to save it into Elasticsearch. However, it proves us wrong later.\\n\\n| **Scenes** | **Requirements** | **Apache Doris\'s Function** |\\n| --- | --- | --- |\\n| Effect Analysis Associated with Execution Details | Doris query associated with TiDB | Query Association with other databases |\\n| Group Tags Associated with Behavior Aggregation | Doris query associated with Elasticsearch |\\n\\n## Summery and Benefits\\n\\n1. **bitmap**. Our volume are not big enough to test its full efficiency. If the volume reaches a certain level, using bitmap might have a good performance improvement. For example, when calculating UV , bitmap aggregation can be considered if the full set of Ids is greater than 50 million.\\n\\n2. **The performance is good** when Elasticsearch single-table query is associated with Doris.\\n\\n3. **Better to update columns in batches**. In order to reduce the number of tables and improve the performance of the JOIN table, the table designed should be as streamlined as possible and aggregated as much as possible. However, fields of the same type may have different update frequencies. Some fields need to be updated at daily level, while others may need to be updated at hourly level. Updating a column alone is an important requirement. The solution from Apache Doris is to use REPLACE\\\\_IF\\\\_NOT\\\\_NULL. Note: It is impossible to replace the original non-null value with null. You can replace all nulls with meaningful default values, such as unknown.\\n\\n4. **Online Services**. Apache Doris serves online and offline scenarios at the same time, which requires high resource isolation."},{"id":"/scenario","metadata":{"permalink":"/blog/scenario","source":"@site/blog/scenario.md","title":"How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario","description":"\x3c!--","date":"2022-11-24T00:00:00.000Z","formattedDate":"November 24, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Zhao Wei"}],"frontMatter":{"title":"How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario","summary":"Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.","date":"2022-11-24","author":"Zhao Wei","tags":["Best Practice"]},"prevItem":{"title":"The Application of Apache Doris in NIO","permalink":"/blog/NIO"},"nextItem":{"title":"Apache Doris 1.2 Star-Schema-Benchmark Performance Test Report","permalink":"/blog/ssb"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario\\n\\n![kv](/images/kv.png)\\n\\n>Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.\\n\\nAuthor\uff5cZhao Wei, Head Developer of AISPEACH\'s Big Data Departpment\\n\\n## Backgounds\\n\\nAISPEACH is a professional conversational artificial intelligence company in China. It has full-link intelligent voice and language technology. It is committed to becoming a platform-based enterprise for full-link intelligent voice and language interaction. Recently it has developed a new generation of human-computer interaction platform DUI and artificial intelligence chip TH1520, providing natural language interaction solutions for partners in many industry scenarios such as Internet of Vehicles, IoT, government affairs and fintech.\\n\\nAspire introduced Apache Doris for the first time in 2019 and built a real-time and offline data warehouse based on Apache Doris. Compared with the previous architecture, Apache Doris has many advantages such as flexible query model, extremely low maintenance cost, high development efficiency and excellent query performance. Multiple business scenarios have been applied to meet various data analysis needs such as device portraits/user tags, real-time operation of business scenarios, data analysis dashboards, self-service BI, and financial reconciliation.\\n\\n## Architecture Evolution\\n\\nOffline data analysis in the early business was our main requirement. Recently, with the continuous development of business, the requirements for real-time data analysis in business scenarios have become higher and higher. The early datawarehouse architecture failed to meet our requirements. In order to meet the higher requirements of business scenarios for query performance, response time, and concurrency capabilities, Apache Doris was officially introduced in 2019 to build a real-time and offline integrated datawarehouse architecture.\\n\\nIn the following I will introduce the evolution of the AISPEACH Data Warehouse architecture, and share the reasons why we chose Apache Doris to build a new architecture.\\n\\n### Early Data Warehouse Architecture\\n\\nAs shown in the architecture diagram below, the offline data warehouse is based on Hive + Kylin while the real-time data warehouse is based on Spark + MySQL.\\n\\n![data_wharehouse_architecture_v1_0_git](/images/data_wharehouse_architecture_v1_0_git.png)\\n\\nThere are three main types of data sources in our business, business databases such as MySQL, application systems such as K8s container service logs, and logs of automotive T-Box. Data sources are first written to Kafka through various methods such as MQTT/HTTP protocol, business database Binlog, and Filebeat log collection. In the early time, the data will be divided into real-time and offline links after passing through Kafka. Real-time part has a shorter link. The data buffered by Kafka is processed by Spark and put into MySQL for further analysis. MySQL can basically meet the early analysis requirements. After data cleaning and processing by Spark, an offline datawarehouse is built in Hive, and Apache Kylin is used to build Cube. Before building Cube, it is necessary to design the data model in advance, including association tables, dimension tables, index fields, and aggregation functions. After construction through the scheduling system, we can finally use HBase to store the Cube.\\n\\n#### Pain Points of Early Architecture\uff1a\\n\\n1. **There are many dependent components.** Kylin strongly relies on Hadoop and HBase in versions 2.x and 3.x. The large number of application components leads to low development efficiency, many hidden dangers of architecture stability, and high maintenance costs.\\n\\n2. **The construction process of Kylin is complicated and the construction task always fail.** When we do construction for Kylin, we always need to do the following: widen tables, de-duplicate columns, generate dictionaries, build cubes, etc. If there are 1000-2000 or more tasks per day, at least 10 or more tasks will fail to build, resulting in a lot of time to write automatic operation and maintenance scripts.\\n\\n3. **Dimension/dictionary expansion is heavy.** Dimension expansion refers to the need for multiple analysis conditions and fields in some business scenarios. If many fields are selected in the data analysis model without pruning, it will lead to severe cube dimension expansion and longer construction time. Dictionary inflation means that in some scenarios, it takes a long time to do global accurate deduplication, which will make the dictionary construction bigger and bigger, and the construction time will become longer and longer, resulting in a continuous decline in data analysis performance.\\n\\n4. **The data analysis model is fixed and low in flexibility.** In the actual application, if a calculation field or business scenario is changed, some or even all of the data needs to be backtracked.\\n\\n5. **Data detail query is not supported.** The early data warehouse architecture could not provide detailed data query. The official Kylin solution is to relate to Presto for detailed query, which introduces another architecture and increases development costs.\\n\\n### Architecture Selection\\n\\nIn order to solve the problems above, we began to explore other datawarehouse architecture solutions. And we conducted a series of research on OLAP engines such as Apache Doris and Clickhouse, which are most widely used in the market.\\n\\nAs the original creator, SelectDB provides commercial support and services for Apache Doris. With the new Apache Doris, SelectDB is now providing global users with a fully-managed database option for deployment.\\n\\nComparing with ClickHouse\'s heavy maintenance, various table types, and lack of support for associated queries, Apache Doris performed better. And combined with our OLAP analysis scenario, we finally decided to introduce Apache Doris.\\n\\n### New Data Warehouse Architecture\\n\\n![data_wharehouse_architecture_v2_0_git](/images/data_wharehouse_architecture_v2_0_git.png)\\n\\nAs shown in the figure above, we built a new real-time + offline data warehouse architecture based on Apache Doris. Unlike the previous architecture, real-time and offline data are processed separately and written to Apache Doris for analysis.\\n\\nDue to some historical reasons, data migration is difficult. The offline data is basically consistent with the previous datawarehouse architecture, and it is entirely possible to directly build an offline data warehouse on Apache Doris.\\n\\nComparing with the earlier architecture, the offline data is cleaned and processed by Spark, which is possible to build data warehouse in Hive. Then the data stored in Hive can be written to Apache Doris through Broker Load. What I want to explain here is that the data import speed of Broker Load is very fast and it only takes 10-20 minutes to import 100-200G data into Apache Doris on a daily basis.\\n\\nWhen it comes to the real-time data flow, the new architecture uses Doris-Spark-Connector to consume data in Kafka and write it to Apache Doris after simple tasks. As shown in the architecture diagram, real-time and offline data are analyzed and processed in Apache Doris, which meets the business requirements of data applications for both real-time and offline.\\n\\n#### Benefits of the New Architecture:\\n\\n1. **Simplified operation, low maintenance cost, and does not depend on Hadoop ecological components.** The deployment of Apache Doris is simple. There are only two processes of FE and BE. Both FE and BE processes can be scaled out. A single cluster supports hundreds of machines and tens of PB storage capacity. These two types of processes pass the consistency agreement to ensure high availability of services and high reliability of data. This highly integrated architecture design greatly reduces the operation and maintenance cost of a distributed system. The operation and maintenance time spent in the three years of using Doris is very small. Comparing with the previous architecture based on Kylin, the new architecture spends little time on operation and maintenance.\\n\\n2. **The difficulty of developing and troubleshooting problems is greatly reduced.** The real-time and offline unified data warehouse based on Doris supports real-time data services, interactive data analysis, and offline data processing scenarios, which greatly reduces the difficulty of troubleshooting.\\n\\n3. **Apache Doris supports JOIN query in Runtime format.** Runtime is similar to MySQL\'s table association, which is friendly to the scene where the data analysis model changes frequently, and solves the problem of low flexibility in the early structured data model.\\n\\n4. **Apache Doris supports JOIN, aggregation, and detailed query at the same time.** Meanwhile, it solves the problem that data details could not be queried in the previous architecture.\\n\\n5. **Apache Doris supports multiple accelerated query methods.** And it also supports rollup index, materialized view, and implements secondary index through rollup index to speed up query, which greatly improves query response time.\\n\\n6. **Apache Doris supports multiple types of Query Federation.** And it supports Federation Query analysis on data lakes such as Hive, Iceberg, and Hudi, and also databases such as MySQL and Elasticsearch.\\n\\n## Applications\\n\\nApache Doris was first applied in real-time business and AI Chatbots analysis scenarios in AISPEACH. This chapter will introduce the requirements and applications of the two scenarios.\\n\\n### Real-time Business\\n\\n![real-time_operation_git](/images/real-time_operation_git.png)\\n\\nAs shown in the figure above, the technical architecture of the real-time operation business is basically the same as the new version of the data warehouse architecture mentioned above:\\n\\n- Data Source: The data source is consistent in the new version with the architecture diagram in the new version, including business data in MySQL, event tracking data of the application system, device and terminal logs.\\n\\n- Data Import: Broker Load is used for offline data import, and Doris-Spark-Connector is used for real-time data import.\\n\\n- Data Storage and Development: Almost all real-time data warehouses are built on Apache Doris, and some offline data is placed on Airflow to perform DAG batch tasks.\\n\\n- Data Application: The top layer is the business analysis requirements, including large-screen display, real-time dashboard for data operation, user portrait, BI tools, etc.\\n\\n**In real-time operation business, there are two main requirements for data analysis:**\\n\\n- Due to the large amount of real-time imported data, the query efficiency requirement is high.\\n\\n- In this scenario, a team of 20+ people is in charge. The data operation dashboard needs to be opened at the same time, so there will be relatively high requirements for real-time writing performance and query concurrency.\\n\\n### AI Chatbots Analysis\\n\\nIn addition, the second application of Apache Doris in AISPEACG is a AI Chatbots analysis.\\n\\n![ai_chatbots_git](/images/ai_chatbots_git.png)\\n\\nAs shown in the figure above, different from normal BI cases, our users only needs to describe the data analysis needs by typing. Based on our company\'s NLP capabilities, AI Chatbots BI will convert natural language into SQL, which similar to NL2SQL technology. It should be noted that the natural language analysis used here is customized. Comparing with open source NL2SQL, the hit rate is high and the analysis is more precise. After the natural language is converted into SQL, the SQL will give Apache Doris query to get the analysis result. As a result, users can view detailed data in any cases at any time by typing. **Compared with pre-computed OLAP engines such as Apache Kylin and Apache Druid, Apache Doris performs better for the following reasons:**\\n\\n- The query is flexible and the model is not fixed, which supports customization.\\n\\n- It needs to support table association, aggregation calculation, and detailed query.\\n\\n- Response time needs to be fast.\\n\\nTherefore, we have successfully implemented AI Chatbots analysis by using Apache Doris. At the same time, feedback on the application in our company is awesome.\\n\\n## Experience\\n\\nBased on the above two scenarios, we have accumulated some experience and insights and I will share them with you now.\\n\\n### Datawarehouse Table Design:\\n\\n1. Tables which contain about tens of millions of data(for reference, related to the size of the cluster) is better to use the Duplicate table type. The Duplicate table type supports aggregation and detailed query at the same time, without additional detailed tables required.\\n\\n2. When the amount of data is relatively large, we suggest to use the Aggregate aggregation table type, build a rollup index on the aggregation table type, use materialized views to optimize queries, and optimize aggregation fields.\\n\\n3. When the amount of data is large with many associated tables, ETL can be used to write wide tables, imports to Doris, combined with Aggregate to optimize the aggregation table type. Or we suggest you use the official Doris JOIN optimization refer to: https://doris .apache.org/en-US/docs/dev/advanced/join-optimization/doris-join-optimization\\n\\n### Storage:\\n\\nWe use SSD and HDD to separate hot and warm data storage. Data within the past year is stored in SSD, and data more than one year is stored in HDD. Apache Doris supports setting cooling time for partitions. The current solution is to set automatic synchronization to migrate historical data from SSD to HDD to ensure that the data within one year is placed in on the SSD.\\n\\n### Upgrade\\n\\nMake sure to back up the metadata before upgrading. You can also use the method of starting a new cluster to back up the data files to a remote storage system such as S3 or HDFS through Broker, and then import the previous cluster data into the new cluster through backup and recovery.\\n\\n### Performance Comparison\\n\\nAspire started using Apache Doris from version 0.12. This year we completed the upgrade from version 0.15 to the latest version 1.1, and conducted performance tests based on real business data.\\n\\n![doris_1_1_performance_test_git](/images/doris_1_1_performance_test_git.png)\\n\\nAs can be seen from the test report, among the 13 SQLs test in total, the performance difference of the first 3 SQLs after the upgrade is not obvious, because these 3 scenarios are mainly simple aggregation functions, which do not require high performance of Apache Doris. Version 0.15 can meet demand. In the scenario after Q4, SQL is more complex while Group By needs multiple fields, aggregation functions and complex functions. Therefore, the performance improvement after upgrading is obvious to see: the average query performance is 2- 3 times. We highly recommend that you upgrade to the latest version of Apache Doris.\\n\\n## Summary and Benefits\\n\\n1. Apache Doris supports the construction of offline plus real-time unified data warehouses. One ETL script can support both real-time and offline data warehouses, which greatly greatly improved efficiency, reduces storage costs, and avoids problems such as inconsistencies between offline and real-time indicators.\\n\\n2. Apache Doris 1.1.x version fully supports vectorization, which improves the query performance by 2-3 times compared with the previous version. After testing, the query performance of Apache Doris version 1.1.x in the wide table is equal to that of ClickHouse.\\n\\n3. Apache Doris is powerful and does not depend on other components. Compared with Apache Kylin, Apache Druid, ClickHouse, Apache Doris does not need a second component to fill the technical gap. Apache Doris supports aggregation, detailed queries, and associated queries. Currently, more than 90% of AISPEACH\' analysis have migrated to Apache Doris. Thanks to this advantage, developers operate and maintain fewer components, which greatly reduces the cost of operation and maintenance.\\n\\n4. It is extremely easy to use, supporting MySQL protocol and standard SQL, which greatly reduces user learning costs.\\n\\n_Special thanks to SelectDB, the company building Apache Doris helps us work with the community and get sufficient technical support._"},{"id":"/ssb","metadata":{"permalink":"/blog/ssb","source":"@site/blog/ssb.md","title":"Apache Doris 1.2 Star-Schema-Benchmark Performance Test Report","description":"\x3c!--","date":"2022-11-22T00:00:00.000Z","formattedDate":"November 22, 2022","tags":[{"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris 1.2 Star-Schema-Benchmark Performance Test Report","summary":"On the SSB flat wide table, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 4 times compared with Apache Doris 1.1.3, and nearly 10 times compared with Apache Doris 0.15.0 RC04. On the SQL test with standard SSB, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 2 times compared with Apache Doris 1.1.3, and nearly 31 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22","author":"Apache Doris","tags":["Tech Sharing"]},"prevItem":{"title":"How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario","permalink":"/blog/scenario"},"nextItem":{"title":"Apache Doris 1.2 TPC-H Performance Test Report","permalink":"/blog/tpch"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Star Schema Benchmark\\n\\n[Star Schema Benchmark(SSB)](https://www.cs.umb.edu/~poneil/StarSchemaB.PDF) is a lightweight performance test set in the data warehouse scenario. SSB provides a simplified star schema data based on [TPC-H](http://www.tpc.org/tpch/), which is mainly used to test the performance of multi-table JOIN query under star schema.  In addition, the industry usually flattens SSB into a wide table model (Referred as: SSB flat) to test the performance of the query engine, refer to [Clickhouse](https://clickhouse.com/docs/zh/getting-started).\\n\\nThis document mainly introduces the performance of Doris on the SSB 100G test set.\\n\\n> Note 1: The standard test set including SSB usually has a large gap with the actual business scenario, and some tests will perform parameter tuning for the test set. Therefore, the test results of the standard test set can only reflect the performance of the database in a specific scenario. It is recommended that users use actual business data for further testing.\\n>\\n> Note 2: The operations involved in this document are all performed in the Ubuntu Server 20.04 environment, and CentOS 7 as well.\\n\\nWith 13 queries on the SSB standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions.\\n\\nOn the SSB flat wide table, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 4 times compared with Apache Doris 1.1.3, and nearly 10 times compared with Apache Doris 0.15.0 RC04.\\n\\nOn the SQL test with standard SSB, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 2 times compared with Apache Doris 1.1.3, and nearly 31 times compared with Apache Doris 0.15.0 RC04.\\n\\n## 1. Hardware Environment\\n\\n| Number of machines | 4 Tencent Cloud Hosts (1 FE, 3 BEs)        |\\n| ------------------ | ----------------------------------------- |\\n| CPU                | AMD EPYC\u2122 Milan (2.55GHz/3.5GHz) 16 Cores |\\n| Memory             | 64G                                       |\\n| Network Bandwidth  | 7Gbps                                     |\\n| Disk               | High-performance Cloud Disk               |\\n\\n## 2. Software Environment\\n\\n- Doris deployed 3BEs and 1FE;\\n- Kernel version: Linux version 5.4.0-96-generic (buildd@lgw01-amd64-051)\\n- OS version: Ubuntu Server 20.04 LTS 64-bit\\n- Doris software versions: Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04\\n- JDK: openjdk version \\"11.0.14\\" 2022-01-18\\n\\n## 3. Test Data Volume\\n\\n| SSB Table Name | Rows | Annotation                          |\\n| :------------- | :------------- | :------------------------------- |\\n| lineorder      | 600,037,902    | Commodity Order Details             |\\n| customer       | 3,000,000      | Customer Information        |\\n| part           | 1,400,000      | Parts Information          |\\n| supplier       | 200,000        | Supplier Information        |\\n| date           | 2,556          | Date                        |\\n| lineorder_flat | 600,037,902    | Wide Table after Data Flattening |\\n\\n## 4. Test Results\\n\\nWe use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. The test results are as follows:\\n\\n| Query | Apache Doris 1.2.0-rc01(ms) | Apache Doris 1.1.3 (ms) |  Doris 0.15.0 RC04 (ms) |\\n| ----- | ------------- | ------------- | ----------------- |\\n| Q1.1  | 20            | 90            | 250               |\\n| Q1.2  | 10            | 10            | 30                |\\n| Q1.3  | 30            | 70            | 120               |\\n| Q2.1  | 90            | 360           | 900               |\\n| Q2.2  | 90            | 340           | 1,020              |\\n| Q2.3  | 60            | 260           | 770               |\\n| Q3.1  | 160           | 550           | 1,710              |\\n| Q3.2  | 80            | 290           | 670               |\\n| Q3.3  | 90            | 240           | 550               |\\n| Q3.4  | 20            | 20            | 30                |\\n| Q4.1  | 140           | 480           | 1,250              |\\n| Q4.2  | 50            | 240           | 400               |\\n| Q4.3  | 30            | 200           | 330               |\\n| Total  | 880           | 3,150          | 8,030              |\\n\\n![ssb_v11_v015_compare](/images/ssb_flat.png)\\n\\n**Interpretation of Results**\\n\\n- The data set corresponding to the test results is scale 100, about 600 million.\\n- The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE, 3 BEs deployment.\\n- We select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n\\n\\n## 5. Standard SSB Test Results\\n\\nHere we use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. In the test, we use Query Time\uff08ms\uff09 as the main performance indicator. The test results are as follows:\\n\\n| Query | Apache Doris 1.2.0-rc01 (ms) | Apache Doris 1.1.3 (ms) | Doris 0.15.0 RC04 (ms) |\\n| ----- | ------- | ---------------------- | ------------------------------- |\\n| Q1.1  | 40      | 18                    | 350                           |\\n| Q1.2  | 30      | 100                    | 80                             |\\n| Q1.3  | 20      | 70                     | 80                            |\\n| Q2.1  | 350     | 940                  | 20,680                     |\\n| Q2.2  | 320     | 750                  | 18,250                    |\\n| Q2.3  | 300     | 720                  | 14,760                   |\\n| Q3.1  | 650     | 2,150                 | 22,190                   |\\n| Q3.2  | 260     | 510                 | 8,360                          |\\n| Q3.3  | 220     | 450                  | 6,200                        |\\n| Q3.4  | 60      | 70                   | 160                            |\\n| Q4.1  | 840     | 1,480                   | 24,320                      |\\n| Q4.2  | 460     | 560                 | 6,310                          |\\n| Q4.3  | 610     | 660                  | 10,170                    |\\n| Total  | 4,160    | 8,478                | 131,910 |\\n\\n![ssb_12_11_015](/images/ssb.png)\\n\\n**Interpretation of Results**\\n\\n- The data set corresponding to the test results is scale 100, about 600 million.\\n- The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE 3 BEs deployment.\\n- We select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n\\n## 6. Environment Preparation\\n\\nPlease first refer to the [official documentation](. /install/install-deploy.md) to install and deploy Apache Doris first to obtain a Doris cluster which is working well(including at least 1 FE 1 BE, 1 FE 3 BEs is recommended).\\n\\nThe scripts mentioned in the following documents are stored in the Apache Doris codebase: [ssb-tools](https://github.com/apache/doris/tree/master/tools/ssb-tools)\\n\\n## 7. Data Preparation\\n\\n### 7.1 Download and Install the SSB Data Generation Tool.\\n\\nExecute the following script to download and compile the [ssb-dbgen](https://github.com/electrum/ssb-dbgen.git) tool.\\n\\n```shell\\nsh build-ssb-dbgen.sh\\n````\\n\\nAfter successful installation, the `dbgen` binary will be generated under the `ssb-dbgen/` directory.\\n\\n### 7.2 Generate SSB Test Set\\n\\nExecute the following script to generate the SSB dataset:\\n\\n```shell\\nsh gen-ssb-data.sh -s 100 -c 100\\n````\\n\\n> Note 1: Check the script help via `sh gen-ssb-data.sh -h`.\\n>\\n> Note 2: The data will be generated under the `ssb-data/` directory with the suffix `.tbl`. The total file size is about 60GB and may need a few minutes to an hour to generate.\\n>\\n> Note 3: `-s 100` indicates that the test set size factor is 100, `-c 100` indicates that 100 concurrent threads generate the data of the lineorder table. The `-c` parameter also determines the number of files in the final lineorder table. The larger the parameter, the larger the number of files and the smaller each file.\\n\\nWith the `-s 100` parameter, the resulting dataset size is:\\n\\n| Table     | Rows             | Size | File Number |\\n| --------- | ---------------- | ---- | ----------- |\\n| lineorder | 600,037,902 | 60GB | 100         |\\n| customer  | 3,000,000 | 277M | 1           |\\n| part      | 1,400,000 | 116M | 1           |\\n| supplier  | 200,000   | 17M  | 1           |\\n| date      | 2,556             | 228K | 1           |\\n\\n### 7.3 Create Table\\n\\n#### 7.3.1 Prepare the `doris-cluster.conf` File.\\n\\nBefore import the script, you need to write the FE\u2019s ip port and other information in the `doris-cluster.conf` file.\\n\\nThe file location is at the same level as `load-ssb-dimension-data.sh`.\\n\\nThe content of the file includes FE\'s ip, HTTP port, user name, password and the DB name of the data to be imported:\\n\\n```shell\\nexport FE_HOST=\\"xxx\\"\\nexport FE_HTTP_PORT=\\"8030\\"\\nexport FE_QUERY_PORT=\\"9030\\"\\nexport USER=\\"root\\"\\nexport PASSWORD=\'xxx\'\\nexport DB=\\"ssb\\"\\n```\\n\\n#### 7.3.2 Execute the Following Script to Generate and Create the SSB Table:\\n\\n```shell\\nsh create-ssb-tables.sh\\n````\\n\\nOr copy the table creation statements in [create-ssb-tables.sql](https://github.com/apache/incubator-doris/tree/master/tools/ssb-tools/ddl/create-ssb-tables.sql) and [ create-ssb-flat-table.sql](https://github.com/apache/incubator-doris/tree/master/tools/ssb-tools/ddl/create-ssb-flat-table.sql) and then execute them in the MySQL client.\\n\\nThe following is the `lineorder_flat` table build statement. Create the `lineorder_flat` table in the above `create-ssb-flat-table.sh` script, and perform the default number of buckets (48 buckets). You can delete this table and adjust the number of buckets according to your cluster scale node configuration, so as to obtain a better test result.\\n\\n```sql\\nCREATE TABLE `lineorder_flat` (\\n  `LO_ORDERDATE` date NOT NULL COMMENT \\"\\",\\n  `LO_ORDERKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_LINENUMBER` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_CUSTKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_PARTKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_SUPPKEY` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_ORDERPRIORITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `LO_SHIPPRIORITY` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_QUANTITY` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_EXTENDEDPRICE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_ORDTOTALPRICE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_DISCOUNT` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_REVENUE` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_SUPPLYCOST` int(11) NOT NULL COMMENT \\"\\",\\n  `LO_TAX` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `LO_COMMITDATE` date NOT NULL COMMENT \\"\\",\\n  `LO_SHIPMODE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_ADDRESS` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_CITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_NATION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_REGION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_PHONE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `C_MKTSEGMENT` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_ADDRESS` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_CITY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_NATION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_REGION` varchar(100) NOT NULL COMMENT \\"\\",\\n  `S_PHONE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_NAME` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_MFGR` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_CATEGORY` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_BRAND` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_COLOR` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_TYPE` varchar(100) NOT NULL COMMENT \\"\\",\\n  `P_SIZE` tinyint(4) NOT NULL COMMENT \\"\\",\\n  `P_CONTAINER` varchar(100) NOT NULL COMMENT \\"\\"\\n) ENGINE=OLAP\\nDUPLICATE KEY(`LO_ORDERDATE`, `LO_ORDERKEY`)\\nCOMMENT \\"OLAP\\"\\nPARTITION BY RANGE(`LO_ORDERDATE`)\\n(PARTITION p1 VALUES [(\'0000-01-01\'), (\'1993-01-01\')),\\nPARTITION p2 VALUES [(\'1993-01-01\'), (\'1994-01-01\')),\\nPARTITION p3 VALUES [(\'1994-01-01\'), (\'1995-01-01\')),\\nPARTITION p4 VALUES [(\'1995-01-01\'), (\'1996-01-01\')),\\nPARTITION p5 VALUES [(\'1996-01-01\'), (\'1997-01-01\')),\\nPARTITION p6 VALUES [(\'1997-01-01\'), (\'1998-01-01\')),\\nPARTITION p7 VALUES [(\'1998-01-01\'), (\'1999-01-01\')))\\nDISTRIBUTED BY HASH(`LO_ORDERKEY`) BUCKETS 48\\nPROPERTIES (\\n\\"replication_num\\" = \\"1\\",\\n\\"colocate_with\\" = \\"groupxx1\\",\\n\\"in_memory\\" = \\"false\\",\\n\\"storage_format\\" = \\"DEFAULT\\"\\n);\\n```\\n\\n### 7.4 Import data\\n\\nWe use the following command to complete all data import of SSB test set and SSB FLAT wide table data synthesis and then import into the table.\\n\\n```shell\\n sh bin/load-ssb-data.sh -c 10\\n```\\n\\n`-c 5` means start 10 concurrent threads to import (5 by default). In the case of a single BE node, the lineorder data generated by `sh gen-ssb-data.sh -s 100 -c 100` will also generate the data of the ssb-flat table in the end. If more threads are enabled, the import speed can be accelerated. But it will cost extra memory.\\n\\n> Notes.\\n>\\n> 1. To get faster import speed, you can add `flush_thread_num_per_store=5` in be.conf and then restart BE. This configuration indicates the number of disk writing threads for each data directory, 2 by default. Larger data can improve write data throughput, but may increase IO Util. (Reference value: 1 mechanical disk, with 2 by default, the IO Util during the import process is about 12%. When it is set to 5, the IO Util is about 26%. If it is an SSD disk, it is almost 0%) .\\n>\\n> 2. The flat table data is imported by \'INSERT INTO ... SELECT ... \'.\\n\\n### 7.5 Checking Imported data\\n\\n\\n```sql\\nselect count(*) from part;\\nselect count(*) from customer;\\nselect count(*) from supplier;\\nselect count(*) from date;\\nselect count(*) from lineorder;\\nselect count(*) from lineorder_flat;\\n```\\n\\nThe amount of data should be consistent with the number of rows of generated data.\\n\\n| Table          | Rows             | Origin Size | Compacted Size(1 Replica) |\\n| -------------- | ---------------- | ----------- | ------------------------- |\\n| lineorder_flat | 600,037,902 |             | 59.709 GB                 |\\n| lineorder      | 600,037,902 | 60 GB       | 14.514 GB                 |\\n| customer       | 3,000,000 | 277 MB      | 138.247 MB                |\\n| part           | 1,400,000 | 116 MB      | 12.759 MB                 |\\n| supplier       | 200,000   | 17 MB       | 9.143 MB                  |\\n| date           | 2,556             | 228 KB      | 34.276 KB                 |\\n\\n### 7.6 Query Test\\n\\n- SSB-Flat Query Statement: [ ssb-flat-queries](https://github.com/apache/doris/tree/master/tools/ssb-tools/ssb-flat-queries)\\n- Standard SSB Queries: [ ssb-queries](https://github.com/apache/doris/tree/master/tools/ssb-tools/ssb-queries)\\n\\n#### 7.6.1 SSB FLAT Test for SQL\\n\\n```sql\\n--Q1.1\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE  LO_ORDERDATE >= 19930101  AND LO_ORDERDATE <= 19931231 AND LO_DISCOUNT BETWEEN 1 AND 3  AND LO_QUANTITY < 25;\\n--Q1.2\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE LO_ORDERDATE >= 19940101 AND LO_ORDERDATE <= 19940131  AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35;\\n\\n--Q1.3\\nSELECT SUM(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue\\nFROM lineorder_flat\\nWHERE  weekofyear(LO_ORDERDATE) = 6 AND LO_ORDERDATE >= 19940101  AND LO_ORDERDATE <= 19941231 AND LO_DISCOUNT BETWEEN 5 AND 7  AND LO_QUANTITY BETWEEN 26 AND 35;\\n\\n--Q2.1\\nSELECT SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat WHERE P_CATEGORY = \'MFGR#12\' AND S_REGION = \'AMERICA\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q2.2\\nSELECT  SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat\\nWHERE P_BRAND >= \'MFGR#2221\' AND P_BRAND <= \'MFGR#2228\'  AND S_REGION = \'ASIA\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q2.3\\nSELECT SUM(LO_REVENUE), (LO_ORDERDATE DIV 10000) AS YEAR, P_BRAND\\nFROM lineorder_flat\\nWHERE P_BRAND = \'MFGR#2239\' AND S_REGION = \'EUROPE\'\\nGROUP BY YEAR, P_BRAND\\nORDER BY YEAR, P_BRAND;\\n\\n--Q3.1\\nSELECT C_NATION, S_NATION, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_REGION = \'ASIA\' AND S_REGION = \'ASIA\' AND LO_ORDERDATE >= 19920101  AND LO_ORDERDATE <= 19971231\\nGROUP BY C_NATION, S_NATION, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.2\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_NATION = \'UNITED STATES\' AND S_NATION = \'UNITED STATES\' AND LO_ORDERDATE >= 19920101 AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.3\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND S_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND LO_ORDERDATE >= 19920101 AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q3.4\\nSELECT C_CITY, S_CITY, (LO_ORDERDATE DIV 10000) AS YEAR, SUM(LO_REVENUE) AS revenue\\nFROM lineorder_flat\\nWHERE C_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND S_CITY IN (\'UNITED KI1\', \'UNITED KI5\') AND LO_ORDERDATE >= 19971201  AND LO_ORDERDATE <= 19971231\\nGROUP BY C_CITY, S_CITY, YEAR\\nORDER BY YEAR ASC, revenue DESC;\\n\\n--Q4.1\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR, C_NATION, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE C_REGION = \'AMERICA\' AND S_REGION = \'AMERICA\' AND P_MFGR IN (\'MFGR#1\', \'MFGR#2\')\\nGROUP BY YEAR, C_NATION\\nORDER BY YEAR ASC, C_NATION ASC;\\n\\n--Q4.2\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR,S_NATION, P_CATEGORY, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE C_REGION = \'AMERICA\' AND S_REGION = \'AMERICA\' AND LO_ORDERDATE >= 19970101 AND LO_ORDERDATE <= 19981231 AND P_MFGR IN (\'MFGR#1\', \'MFGR#2\')\\nGROUP BY YEAR, S_NATION, P_CATEGORY\\nORDER BY YEAR ASC, S_NATION ASC, P_CATEGORY ASC;\\n\\n--Q4.3\\nSELECT (LO_ORDERDATE DIV 10000) AS YEAR, S_CITY, P_BRAND, SUM(LO_REVENUE - LO_SUPPLYCOST) AS profit\\nFROM lineorder_flat\\nWHERE S_NATION = \'UNITED STATES\' AND LO_ORDERDATE >= 19970101 AND LO_ORDERDATE <= 19981231 AND P_CATEGORY = \'MFGR#14\'\\nGROUP BY YEAR, S_CITY, P_BRAND\\nORDER BY YEAR ASC, S_CITY ASC, P_BRAND ASC;\\n```\\n\\n#### 7.6.2 SSB Standard Test for SQL\\n\\n```SQL\\n--Q1.1\\nSELECT SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_year = 1993\\n    AND lo_discount BETWEEN 1 AND 3\\n    AND lo_quantity < 25;\\n--Q1.2\\nSELECT SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_yearmonth = \'Jan1994\'\\n    AND lo_discount BETWEEN 4 AND 6\\n    AND lo_quantity BETWEEN 26 AND 35;\\n    \\n--Q1.3\\nSELECT\\n    SUM(lo_extendedprice * lo_discount) AS REVENUE\\nFROM lineorder, dates\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND d_weeknuminyear = 6\\n    AND d_year = 1994\\n    AND lo_discount BETWEEN 5 AND 7\\n    AND lo_quantity BETWEEN 26 AND 35;\\n    \\n--Q2.1\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_category = \'MFGR#12\'\\n    AND s_region = \'AMERICA\'\\nGROUP BY d_year, p_brand\\nORDER BY p_brand;\\n\\n--Q2.2\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_brand BETWEEN \'MFGR#2221\' AND \'MFGR#2228\'\\n    AND s_region = \'ASIA\'\\nGROUP BY d_year, p_brand\\nORDER BY d_year, p_brand;\\n\\n--Q2.3\\nSELECT SUM(lo_revenue), d_year, p_brand\\nFROM lineorder, dates, part, supplier\\nWHERE\\n    lo_orderdate = d_datekey\\n    AND lo_partkey = p_partkey\\n    AND lo_suppkey = s_suppkey\\n    AND p_brand = \'MFGR#2239\'\\n    AND s_region = \'EUROPE\'\\nGROUP BY d_year, p_brand\\nORDER BY d_year, p_brand;\\n\\n--Q3.1\\nSELECT\\n    c_nation,\\n    s_nation,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'ASIA\'\\n    AND s_region = \'ASIA\'\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_nation, s_nation, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.2\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND c_nation = \'UNITED STATES\'\\n    AND s_nation = \'UNITED STATES\'\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.3\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND (\\n        c_city = \'UNITED KI1\'\\n        OR c_city = \'UNITED KI5\'\\n    )\\n    AND (\\n        s_city = \'UNITED KI1\'\\n        OR s_city = \'UNITED KI5\'\\n    )\\n    AND d_year >= 1992\\n    AND d_year <= 1997\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q3.4\\nSELECT\\n    c_city,\\n    s_city,\\n    d_year,\\n    SUM(lo_revenue) AS REVENUE\\nFROM customer, lineorder, supplier, dates\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_orderdate = d_datekey\\n    AND (\\n        c_city = \'UNITED KI1\'\\n        OR c_city = \'UNITED KI5\'\\n    )\\n    AND (\\n        s_city = \'UNITED KI1\'\\n        OR s_city = \'UNITED KI5\'\\n    )\\n    AND d_yearmonth = \'Dec1997\'\\nGROUP BY c_city, s_city, d_year\\nORDER BY d_year ASC, REVENUE DESC;\\n\\n--Q4.1\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    d_year,\\n    c_nation,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'AMERICA\'\\n    AND s_region = \'AMERICA\'\\n    AND (\\n        p_mfgr = \'MFGR#1\'\\n        OR p_mfgr = \'MFGR#2\'\\n    )\\nGROUP BY d_year, c_nation\\nORDER BY d_year, c_nation;\\n\\n--Q4.2\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */  \\n    d_year,\\n    s_nation,\\n    p_category,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND c_region = \'AMERICA\'\\n    AND s_region = \'AMERICA\'\\n    AND (\\n        d_year = 1997\\n        OR d_year = 1998\\n    )\\n    AND (\\n        p_mfgr = \'MFGR#1\'\\n        OR p_mfgr = \'MFGR#2\'\\n    )\\nGROUP BY d_year, s_nation, p_category\\nORDER BY d_year, s_nation, p_category;\\n\\n--Q4.3\\nSELECT /*+SET_VAR(parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    d_year,\\n    s_city,\\n    p_brand,\\n    SUM(lo_revenue - lo_supplycost) AS PROFIT\\nFROM dates, customer, supplier, part, lineorder\\nWHERE\\n    lo_custkey = c_custkey\\n    AND lo_suppkey = s_suppkey\\n    AND lo_partkey = p_partkey\\n    AND lo_orderdate = d_datekey\\n    AND s_nation = \'UNITED STATES\'\\n    AND (\\n        d_year = 1997\\n        OR d_year = 1998\\n    )\\n    AND p_category = \'MFGR#14\'\\nGROUP BY d_year, s_city, p_brand\\nORDER BY d_year, s_city, p_brand;\\n```"},{"id":"/tpch","metadata":{"permalink":"/blog/tpch","source":"@site/blog/tpch.md","title":"Apache Doris 1.2 TPC-H Performance Test Report","description":"\x3c!--","date":"2022-11-22T00:00:00.000Z","formattedDate":"November 22, 2022","tags":[{"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris 1.2 TPC-H Performance Test Report","summary":"On 22 queries on the TPC-H standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions. Compared with Apache Doris 1.1.3, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 3 times, and by nearly 11 times compared with Apache Doris 0.15.0 RC04.","date":"2022-11-22","author":"Apache Doris","tags":["Tech Sharing"]},"prevItem":{"title":"Apache Doris 1.2 Star-Schema-Benchmark Performance Test Report","permalink":"/blog/ssb"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.4","permalink":"/blog/release-1.1.4"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# TPC-H Benchmark\\n\\nTPC-H is a decision support benchmark (Decision Support Benchmark), which consists of a set of business-oriented special query and concurrent data modification. The data that is queried and populates the database has broad industry relevance. This benchmark demonstrates a decision support system that examines large amounts of data, executes highly complex queries, and answers key business questions. The performance index reported by TPC-H is called TPC-H composite query performance index per hour (QphH@Size), which reflects multiple aspects of the system\'s ability to process queries. These aspects include the database size chosen when executing the query, the query processing capability when the query is submitted by a single stream, and the query throughput when the query is submitted by many concurrent users.\\n\\nThis document mainly introduces the performance of Doris on the TPC-H 100G test set.\\n\\n> Note 1: The standard test set including TPC-H is usually far from the actual business scenario, and some tests will perform parameter tuning for the test set. Therefore, the test results of the standard test set can only reflect the performance of the database in a specific scenario. We suggest users use actual business data for further testing.\\n>\\n> Note 2: The operations involved in this document are all tested on CentOS 7.x.\\n\\nOn 22 queries on the TPC-H standard test data set, we conducted a comparison test based on Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 versions. Compared with Apache Doris 1.1.3, the overall performance of Apache Doris 1.2.0-rc01 has been improved by nearly 3 times, and by nearly 11 times compared with Apache Doris 0.15.0 RC04.\\n\\n## 1. Hardware Environment\\n\\n| Hardware           | Configuration Instructions                                   |\\n| -------- | ------------------------------------ |\\n| Number of mMachines | 4 Tencent Cloud Virtual Machine\uff081FE\uff0c3BEs\uff09 |\\n| CPU      | Intel Xeon(Cascade Lake) Platinum 8269CY  16C  (2.5 GHz/3.2 GHz) |\\n| Memory | 64G                                  |\\n| Network | 5Gbps                              |\\n| Disk   | ESSD Cloud Hard Disk  |\\n\\n## 2. Software Environment\\n\\n- Doris Deployed 3BEs and 1FE\\n- Kernel Version: Linux version 5.4.0-96-generic (buildd@lgw01-amd64-051)\\n- OS version: CentOS 7.8\\n- Doris software version: Apache Doris 1.2.0-rc01\u3001 Apache Doris 1.1.3 \u3001 Apache Doris 0.15.0 RC04\\n- JDK: openjdk version \\"11.0.14\\" 2022-01-18\\n\\n## 3. Test Data Volume\\n\\nThe TPCH 100G data generated by the simulation of the entire test are respectively imported into Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for testing. The following is the relevant description and data volume of the table.\\n\\n| TPC-H Table Name | Rows        | Size after Import  | Annotation |\\n| :--------------- | :---------- | ---------- | :----- |\\n| REGION           | 5           | 400KB      | Region       |\\n| NATION           | 25          | 7.714 KB   | Nation       |\\n| SUPPLIER         | 1,000,000 | 85.528 MB  | Supplier       |\\n| PART             | 20,000,000  | 752.330 MB | Parts       |\\n| PARTSUPP         | 20,000,000  | 4.375 GB   | Parts Supply       |\\n| CUSTOMER         | 15,000,000  | 1.317 GB   | Customer        |\\n| ORDERS           | 1,500,000,000 | 6.301 GB   | Orders        |\\n| LINEITEM         | 6,000,000,000   | 20.882 GB  | Order Details       |\\n\\n## 4. Test SQL\\n\\nTPCH 22 test query statements \uff1a [TPCH-Query-SQL](https://github.com/apache/incubator-doris/tree/master/tools/tpch-tools/queries)\\n\\n**Notice:**\\n\\nThe following four parameters in the above SQL do not exist in Apache Doris 0.15.0 RC04. When executing, please remove:\\n\\n```\\n1. enable_vectorized_engine=true,\\n2. batch_size=4096,\\n3. disable_join_reorder=false\\n4. enable_projection=true\\n```\\n\\n## 5. Test Results\\n\\nHere we use Apache Doris 1.2.0-rc01, Apache Doris 1.1.3 and Apache Doris 0.15.0 RC04 for comparative testing. In the test, we use Query Time(ms) as the main performance indicator. The test results are as follows:\\n\\n| Query    | Apache Doris 1.2.0-rc01 (ms) | Apache Doris 1.1.3 (ms) | Apache Doris 0.15.0 RC04 (ms) |\\n| -------- | --------------------------- | ---------------------- | ---------------------------- |\\n| Q1       | 2.12                        | 3.75                   | 28.63                        |\\n| Q2       | 0.20                        | 4.22                   | 7.88                         |\\n| Q3       | 0.62                        | 2.64                   | 9.39                         |\\n| Q4       | 0.61                        | 1.5                    | 9.3                          |\\n| Q5       | 1.05                        | 2.15                   | 4.11                         |\\n| Q6       | 0.08                        | 0.19                   | 0.43                         |\\n| Q7       | 0.58                        | 1.04                   | 1.61                         |\\n| Q8       | 0.72                        | 1.75                   | 50.35                        |\\n| Q9       | 3.61                        | 7.94                   | 16.34                        |\\n| Q10      | 1.26                        | 1.41                   | 5.21                         |\\n| Q11      | 0.15                        | 0.35                   | 1.72                         |\\n| Q12      | 0.21                        | 0.57                   | 5.39                         |\\n| Q13      | 2.62                        | 8.15                   | 20.88                        |\\n| Q14      | 0.16                        | 0.3                    |                              |\\n| Q15      | 0.30                        | 0.66                   | 1.86                         |\\n| Q16      | 0.38                        | 0.79                   | 1.32                         |\\n| Q17      | 0.65                        | 1.51                   | 26.67                        |\\n| Q18      | 2.28                        | 3.364                  | 11.77                        |\\n| Q19      | 0.20                        | 0.829                  | 1.71                         |\\n| Q20      | 0.21                        | 2.77                   | 5.2                          |\\n| Q21      | 1.17                        | 4.47                   | 10.34                        |\\n| Q22      | 0.46                        | 0.9                    | 3.22                         |\\n| **Total** | **19.64**                   | **51.253**             | **223.33**                   |\\n\\n![image-20220614114351241](/images/tpch.png)\\n\\n- **Result Description**\\n    - The data set corresponding to the test results is scale 100, about 600 million.\\n    - The test environment is configured as the user\'s common configuration, with 4 cloud servers, 16-core 64G SSD, and 1 FE 3 BEs deployment.\\n    - Select the user\'s common configuration test to reduce the cost of user selection and evaluation, but the entire test process will not consume so many hardware resources.\\n    - Apache Doris 0.15 RC04 failed to execute Q14 in the TPC-H test, unable to complete the query.\\n\\n## 6. Environmental Preparation\\n\\nPlease refer to the [official document](../install/install-deploy.md) to install and deploy Doris to obtain a normal running Doris cluster (at least 1 FE 1 BE, 1 FE 3 BE is recommended).\\n\\n## 7. Data Preparation\\n\\n### 7.1 Download and Install TPC-H Data Generation Tool\\n\\nExecute the following script to download and compile the [tpch-tools](https://github.com/apache/incubator-doris/tree/master/tools/tpch-tools) tool.\\n\\n```shell\\nsh build-tpch-dbgen.sh\\n```\\n\\nAfter successful installation, the `dbgen` binary will be generated under the `TPC-H_Tools_v3.0.0/` directory.\\n\\n### 7.2 Generating the TPC-H Test Set\\n\\nExecute the following script to generate the TPC-H dataset:\\n\\n```shell\\nsh gen-tpch-data.sh\\n```\\n\\n> Note 1: Check the script help via `sh gen-tpch-data.sh -h`.\\n>\\n> Note 2: The data will be generated under the `tpch-data/` directory with the suffix `.tbl`. The total file size is about 100GB and may need a few minutes to an hour to generate.\\n>\\n> Note 3: A standard test data set of 100G is generated by default.\\n\\n### 7.3 Create Table\\n\\n#### 7.3.1 Prepare the `doris-cluster.conf` File\\n\\nBefore import the script, you need to write the FE\u2019s ip port and other information in the `doris-cluster.conf` file.\\n\\nThe file location is at the same level as `load-tpch-data.sh`.\\n\\nThe content of the file includes FE\'s ip, HTTP port, user name, password and the DB name of the data to be imported:\\n\\n```shell\\n# Any of FE host\\nexport FE_HOST=\'127.0.0.1\'\\n# http_port in fe.conf\\nexport FE_HTTP_PORT=8030\\n# query_port in fe.conf\\nexport FE_QUERY_PORT=9030\\n# Doris username\\nexport USER=\'root\'\\n# Doris password\\nexport PASSWORD=\'\'\\n# The database where TPC-H tables located\\nexport DB=\'tpch1\'\\n```\\n\\n#### Execute the Following Script to Generate and Create TPC-H Table\\n\\n```shell\\nsh create-tpch-tables.sh\\n```\\nOr copy the table creation statement in [create-tpch-tables.sql](https://github.com/apache/incubator-doris/blob/master/tools/tpch-tools/create-tpch-tables.sql) and excute it in Doris.\\n\\n\\n### 7.4 Import Data\\n\\nPlease perform data import with the following command:\\n\\n```shell\\nsh ./load-tpch-data.sh\\n```\\n\\n### 7.5 Check Imported Data\\n\\nExecute the following SQL statement to check that the imported data is consistent with the above data.\\n\\n```sql\\nselect count(*)  from  lineitem;\\nselect count(*)  from  orders;\\nselect count(*)  from  partsupp;\\nselect count(*)  from  part;\\nselect count(*)  from  customer;\\nselect count(*)  from  supplier;\\nselect count(*)  from  nation;\\nselect count(*)  from  region;\\nselect count(*)  from  revenue0;\\n```\\n\\n### 7.6 Query Test\\n\\n#### 7.6.1 Executing Query Scripts\\n\\nExecute the above test SQL or execute the following command\\n\\n```\\n./run-tpch-queries.sh\\n```\\n\\n>Notice:\\n>\\n>1. At present, the query optimizer and statistics functions of Doris are not so perfect, so we rewrite some queries in TPC-H to adapt to the execution framework of Doris, but it does not affect the correctness of the results\\n>\\n>2. Doris\' new query optimizer will be released in future versions\\n>3. Set `set mem_exec_limit=8G` before executing the query\\n\\n#### 7.6.2 Single SQL Execution\\n\\nThe following is the SQL statement used in the test, you can also get the latest SQL from the code base.\\n\\n```SQL\\n--Q1\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=false) */\\n    l_returnflag,\\n    l_linestatus,\\n    sum(l_quantity) as sum_qty,\\n    sum(l_extendedprice) as sum_base_price,\\n    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\\n    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\\n    avg(l_quantity) as avg_qty,\\n    avg(l_extendedprice) as avg_price,\\n    avg(l_discount) as avg_disc,\\n    count(*) as count_order\\nfrom\\n    lineitem\\nwhere\\n    l_shipdate <= date \'1998-12-01\' - interval \'90\' day\\ngroup by\\n    l_returnflag,\\n    l_linestatus\\norder by\\n    l_returnflag,\\n    l_linestatus;\\n\\n--Q2\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    s_acctbal,\\n    s_name,\\n    n_name,\\n    p_partkey,\\n    p_mfgr,\\n    s_address,\\n    s_phone,\\n    s_comment\\nfrom\\n    partsupp join\\n    (\\n        select\\n            ps_partkey as a_partkey,\\n            min(ps_supplycost) as a_min\\n        from\\n            partsupp,\\n            part,\\n            supplier,\\n            nation,\\n            region\\n        where\\n            p_partkey = ps_partkey\\n            and s_suppkey = ps_suppkey\\n            and s_nationkey = n_nationkey\\n            and n_regionkey = r_regionkey\\n            and r_name = \'EUROPE\'\\n            and p_size = 15\\n            and p_type like \'%BRASS\'\\n        group by a_partkey\\n    ) A on ps_partkey = a_partkey and ps_supplycost=a_min ,\\n    part,\\n    supplier,\\n    nation,\\n    region\\nwhere\\n    p_partkey = ps_partkey\\n    and s_suppkey = ps_suppkey\\n    and p_size = 15\\n    and p_type like \'%BRASS\'\\n    and s_nationkey = n_nationkey\\n    and n_regionkey = r_regionkey\\n    and r_name = \'EUROPE\'\\n\\norder by\\n    s_acctbal desc,\\n    n_name,\\n    s_name,\\n    p_partkey\\nlimit 100;\\n\\n--Q3\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true, runtime_filter_wait_time_ms=10000) */\\n    l_orderkey,\\n    sum(l_extendedprice * (1 - l_discount)) as revenue,\\n    o_orderdate,\\n    o_shippriority\\nfrom\\n    (\\n        select l_orderkey, l_extendedprice, l_discount, o_orderdate, o_shippriority, o_custkey from\\n        lineitem join orders\\n        where l_orderkey = o_orderkey\\n        and o_orderdate < date \'1995-03-15\'\\n        and l_shipdate > date \'1995-03-15\'\\n    ) t1 join customer c \\n    on c.c_custkey = t1.o_custkey\\n    where c_mktsegment = \'BUILDING\'\\ngroup by\\n    l_orderkey,\\n    o_orderdate,\\n    o_shippriority\\norder by\\n    revenue desc,\\n    o_orderdate\\nlimit 10;\\n\\n--Q4\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    o_orderpriority,\\n    count(*) as order_count\\nfrom\\n    (\\n        select\\n            *\\n        from\\n            lineitem\\n        where l_commitdate < l_receiptdate\\n    ) t1\\n    right semi join orders\\n    on t1.l_orderkey = o_orderkey\\nwhere\\n    o_orderdate >= date \'1993-07-01\'\\n    and o_orderdate < date \'1993-07-01\' + interval \'3\' month\\ngroup by\\n    o_orderpriority\\norder by\\n    o_orderpriority;\\n\\n--Q5\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    n_name,\\n    sum(l_extendedprice * (1 - l_discount)) as revenue\\nfrom\\n    customer,\\n    orders,\\n    lineitem,\\n    supplier,\\n    nation,\\n    region\\nwhere\\n    c_custkey = o_custkey\\n    and l_orderkey = o_orderkey\\n    and l_suppkey = s_suppkey\\n    and c_nationkey = s_nationkey\\n    and s_nationkey = n_nationkey\\n    and n_regionkey = r_regionkey\\n    and r_name = \'ASIA\'\\n    and o_orderdate >= date \'1994-01-01\'\\n    and o_orderdate < date \'1994-01-01\' + interval \'1\' year\\ngroup by\\n    n_name\\norder by\\n    revenue desc;\\n\\n--Q6\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    sum(l_extendedprice * l_discount) as revenue\\nfrom\\n    lineitem\\nwhere\\n    l_shipdate >= date \'1994-01-01\'\\n    and l_shipdate < date \'1994-01-01\' + interval \'1\' year\\n    and l_discount between .06 - 0.01 and .06 + 0.01\\n    and l_quantity < 24;\\n\\n--Q7\\nselect /*+SET_VAR(exec_mem_limit=458589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    supp_nation,\\n    cust_nation,\\n    l_year,\\n    sum(volume) as revenue\\nfrom\\n    (\\n        select\\n            n1.n_name as supp_nation,\\n            n2.n_name as cust_nation,\\n            extract(year from l_shipdate) as l_year,\\n            l_extendedprice * (1 - l_discount) as volume\\n        from\\n            supplier,\\n            lineitem,\\n            orders,\\n            customer,\\n            nation n1,\\n            nation n2\\n        where\\n            s_suppkey = l_suppkey\\n            and o_orderkey = l_orderkey\\n            and c_custkey = o_custkey\\n            and s_nationkey = n1.n_nationkey\\n            and c_nationkey = n2.n_nationkey\\n            and (\\n                (n1.n_name = \'FRANCE\' and n2.n_name = \'GERMANY\')\\n                or (n1.n_name = \'GERMANY\' and n2.n_name = \'FRANCE\')\\n            )\\n            and l_shipdate between date \'1995-01-01\' and date \'1996-12-31\'\\n    ) as shipping\\ngroup by\\n    supp_nation,\\n    cust_nation,\\n    l_year\\norder by\\n    supp_nation,\\n    cust_nation,\\n    l_year;\\n\\n--Q8\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    o_year,\\n    sum(case\\n        when nation = \'BRAZIL\' then volume\\n        else 0\\n    end) / sum(volume) as mkt_share\\nfrom\\n    (\\n        select\\n            extract(year from o_orderdate) as o_year,\\n            l_extendedprice * (1 - l_discount) as volume,\\n            n2.n_name as nation\\n        from\\n            lineitem,\\n            orders,\\n            customer,\\n            supplier,\\n            part,\\n            nation n1,\\n            nation n2,\\n            region\\n        where\\n            p_partkey = l_partkey\\n            and s_suppkey = l_suppkey\\n            and l_orderkey = o_orderkey\\n            and o_custkey = c_custkey\\n            and c_nationkey = n1.n_nationkey\\n            and n1.n_regionkey = r_regionkey\\n            and r_name = \'AMERICA\'\\n            and s_nationkey = n2.n_nationkey\\n            and o_orderdate between date \'1995-01-01\' and date \'1996-12-31\'\\n            and p_type = \'ECONOMY ANODIZED STEEL\'\\n    ) as all_nations\\ngroup by\\n    o_year\\norder by\\n    o_year;\\n\\n--Q9\\nselect/*+SET_VAR(exec_mem_limit=37179869184, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true, enable_remove_no_conjuncts_runtime_filter_policy=true, runtime_filter_wait_time_ms=100000) */\\n    nation,\\n    o_year,\\n    sum(amount) as sum_profit\\nfrom\\n    (\\n        select\\n            n_name as nation,\\n            extract(year from o_orderdate) as o_year,\\n            l_extendedprice * (1 - l_discount) - ps_supplycost * l_quantity as amount\\n        from\\n            lineitem join orders on o_orderkey = l_orderkey\\n            join[shuffle] part on p_partkey = l_partkey\\n            join[shuffle] partsupp on ps_partkey = l_partkey\\n            join[shuffle] supplier on s_suppkey = l_suppkey\\n            join[broadcast] nation on s_nationkey = n_nationkey\\n        where\\n            ps_suppkey = l_suppkey and \\n            p_name like \'%green%\'\\n    ) as profit\\ngroup by\\n    nation,\\n    o_year\\norder by\\n    nation,\\n    o_year desc;\\n\\n--Q10\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    c_custkey,\\n    c_name,\\n    sum(t1.l_extendedprice * (1 - t1.l_discount)) as revenue,\\n    c_acctbal,\\n    n_name,\\n    c_address,\\n    c_phone,\\n    c_comment\\nfrom\\n    customer,\\n    (\\n        select o_custkey,l_extendedprice,l_discount from lineitem, orders\\n        where l_orderkey = o_orderkey\\n        and o_orderdate >= date \'1993-10-01\'\\n        and o_orderdate < date \'1993-10-01\' + interval \'3\' month\\n        and l_returnflag = \'R\'\\n    ) t1,\\n    nation\\nwhere\\n    c_custkey = t1.o_custkey\\n    and c_nationkey = n_nationkey\\ngroup by\\n    c_custkey,\\n    c_name,\\n    c_acctbal,\\n    c_phone,\\n    n_name,\\n    c_address,\\n    c_comment\\norder by\\n    revenue desc\\nlimit 20;\\n\\n--Q11\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    ps_partkey,\\n    sum(ps_supplycost * ps_availqty) as value\\nfrom\\n    partsupp,\\n    (\\n    select s_suppkey\\n    from supplier, nation\\n    where s_nationkey = n_nationkey and n_name = \'GERMANY\'\\n    ) B\\nwhere\\n    ps_suppkey = B.s_suppkey\\ngroup by\\n    ps_partkey having\\n        sum(ps_supplycost * ps_availqty) > (\\n            select\\n                sum(ps_supplycost * ps_availqty) * 0.000002\\n            from\\n                partsupp,\\n                (select s_suppkey\\n                 from supplier, nation\\n                 where s_nationkey = n_nationkey and n_name = \'GERMANY\'\\n                ) A\\n            where\\n                ps_suppkey = A.s_suppkey\\n        )\\norder by\\n    value desc;\\n\\n--Q12\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    l_shipmode,\\n    sum(case\\n        when o_orderpriority = \'1-URGENT\'\\n            or o_orderpriority = \'2-HIGH\'\\n            then 1\\n        else 0\\n    end) as high_line_count,\\n    sum(case\\n        when o_orderpriority <> \'1-URGENT\'\\n            and o_orderpriority <> \'2-HIGH\'\\n            then 1\\n        else 0\\n    end) as low_line_count\\nfrom\\n    orders,\\n    lineitem\\nwhere\\n    o_orderkey = l_orderkey\\n    and l_shipmode in (\'MAIL\', \'SHIP\')\\n    and l_commitdate < l_receiptdate\\n    and l_shipdate < l_commitdate\\n    and l_receiptdate >= date \'1994-01-01\'\\n    and l_receiptdate < date \'1994-01-01\' + interval \'1\' year\\ngroup by\\n    l_shipmode\\norder by\\n    l_shipmode;\\n\\n--Q13\\nselect /*+SET_VAR(exec_mem_limit=45899345920, parallel_fragment_exec_instance_num=16, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    c_count,\\n    count(*) as custdist\\nfrom\\n    (\\n        select\\n            c_custkey,\\n            count(o_orderkey) as c_count\\n        from\\n            orders right outer join customer on\\n                c_custkey = o_custkey\\n                and o_comment not like \'%special%requests%\'\\n        group by\\n            c_custkey\\n    ) as c_orders\\ngroup by\\n    c_count\\norder by\\n    custdist desc,\\n    c_count desc;\\n\\n--Q14\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true, runtime_filter_mode=OFF) */\\n    100.00 * sum(case\\n        when p_type like \'PROMO%\'\\n            then l_extendedprice * (1 - l_discount)\\n        else 0\\n    end) / sum(l_extendedprice * (1 - l_discount)) as promo_revenue\\nfrom\\n    part,\\n    lineitem\\nwhere\\n    l_partkey = p_partkey\\n    and l_shipdate >= date \'1995-09-01\'\\n    and l_shipdate < date \'1995-09-01\' + interval \'1\' month;\\n\\n--Q15\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    s_suppkey,\\n    s_name,\\n    s_address,\\n    s_phone,\\n    total_revenue\\nfrom\\n    supplier,\\n    revenue0\\nwhere\\n    s_suppkey = supplier_no\\n    and total_revenue = (\\n        select\\n            max(total_revenue)\\n        from\\n            revenue0\\n    )\\norder by\\n    s_suppkey;\\n\\n--Q16\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=8, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    p_brand,\\n    p_type,\\n    p_size,\\n    count(distinct ps_suppkey) as supplier_cnt\\nfrom\\n    partsupp,\\n    part\\nwhere\\n    p_partkey = ps_partkey\\n    and p_brand <> \'Brand#45\'\\n    and p_type not like \'MEDIUM POLISHED%\'\\n    and p_size in (49, 14, 23, 45, 19, 3, 36, 9)\\n    and ps_suppkey not in (\\n        select\\n            s_suppkey\\n        from\\n            supplier\\n        where\\n            s_comment like \'%Customer%Complaints%\'\\n    )\\ngroup by\\n    p_brand,\\n    p_type,\\n    p_size\\norder by\\n    supplier_cnt desc,\\n    p_brand,\\n    p_type,\\n    p_size;\\n\\n--Q17\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=1, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    sum(l_extendedprice) / 7.0 as avg_yearly\\nfrom\\n    lineitem join [broadcast]\\n    part p1 on p1.p_partkey = l_partkey\\nwhere\\n    p1.p_brand = \'Brand#23\'\\n    and p1.p_container = \'MED BOX\'\\n    and l_quantity < (\\n        select\\n            0.2 * avg(l_quantity)\\n        from\\n            lineitem join [broadcast]\\n            part p2 on p2.p_partkey = l_partkey\\n        where\\n            l_partkey = p1.p_partkey\\n            and p2.p_brand = \'Brand#23\'\\n            and p2.p_container = \'MED BOX\'\\n    );\\n\\n--Q18\\n\\nselect /*+SET_VAR(exec_mem_limit=45899345920, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\n    c_name,\\n    c_custkey,\\n    t3.o_orderkey,\\n    t3.o_orderdate,\\n    t3.o_totalprice,\\n    sum(t3.l_quantity)\\nfrom\\ncustomer join\\n(\\n  select * from\\n  lineitem join\\n  (\\n    select * from\\n    orders left semi join\\n    (\\n      select\\n          l_orderkey\\n      from\\n          lineitem\\n      group by\\n          l_orderkey having sum(l_quantity) > 300\\n    ) t1\\n    on o_orderkey = t1.l_orderkey\\n  ) t2\\n  on t2.o_orderkey = l_orderkey\\n) t3\\non c_custkey = t3.o_custkey\\ngroup by\\n    c_name,\\n    c_custkey,\\n    t3.o_orderkey,\\n    t3.o_orderdate,\\n    t3.o_totalprice\\norder by\\n    t3.o_totalprice desc,\\n    t3.o_orderdate\\nlimit 100;\\n\\n--Q19\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=false, enable_cost_based_join_reorder=false, enable_projection=true) */\\n    sum(l_extendedprice* (1 - l_discount)) as revenue\\nfrom\\n    lineitem,\\n    part\\nwhere\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#12\'\\n        and p_container in (\'SM CASE\', \'SM BOX\', \'SM PACK\', \'SM PKG\')\\n        and l_quantity >= 1 and l_quantity <= 1 + 10\\n        and p_size between 1 and 5\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    )\\n    or\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#23\'\\n        and p_container in (\'MED BAG\', \'MED BOX\', \'MED PKG\', \'MED PACK\')\\n        and l_quantity >= 10 and l_quantity <= 10 + 10\\n        and p_size between 1 and 10\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    )\\n    or\\n    (\\n        p_partkey = l_partkey\\n        and p_brand = \'Brand#34\'\\n        and p_container in (\'LG CASE\', \'LG BOX\', \'LG PACK\', \'LG PKG\')\\n        and l_quantity >= 20 and l_quantity <= 20 + 10\\n        and p_size between 1 and 15\\n        and l_shipmode in (\'AIR\', \'AIR REG\')\\n        and l_shipinstruct = \'DELIVER IN PERSON\'\\n    );\\n\\n--Q20\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=2, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true, runtime_bloom_filter_size=551943) */\\ns_name, s_address from\\nsupplier left semi join\\n(\\n    select * from\\n    (\\n        select l_partkey,l_suppkey, 0.5 * sum(l_quantity) as l_q\\n        from lineitem\\n        where l_shipdate >= date \'1994-01-01\'\\n            and l_shipdate < date \'1994-01-01\' + interval \'1\' year\\n        group by l_partkey,l_suppkey\\n    ) t2 join\\n    (\\n        select ps_partkey, ps_suppkey, ps_availqty\\n        from partsupp left semi join part\\n        on ps_partkey = p_partkey and p_name like \'forest%\'\\n    ) t1\\n    on t2.l_partkey = t1.ps_partkey and t2.l_suppkey = t1.ps_suppkey\\n    and t1.ps_availqty > t2.l_q\\n) t3\\non s_suppkey = t3.ps_suppkey\\njoin nation\\nwhere s_nationkey = n_nationkey\\n    and n_name = \'CANADA\'\\norder by s_name;\\n\\n--Q21\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4, enable_vectorized_engine=true, batch_size=4096, disable_join_reorder=true, enable_cost_based_join_reorder=true, enable_projection=true) */\\ns_name, count(*) as numwait\\nfrom\\n  lineitem l2 right semi join\\n  (\\n    select * from\\n    lineitem l3 right anti join\\n    (\\n      select * from\\n      orders join lineitem l1 on l1.l_orderkey = o_orderkey and o_orderstatus = \'F\'\\n      join\\n      (\\n        select * from\\n        supplier join nation\\n        where s_nationkey = n_nationkey\\n          and n_name = \'SAUDI ARABIA\'\\n      ) t1\\n      where t1.s_suppkey = l1.l_suppkey and l1.l_receiptdate > l1.l_commitdate\\n    ) t2\\n    on l3.l_orderkey = t2.l_orderkey and l3.l_suppkey <> t2.l_suppkey  and l3.l_receiptdate > l3.l_commitdate\\n  ) t3\\n  on l2.l_orderkey = t3.l_orderkey and l2.l_suppkey <> t3.l_suppkey \\n\\ngroup by\\n    t3.s_name\\norder by\\n    numwait desc,\\n    t3.s_name\\nlimit 100;\\n\\n--Q22\\n\\nwith tmp as (select\\n                    avg(c_acctbal) as av\\n                from\\n                    customer\\n                where\\n                    c_acctbal > 0.00\\n                    and substring(c_phone, 1, 2) in\\n                        (\'13\', \'31\', \'23\', \'29\', \'30\', \'18\', \'17\'))\\n\\nselect /*+SET_VAR(exec_mem_limit=8589934592, parallel_fragment_exec_instance_num=4,runtime_bloom_filter_size=4194304) */\\n    cntrycode,\\n    count(*) as numcust,\\n    sum(c_acctbal) as totacctbal\\nfrom\\n    (\\n\\tselect\\n            substring(c_phone, 1, 2) as cntrycode,\\n            c_acctbal\\n        from\\n             orders right anti join customer c on  o_custkey = c.c_custkey join tmp on c.c_acctbal > tmp.av\\n        where\\n            substring(c_phone, 1, 2) in\\n                (\'13\', \'31\', \'23\', \'29\', \'30\', \'18\', \'17\')\\n    ) as custsale\\ngroup by\\n    cntrycode\\norder by\\n    cntrycode;\\n```"},{"id":"/release-1.1.4","metadata":{"permalink":"/blog/release-1.1.4","source":"@site/blog/release-1.1.4.md","title":"Apache Doris announced the official release of version 1.1.4","description":"\x3c!--","date":"2022-11-11T00:00:00.000Z","formattedDate":"November 11, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.4","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1.4 on November 11, 2022! This release is a hotfix version of 1.1.3","date":"2022-11-11","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Apache Doris 1.2 TPC-H Performance Test Report","permalink":"/blog/tpch"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.3","permalink":"/blog/release-1.1.3"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\nIn this release, Doris Team has fixed about 60 issues or performance improvement since 1.1.3. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n\\n# Features\\n\\n- Support obs broker load for Huawei Cloud. [#13523](https://github.com/apache/doris/pull/13523)\\n\\n- SparkLoad support parquet and orc file.[#13438](https://github.com/apache/doris/pull/13438)\\n\\n# Improvements\\n\\n- Do not acquire mutex in metric hook since it will affect query performance during heavy load.[#10941](https://github.com/apache/doris/pull/10941)\\n\\n\\n# BugFix\\n\\n- The where condition does not take effect when spark load loads the file. [#13804](https://github.com/apache/doris/pull/13804)\\n\\n- If function return error result when there is nullable column in vectorized mode. [#13779](https://github.com/apache/doris/pull/13779)\\n\\n- Fix incorrect result when using anti join with other join predicates. [#13743](https://github.com/apache/doris/pull/13743)\\n\\n- BE crash when call function concat(ifnull). [#13693](https://github.com/apache/doris/pull/13693)\\n\\n- Fix planner bug when there is a function in group by clause. [#13613](https://github.com/apache/doris/pull/13613)\\n\\n- Table name and column name is not recognized correctly in lateral view clause. [#13600](https://github.com/apache/doris/pull/13600)\\n\\n- Unknown column when use MV and table alias. [#13605](https://github.com/apache/doris/pull/13605)\\n\\n- JSONReader release memory of both value and parse allocator. [#13513](https://github.com/apache/doris/pull/13513)\\n\\n- Fix allow create mv using to_bitmap() on negative value columns when enable_vectorized_alter_table is true. [#13448](https://github.com/apache/doris/pull/13448)\\n\\n- Microsecond in function from_date_format_str is lost. [#13446](https://github.com/apache/doris/pull/13446)\\n\\n- Sort exprs nullability property may not be right after subsitute using child\'s smap info. [#13328](https://github.com/apache/doris/pull/13328)\\n\\n- Fix core dump on case when have 1000 condition. [#13315](https://github.com/apache/doris/pull/13315)\\n\\n- Fix bug that last line of data lost for stream load. [#13066](https://github.com/apache/doris/pull/13066)\\n\\n- Restore table or partition with the same replication num as before the backup. [#11942](https://github.com/apache/doris/pull/11942)"},{"id":"/release-1.1.3","metadata":{"permalink":"/blog/release-1.1.3","source":"@site/blog/release-1.1.3.md","title":"Apache Doris announced the official release of version 1.1.3","description":"\x3c!--","date":"2022-10-17T00:00:00.000Z","formattedDate":"October 17, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.3","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1.3 on October 17, 2022! This release is a hotfix version of 1.1.2","date":"2022-10-17","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1.4","permalink":"/blog/release-1.1.4"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.2","permalink":"/blog/release-1.1.2"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nIn this release, Doris Team has fixed more than 80 issues or performance improvement since 1.1.2. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n\\n# Features\\n\\n- Support escape identifiers for sqlserver and postgresql in ODBC table.\\n\\n- Could use Parquet as output file format.\\n\\n# Improvements\\n\\n- Optimize flush policy to avoid small segments. [#12706](https://github.com/apache/doris/pull/12706) [#12716](https://github.com/apache/doris/pull/12716)\\n\\n- Refactor runtime filter to reduce the prepare time. [#13127](https://github.com/apache/doris/pull/13127)\\n\\n- Lots of memory control related issues during query or load process. [#12682](https://github.com/apache/doris/pull/12682) [#12688](https://github.com/apache/doris/pull/12688) [#12708](https://github.com/apache/doris/pull/12708) [#12776](https://github.com/apache/doris/pull/12776) [#12782](https://github.com/apache/doris/pull/12782) [#12791](https://github.com/apache/doris/pull/12791) [#12794](https://github.com/apache/doris/pull/12794) [#12820](https://github.com/apache/doris/pull/12820) [#12932](https://github.com/apache/doris/pull/12932) [#12954](https://github.com/apache/doris/pull/12954) [#12951](https://github.com/apache/doris/pull/12951)\\n\\n# BugFix\\n\\n- Core dump on compaction with largeint. [#10094](https://github.com/apache/doris/pull/10094)\\n\\n- Grouping sets cause be core or return wrong results. [#12313](https://github.com/apache/doris/pull/12313)\\n\\n- PREAGGREGATION flag in orthogonal_bitmap_union_count operator is wrong. [#12581](https://github.com/apache/doris/pull/12581)\\n\\n- Level1Iterator should release iterators in heap and it may cause memory leak. [#12592](https://github.com/apache/doris/pull/12592)\\n\\n- Fix decommission failure with 2 BEs and existing colocation table. [#12644](https://github.com/apache/doris/pull/12644)\\n\\n- BE may core dump because of stack-buffer-overflow when TBrokerOpenReaderResponse too large. [#12658](https://github.com/apache/doris/pull/12658)\\n\\n- BE may OOM during load when error code -238 occurs. [#12666](https://github.com/apache/doris/pull/12666)\\n\\n- Fix wrong child expression of lead function. [#12587](https://github.com/apache/doris/pull/12587)\\n\\n- Fix intersect query failed in row storage code. [#12712](https://github.com/apache/doris/pull/12712)\\n\\n- Fix wrong result produced by curdate()/current_date() function. [#12720](https://github.com/apache/doris/pull/12720)\\n\\n- Fix lateral view explode_split with temp table bug. [#13643](https://github.com/apache/doris/pull/13643)\\n\\n- Bucket shuffle join plan is wrong in two same table. [#12930](https://github.com/apache/doris/pull/12930)\\n\\n- Fix bug that tablet version may be wrong when doing alter and load. [#13070](https://github.com/apache/doris/pull/13070)\\n\\n- BE core when load data using broker with md5sum()/sm3sum(). [#13009](https://github.com/apache/doris/pull/13009)\\n\\n# Upgrade Notes\\n\\nPageCache and ChunkAllocator are disabled by default to reduce memory usage and can be re-enabled by modifying the configuration items `disable_storage_page_cache` and `chunk_reserved_bytes_limit`.\\n\\nStorage Page Cache and Chunk Allocator cache user data chunks and memory preallocation, respectively.\\n\\nThese two functions take up a certain percentage of memory and are not freed. This part of memory cannot be flexibly allocated, which may lead to insufficient memory for other tasks in some scenarios, affecting system stability and availability. Therefore, we disabled these two features by default in version 1.1.3.\\n\\nHowever, in some latency-sensitive reporting scenarios, turning off this feature may lead to increased query latency. If you are worried about the impact of this feature on your business after upgrade, you can add the following parameters to be.conf to keep the same behavior as the previous version.\\n\\n```\\ndisable_storage_page_cache=false\\nchunk_reserved_bytes_limit=10%\\n```\\n\\n* ``disable_storage_page_cache``: Whether to disable Storage Page Cache. version 1.1.2 (inclusive), the default is false, i.e., on. version 1.1.3 defaults to true, i.e., off.\\n* `chunk_reserved_bytes_limit`: Chunk allocator reserved memory size. 1.1.2 (and earlier), the default is 10% of the overall memory. 1.1.3 version default is 209715200 (200MB)."},{"id":"/release-1.1.2","metadata":{"permalink":"/blog/release-1.1.2","source":"@site/blog/release-1.1.2.md","title":"Apache Doris announced the official release of version 1.1.2","description":"\x3c!--","date":"2022-09-13T00:00:00.000Z","formattedDate":"September 13, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.2","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1.2 on September 13, 2022! This release is a hotfix version of 1.1.1","date":"2022-09-13","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1.3","permalink":"/blog/release-1.1.3"},"nextItem":{"title":"Doris Stream Load Principle Analysis","permalink":"/blog/principle-of-Doris-Stream-Load"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\nIn this release, Doris Team has fixed more than 170 issues or performance improvement since 1.1.1. This release is a bugfix release on 1.1 and all users are encouraged to upgrade to this release.\\n\\n# Features\\n\\n### New MemTracker\\n\\nIntroduced new MemTracker for both vectorized engine and non-vectorized engine which is more accurate.\\n\\n### Add API for showing current queries and kill query\\n\\n### Support read/write emoji of UTF16 via ODBC Table\\n\\n# Improvements\\n\\n### Data Lake related improvements\\n\\n- Improved HDFS ORC File scan performance about 300%. [#11501](https://github.com/apache/doris/pull/11501)\\n\\n- Support HDFS HA mode when query Iceberg table.\\n\\n- Support query Hive data created by [Apache Tez](https://tez.apache.org/)\\n\\n- Add Ali OSS as Hive external support.\\n\\n### Add support for string and text type in Spark Load\\n\\n\\n### Add reuse block in non-vectorized engine and have 50% performance improvement in some cases. [#11392](https://github.com/apache/doris/pull/11392)\\n\\n### Improve like or regex performance\\n\\n### Disable tcmalloc\'s aggressive_memory_decommit \\n\\nIt will have 40% performance gains in load or query.\\n\\nCurrently it is a config, you can change it by set config `tc_enable_aggressive_memory_decommit`.\\n\\n# Bug Fix\\n\\n### Some issues about FE that will cause FE failure or data corrupt.\\n\\n- Add reserved disk config to avoid too many reserved BDB-JE files.**(Serious)**   In an HA environment, BDB JE will retains as many reserved files. The BDB-je log doesn\'t delete until approaching a disk limit.\\n\\n- Fix fatal bug in BDB-JE which will cause FE replica could not start correctly or data corrupted.** (Serious)**\\n\\n### Fe will hang on waitFor_rpc during query and BE will hang in high concurrent scenarios.\\n\\n[#12459](https://github.com/apache/doris/pull/12459) [#12458](https://github.com/apache/doris/pull/12458) [#12392](https://github.com/apache/doris/pull/12392)\\n\\n### A fatal issue in vectorized storage engine which will cause wrong result. **(Serious)**\\n\\n[#11754](https://github.com/apache/doris/pull/11754) [#11694](https://github.com/apache/doris/pull/11694)\\n\\n### Lots of planner related issues that will cause BE core or in abnormal state.\\n\\n[#12080](https://github.com/apache/doris/pull/12080) [#12075](https://github.com/apache/doris/pull/12075) [#12040](https://github.com/apache/doris/pull/12040) [#12003](https://github.com/apache/doris/pull/12003) [#12007](https://github.com/apache/doris/pull/12007) [#11971](https://github.com/apache/doris/pull/11971) [#11933](https://github.com/apache/doris/pull/11933) [#11861](https://github.com/apache/doris/pull/11861) [#11859](https://github.com/apache/doris/pull/11859) [#11855](https://github.com/apache/doris/pull/11855) [#11837](https://github.com/apache/doris/pull/11837) [#11834](https://github.com/apache/doris/pull/11834) [#11821](https://github.com/apache/doris/pull/11821) [#11782](https://github.com/apache/doris/pull/11782) [#11723](https://github.com/apache/doris/pull/11723) [#11569](https://github.com/apache/doris/pull/11569)"},{"id":"/principle-of-Doris-Stream-Load","metadata":{"permalink":"/blog/principle-of-Doris-Stream-Load","source":"@site/blog/principle-of-Doris-Stream-Load.md","title":"Doris Stream Load Principle Analysis","description":"\x3c!--","date":"2022-09-08T00:00:00.000Z","formattedDate":"September 8, 2022","tags":[{"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Doris Stream Load Principle Analysis","summary":"Stream Load, one of the most commonly used data import methods for Doris users, is a synchronous import method. It allows users to import data into Doris in batch through HTTP access and returns the results of data import.","date":"2022-09-08","author":"Apache Doris","tags":["Tech Sharing"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1.2","permalink":"/blog/release-1.1.2"},"nextItem":{"title":"Doris analysis: Doris SQL principle analysis","permalink":"/blog/principle-of-Doris-SQL-parsing"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n**Lead\uff1a**\\n\\nStream Load, one of the most commonly used data import methods for Doris users, is a synchronous import method. It allows users to import data into Doris in batch through HTTP access and returns the results of data import. The user can not only directly judge whether the data import is successful through the return body of the HTTP request, but also query the results of historical tasks by executing query SQL on the client.\\n\\n#  **Introduction to Stream Load**\\n\\nThe Doris import (Load) function is to import the user\'s original data into the Doris table. And Doris realizes a unified streaming import framework at the bottom. On this basis, Doris provides a very rich import mode to adapt to different data sources and data import requirements. Stream Load is one of the most commonly used data import methods for Doris users. It is a synchronous import method that allows users to import data in CSV format or JSON format into Doris in batch through HTTP access and return the results of data import. User can directly judge whether the data import is successful through the return body of the HTTP request, and can query the results of historical tasks by executing query SQL on the client. In addition, Doris also provides the operation audit function for Stream Load, which can audit the historical Stream Load task information through the audit log. The implementation principle of Stream Load will be deeply analyzed from the aspects of execution process, transaction management, implementation of import plan, data writing and operation audit of Stream Load.\\n\\n# 1 Implementation Process\\n\\nThe user submits the HTTP request of Stream Load to the FE, and the FE will forward the data import request to a BE node through HTTP Redirect, which will be the Coordinator of this Stream Load task. In this process, the FE node receiving the request only provides forwarding service. The BE node as the Coordinator is actually responsible for the entire import job, such as sending transaction requests to the Master FE, obtaining import execution plans from the FE, receiving real-time data, distributing data to other Executor BE nodes, and returning results to the user after data import. The user can also submit the HTTP request of Stream Load directly to a specified BE node, and the node will act as the Coordinator of this Stream Load task. During the Stream Load process, the Executor BE node is responsible for writing data to the storage layer.\\n\\nIn the Coordinator BE, all HTTP requests, including Stream Load requests, are processed through a thread pool. A Stream Load task is uniquely identified by the imported Label. The principle block diagram of Stream Load is shown in Figure 1.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_1_en.png)\\n           \\n\\nThe complete execution process of Stream Load is shown in Figure 2:\\n\\n(1)The user submits the HTTP request of Stream Load to the FE (the user can also directly submit the HTTP request of Stream Load to the Coordinator BE).\\n\\n(2)FE, after receiving the Stream Load request submitted by the user, will perform HTTP Header parsing (including the library, table, Label and other information imported by parsing data), and then perform user authentication. If the HTTP Header is successfully resolved and the user authentication passes, the FE will forward the HTTP request of Stream Load to a BE node, which will be the Coordinator of this Stream Load. Otherwise, the FE will directly return the failure information of Stream Load to the user.\\n\\n(3)After receiving the HTTP request from Stream Load, the Coordinator BE will first perform HTTP Header parsing and data verification, including the file format of the parsed data, the size of the data body, the HTTP timeout, and user authentication. If the Header data verification fails, the Stream Load failure information will be directly returned to the user.\\n\\n(4)After the HTTP Header data verification is passed, the Coordinator BE will send a Begin Transaction request to the FE through Thrift RPC.\\n\\n(5)After the FE receives the Begin Transaction request sent by the Coordinator BE, it will start a transaction and return the Transaction ID to the Coordinator BE.\\n\\n(6)After the Coordinator BE receives the Begin Transaction success information, it will send a request to get the import plan to the FE through Thrift RPC.\\n\\n(7)After receiving the request for obtaining the import plan sent by the Coordinator BE, the FE will generate the import plan for the Stream Load task and return it to the Coordinator BE.\\n\\n(8)After receiving the import plan, the Coordinator BE starts to execute the import plan, including receiving the real-time data from HTTP and distributing the real-time data to other Executor BE through BRPC.\\n\\n(9)After receiving the real-time data distributed by the Coordinator BE, the Executor BE is responsible for writing the data to the storage layer.\\n\\n(10)After the Executor BE completes data writing, the Coordinator BE sends a Commit Transaction request to the FE through Thrift RPC.\\n\\n(11)After receiving the Commit Transaction request sent by the Coordinator BE, the FE will commit transaction, send the Publish Version task to the Executor BE, and wait for the Executor BE to execute the Publish Version.\\n\\n(12)The Executor BE asynchronously executes the Publish Version to change the Rowset generated by data import into a visible data version.\\n\\n(13)After the Publish Version completes normally or the execution timeout, the FE returns the results of the Commit Transaction and the Publish Version to the Coordinator BE.\\n\\n(14)The Coordinator BE returns the final result of Stream Load to the user.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_2_en.png)\\n# 2 Transaction Management\\n\\nDoris ensures the atomicity of data import through Transaction. One Stream Load task corresponds to one transaction. The FE is responsible for the transaction management of Stream Load. The FE receives the Thrift RPC transaction request sent by the Coordinator BE node through the FrontendService. Transaction request types include Begin Transaction, Commit Transaction and Rollback Transaction. The transaction states of Doris include PREPARE, COMMITTED, VISIBLE, and ABORTED. The status flow process of the Stream Load transaction is shown in Figure 3.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_3_en.png)\\n\\nThe Coordinator BE node will send a Begin Transaction request to the FE before data import. The FE will check whether the label requested by the Begin Transaction already exists. If the label does not exist in the system, it will open a new transaction for the current label, assign a Transaction ID to the transaction, and set the transaction status to PREPARE, then returns the Transaction ID and the success information of the Begin Transaction to the Coordinator BE. Otherwise, this transaction may be a repeated data import. The FE returns the Begin Transaction failure message to the Coordinator BE, and the Stream Load task exits.\\n\\nAfter the data is written in all Executor BE nodes, the Coordinator BE node will send a Commit Transaction request to the FE. After receiving the Commit Transaction request, the FE will execute the Commit Transaction and Publish Version operations. First, the FE will judge whether the number of replicas of data successfully written by each Tablet exceeds half of the total number of replicas of the tablet. If the number of replicas of data successfully written by each Tablet exceeds half of the total number of replicas of the Tablet (most of them are successful), the Commit Transaction is successful and the transaction status is set to COMMITTED; Otherwise, the Commit Transaction failure information is returned to the Coordinator BE. The COMMITTED status indicates that the data has been written successfully, but the data is not visible. You need to continue to execute the Publish Version task. After that, the transaction cannot be rolled back.\\n\\nThe FE will have a separate thread to execute the Publish Version on the Transaction with successful Commit. When the Publish Version is executed, the FE will send the Publish Version request to all Executor BE nodes related to the Transaction through Thrift RPC. The Publish Version task is executed asynchronously on each Executor BE node, and the Rowset generated by data import is changed into a visible data version. When all the Publish Version tasks on the Executor BE are successfully executed, the FE will set the transaction status to VISIBLE, and return the Commit Transaction and Publish Version success information to the Coordinator BE. When some Publish Version tasks fail, the FE will repeatedly issue a Publish Version request to the Executor BE node until the previously failed Publish Version task succeeds. If the transaction status has not been set to VISIBLE after a certain timeout period, the FE will return to the Coordinator BE the information that the Commit Transaction was successful but the Publish Version timed out (note that at this time, the data is still written successfully, but it is still invisible, and the user needs to wait for the transaction status to finally become VISIBLE).\\n\\nWhen obtaining the import plan from the FE fails, executing data import fails, or Commit Transaction fails, the Coordinator BE node will send a Rollback Transaction request to the FE to execute transaction rollback. After receiving the transaction rollback request, the FE will set the transaction status to ABORTED, and send a Clear Transaction request to the Executor BE through Thrift RPC. The Clear Transaction task is asynchronously executed at the BE node, marking the Rowset generated by data import as unavailable. These Rowset will be deleted from the BE later. Transactions with COMMITTED status (transactions with Commit Transaction succeeded but Publish Version timed out) cannot be rolled back.\\n\\n# 3 Execution of the import plan\\nIn Doris BE, all execution plans are managed by FragmentMgr, and the execution of each import plan is managed by PlanFragmentExecutor. After the BE obtains the import execution plan from the FE, it will submit the import plan to the thread pool of FragmentMgr for execution. The import execution plan of Stream Load has only one Fragment, including one BrokerScanNode and one OlapTableSink. BrokerScanNode is responsible for reading streaming data in real time and converting the data lines in CSV format or JSON format to the Tuple format of Doris. OlapTableSink is responsible for sending real-time data to the corresponding Executor BE node. The Executor BE node corresponding to each data row is determined by which BE the Tablet where the data row is stored. The Partition and Tablet where the data row is stored can be determined according to the PartitionKey and DistributionKey of the data row. The BE node on which each Tablet and its replica are stored has been determined when the Table or Partition is created.\\n\\nAfter importing the execution plan and submitting it to the thread pool of FragmentMgr, the Stream Load thread will receive the real-time data transmitted through HTTP in chunks and write it to the StreamLoadPipe. The BrokerScanNode will read the real-time data in batches from the StreamLoadPipe. OlapTableSink will send the batch data read by the BrokerScanNode to the Executor BE through BRPC for data writing. After all real-time data is written to the StreamLoadPipe, the Stream Load thread will wait for the import plan to finish.\\n\\nThe PlanFragmentExecutor executes a specific import plan process, which consists of three stages: Prepare, Open, and Close. In the Prepare stage, the import execution plan from the FE is mainly analyzed; In the Open stage, BrokerScanNode and OlapTableSink will be opened. BrokerScanNode is responsible for reading the real-time data of one Batch at a time, and OlapTableSink is responsible for calling BRPC to send the data of each Batch to other Executor BE nodes; In the Close stage, it is responsible for waiting for the data import to end and closing the BrokerScanNode and OlapTableSink. The import execution plan of Stream Load is shown in Figure 4.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_4_en.png)\\n\\nOlapTableSink is responsible for the data distribution of the Stream Load task. Tables in Doris may have Rollup or Materialized view. Each Table and its Rollup and Materialized view are called an Index. In the process of data distribution, the IndexChannel will maintain a data distribution channel of the Index. The Tablet under the Index may have multiple replicas and are distributed on different BE nodes. The NodeChannel will maintain the data distribution channel of an Executor BE node under the IndexChannel. Therefore, the OlapTableSink contains multiple IndexChannel, and each NodeChannel contains multiple NodeChannel, as shown in Figure 5.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_5_en.png)\\n\\nWhen OlapTableSink distributes data, it will read the data Batch obtained by BrokerScanNode row by row, and add the data row to the IndexChannel of each Index. The Partition and Tablet of the data row can be determined according to the PartitionKey and DistributionKey, and then the corresponding Tablet of the data row in other Index can be calculated according to the order of the Tablet in the Partition. Each Tablet may have multiple replicas distributed on different BE nodes. Therefore, in the IndexChannel, each data row will be added to the NodeChannel corresponding to each replica of its Tablet. Each NodeChannel has a send queue. When the new data rows in NodeChannel accumulate to a certain size, they will be added to the send queue as a data Batch. There will be a fixed thread in OlapTableSink to train each NodeChannel under each IndexChannel in turn, and call BRPC to send a data Batch in the sending queue to the corresponding Executor BE. The data distribution process of the Stream Load task is shown in Figure 6.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_6_en.png)\\n\\n# 4 **Data Write**\\n\\nAfter receiving the data Batch sent by the Coordinator BE, the BRPC server of the Executor BE will submit the data writing task to the thread pool for asynchronous execution. In Doris BE, data is written to the storage layer in a hierarchical manner. Each Stream Load task corresponds to a LoadChannel on each Executor BE. The LoadChannel maintains the data writing channel of a Stream Load task and is responsible for the data writing of a Stream Load task on the current Executor BE node, LoadChannel can write the data of a Stream Load task in the current BE node to the storage layer in batches until the Stream Load task is completed. Each LoadChannel is uniquely identified by the load ID, and all LoadChannel on the BE node are managed by LoadChannelMgr. The Table corresponding to a Stream Load task may have multiple Index. Each Index corresponds to a TabletsChannel, which is uniquely identified by the Index ID. Therefore, there will be multiple TabletsChannel under each LoadChannel. The TabletsChannel maintains an Index data writing channel, which is responsible for managing the data writing of all the Tablet under the Index. The TabletsChannel will read the data Batch row by row and write it to the corresponding Tablet through the DeltaWriter. The DeltaWriter maintains a data writing channel of a Tablet, which is uniquely identified by the Tablet ID. it is responsible for receiving the data import of a single Tablet and writing the data into the MemTable corresponding to the tablet. When the MemTable is full, the data in the MemTable will be flushed to the disk and Segment files will be generated. MemTable adopts the data structure of SkipList to temporarily store the data in memory. SkipList will sort the data rows according to the Key of Schema. In addition, if the data model is Aggregate or Unique, MemTable will aggregate data rows with the same Key. The data write channel of the Stream Load task is shown in Figure 7.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_7_en.png)\\n\\nThe Flush operation of MemTable is performed asynchronously by MemtableFlushExecutor. After the MemTable Flush task is submitted to the thread pool, a new MemTable will be generated to receive the subsequent data writing of the current Tablet. When the MemtableFlushExecutor performs data Flush, the RowsetWriter will read out all the data in the MemTable and write out multiple Segment files through the SegmentWriter. The size of each Segment file is no more than 256MB. For a Tablet, each Stream Load task will generate a newRowset. The generated Rowset can contain multiple Segment files. The data writing process of the Stream Load task is shown in Figure 8.\\n\\n![](/images/blogs/principle-of-Doris-Stream-Load/Figure_8_en.png)\\n\\nThe TxnManager on the Executor BE node is responsible for transaction management of Tablet level data import. When the Delta Writer is initialized, the PrepareTransaction will be executed to add the data write transaction of the corresponding Tablet in the current Stream Load task to the TxnManager for management. When the data write Tablet is completed and the DeltaWriter is closed, the Commit Transaction will be executed to add the new Rowset generated by the data import to the TxnManager for management. Note that the TxnManager here is only responsible for the transactions on a single BE, while the transaction management in the FE is responsible for the overall import of transactions.\\n\\nAfter the data import is completed, when the Executor BE executes the Publish Version task issued by the FE, it will execute the Publish Transaction to change the new Rowset generated by the data import into a visible version, and delete the data writing task of the corresponding Tablet in the current Stream Load task from the TxnManager. This means that the data writing transaction of the Tablet in the current Stream Load task ends.\\n\\n# 5 **Stream Load Operation Audit**\\n\\nDoris adds the operation audit function to Stream Load. After each Stream Load task is completed and the results are returned to the user, the Coordinator BE will persistently store the detailed information of this Stream Load task on the local RocksDB. The Master FE periodically pulls the information of the completed Stream Load task from each BE node of the cluster through Thrift RPC, pulls a batch of Stream Load operation records from one BE node at a time, and writes the pulled Stream Load task information into the audit log (fe.audit.log). Each Stream Load task information stored on the BE will be set with an expiration time (TTL), and the expired Stream Load task information will be deleted when RocksDB executes the Compaction. The user can audit the historical Stream Load task information through the FE audit log.\\n\\nWhen the FE writes the pulled Stream Load task information into the Audit log, it will keep a copy in the memory. In order to prevent memory expansion, a fixed number of Stream Load task information will be kept in the memory. As the subsequent data pulling continues, the early Stream Load task information will be gradually eliminated from the FE memory. The user can query the latest Stream Load task information by executing the SHOW STREAM LOAD command at the client.\\n\\n# **Summary**\\n\\nIn this paper, the implementation principle of Stream Load is deeply analyzed from the aspects of execution process, transaction management, implementation of import plan, data writing and operation audit of Stream Load. Stream Load is one of the most commonly used data import methods for Doris users. It is a synchronous import method that allows users to import data into Doris in batch through HTTP access and return the results of data import. The user can not only directly judge whether the data import is successful through the return body of the HTTP request, but also query the results of historical tasks by executing query SQL on the client. Otherwise, Doris also provides the result audit function for Stream Load, which can audit the historical Stream Load task information through the audit log."},{"id":"/principle-of-Doris-SQL-parsing","metadata":{"permalink":"/blog/principle-of-Doris-SQL-parsing","source":"@site/blog/principle-of-Doris-SQL-parsing.md","title":"Doris analysis: Doris SQL principle analysis","description":"\x3c!--","date":"2022-08-25T00:00:00.000Z","formattedDate":"August 25, 2022","tags":[{"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Doris analysis: Doris SQL principle analysis","summary":"This article mainly introduces the principle of Doris SQL parsing.Since there are many types of SQL, this article focuses on the analysis of query SQL. Doris\'s SQL analysis will be explained deeply in the algorithm principle and code implementation.","date":"2022-08-25","author":"Apache Doris","tags":["Tech Sharing"]},"prevItem":{"title":"Doris Stream Load Principle Analysis","permalink":"/blog/principle-of-Doris-Stream-Load"},"nextItem":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","permalink":"/blog/Flink-realtime-write"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n**Lead\uff1a**\\nThis article mainly introduces the principle of Doris SQL parsing.\\n\\nIt focuses on generating a single-machine logical plan, developing a distributed logical plan, and generating a distributed physical plan. Analyze, SinglePlan, DistributedPlan, and Schedule four parts correspond to the code implementation.\\n\\nFirst, AST will be processed preliminary by Analyze and then optimized by SinglePlan to generate a single-machine query plan. Third, DistributedPlan will split the single-machine query plan into distributed query plans. In the end, the query plan will be sent to machines and executed orderly, which decide by Schedule.\\n\\nSince there are many types of SQL, this article focuses on the analysis of query SQL. Doris\'s SQL analysis will be explained deeply in the algorithm principle and code implementation.\\n\\n# 1. Introduction to Doris\\nDoris is an interactive SQL database based on MPP architecture, mainly used to solve near real-time reports and multi-dimensional analysis. The Doris architecture is straightforward, with only two types of processes.\\n\\n- Frontend\uff08FE\uff09: It is mainly responsible for user request access, query parsing and planning, storage and management of metadata, and node management-related work.\\n\\n- Backend\uff08BE\uff09: It is mainly responsible for data storage and query plan execution.\\n\\nIn Doris\' storage engine, data will be horizontally divided into several data shards (Tablet, also called data bucket). Each tablet contains several rows of data. Multiple Tablets belong to different partitions logically. A Tablet only belongs to one Partition. And a Partition contains several Tablets. Tablet is the smallest physical storage unit for operations such as data movement, copying, etc.\\n\\n# 2. SQL parsing In Apache Doris\\nSQL parsing in this article refers to **the process of generating a complete physical execution plan after a series of parsing of an SQL statement**.\\n\\nThis process includes the following four steps: lexical analysis, syntax analysis, generating a logical plan, and generating a physical plan.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_1_en.png)\\n\\n## 2.1 Lexical analysis\\nThe lexical analysis will identify the SQL in the form of a string into tokens, in preparation for the grammatical analysis.\\n```undefined\\nselect ......  from ...... where ....... group by ..... order by ......\\n\\nSQL Tokens could be divided into the following categories:\\n\uffee Keywords (select, from, where)\\n\uffee operator (+, -, >=)\\n\uffee Open/close flag ((, CASE)\\n\uffee placeholder (?)\\n\uffee Comments\\n\uffee space\\n......\\n```\\n## 2.2 Syntax analysis\\nThe syntax analysis will convert the token generated by the lexical analysis into an abstract syntax tree based on the syntax rules, as shown in Figure 2.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_2_en.png)\\n\\n## 2.3 Logical plan\\nThe logical plan converts the abstract syntax tree into an algebraic relation, which is an operator tree, and each node represents a calculation method for data. The entire tree represents the calculation method and flows direction of data, as shown in Figure 3.\\n\\n ![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_3_en.png)\\n\\n## 2.4 Physical plan\\nThe physical plan is the plan that determines which computing operations are performed on which machines. It will be generated based on the logical plan, the distribution of machines, and the distribution of data.\\n\\nThe SQL parsing of the Doris system also adopts these steps, but it is refined and optimized according to the characteristics of the Doris system structure and the storage method of data to maximize the computing power of the machine.\\n\\n# 3. Design goals\\nThe design goals of the Doris SQL parsing architecture are:\\n\\n1. Maximize Computational Parallelism\\n\\n2. Minimize network transfer of data\\n\\n3. Minimize the amount of data that needs to be scanned\\n\\n# 4. Architecture\\nDoris SQL parsing includes five steps: lexical analysis, syntax analysis, generation of a stand-alone logical plan, generation of a distributed logical plan, and generation of a physical execution plan.\\n\\nIn terms of code implementation, it corresponds to the following five steps: Parse, Analyze, SinglePlan, DistributedPlan, and Schedule, which as shown in Figure 4.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_4_en.png)\\n\\nThe Parse phase will not be discussed in this article. Analyze will do some pre-processing of the AST. A stand-alone query plan will be optimized by SinglePlan based on the AST. DistributedPlan will split the stand-alone query plan into distributed query plans. Schedule phase will determine which machines the query plan will be sent to for execution.\\n\\n**Since there are many types of SQL, this article focuses on the analysis of query SQL.**\\n\\nFigure 5 shows a simple query SQL parsing implementation in Doris.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_5_en.png)\\n\\n# 5. Parse Phase\\nIn the Parse stage, JFlex technology is used for lexical analysis, java cup parser technology is used for syntax analysis, and an AST\uff08Abstract Syntax Tree\uff09will finally generate. These are existing and mature technologies and will not be introduced in detail here.\\n\\nAST has a tree-like structure, which represents a piece of SQL. Therefore, different types of queries -- select, insert, show, set, alter table, create table, etc. will generate additional data structures after Parse (SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt, etc.). However, they all inherit from Statements and will perform some specific processing according to their own grammar rules. For example: for select type SQL, the SelectStmt structure will be generated after Parse.\\n\\nSelectStmt structure contains SelectList, FromClause, WhereClause, GroupByClause, SortInfo and other structures. These structures contain more basic data structures. For Example, WhereClause contains BetweenPredicate, BinaryPredicate, CompoundPredicate, InPredicate, and so on.\\n\\nAll structures in AST are composed of basic structure expressions--Expr by using various combinations, as shown in Figure 6.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_6_en.png)\\n# 6. Analyze Phase\\nAnalyze will perform pre-processing and semantic analysis on the abstract syntax tree AST generated in the Parse phase, preparing for the generation of stand-alone logic plans.\\n\\nThe abstract class StatementBase represents the abstract syntax tree. This abstract class contains a most crucial member function--analyze(), which is used to perform what\'s needed to do in Analyze phase.\\n\\nDifferent types of queries (select, insert, show, set, alter table, create table, etc.) will generate different data structures through the Parse stage(SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt, etc.), these data structures inherit From StatementBase, and perform a specific Analysis on a specific type sof SQL by implementing the analyze() function.\\n\\nFor example, a query of select type will be converted into analyze() of the sub-statements SelectList, FromClause, GroupByClause, HavingClause, WhereClause, SortInfo, etc. of select SQL. Then these sub-statements further analyze() their sub-structures, and various scenarios of various types of SQL are analyzed by layer-by-layer iteration. For example, WhereClause will further explore the BetweenPredicate, BinaryPredicate, CompoundPredicate, InPredicate, etc., which it contains.\\n\\n**For query type SQL, Analyze will performs several important steps:**\\n\\n- **Metadata identification and parsing**\uff1a Identify and parse metadata such as Cluster, Database, Table, Column, etc. involved in SQL, and determine which columns, tables, databases, and clusters need to be calculated.\\n\\n- **SQL correctness check**\uff1asuch as the window function cannot DISTINCT, whether the projection column is ambiguous, the where statement cannot contain grouping operations, etc.\\n\\n- **Rewrite SQL simply**\uff1afor example, expand select * to select all columns, convert count distinct to bitmap or hll function, etc.\\n\\n- **Function correctness check**\uff1aCheck whether the functions contained in SQL are consistent with the system-defined procedures, including parameter types, number of parameters, etc.\\n\\n- **Aliasing for Table and Column.**\\n\\n- **Type checking and conversion**\uff1a For example, when the types on both sides of a binary expression are inconsistent, one of the types needs to be converted (with BIGINT and DECIMAL, the BIGINT type needs to be cast to DECIMAL).\\n\\nAfter analyzing the AST, a rewrite operation will be performed again to simplify or convert it into a unified processing method. A present rewrite algorithm is a rule-based approach. It will rewrite the AST with each rule from bottom to top, based on the tree structure of the AST. If the AST changes after rewriting, analysis and rewrite will start again until there is no change in the AST.\\n\\nFor example: simplification of constant expressions: 1 + 1 + 1 is rewritten as 3, 1 > 2 is rewritten as Flase, etc. Convert some statements into a unified processing method, such as rewriting where in, where exists as semi join, where not in, where not exists as anti join.\\n\\n# 7. Generate stand-alone logical Plan phase\\nAt this stage, algebraic relations will be generated according to the AST abstract syntax tree, also known as the operator number. Each node on the tree is an operator, representing an operation.\\n\\nAs shown in Figure 7, ScanNode represents scan and read operations on a table. HashJoinNode represents the join operation. A hash table of a small table will be constructed in memory, and the large table will be traversed to find the exact value of the join key. Project means the projection operation, which represents the column that needs to be output at the end. Figure 7 shows that only citycode column will output.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_7_en.png)\\n\\nWithout optimization, the generated relational algebra is very expensive to send to storage and execute.\\n\\nFor query:\\n```sql\\nselect a.siteid, a.pv from table1 a join table2 b on a.siteid = b.siteid where a.citycode=122216 and b.username=\\"test\\" order by a.pv limit 10\\n```\\nAs shown in Figure 8, for unoptimized relational algebra, all columns need to be read out for a series of calculations. In the end, siteid and pv column are selected and output. A large amount of useless column data wastes computing resources.\\n\\nWhen Doris generates algebraic relations, a lot of optimizations are made: the projection columns and query conditions will be put into the scan operation as much as possible.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_8_en.png)\\n\\n**Specifically, this phase mainly does the following tasks:**\\n\\n- **Slot materialization**\uff1aDetermine the column that needs to be scanned and calculated for the expression. Such as aggregate function expressions and Group By words of aggregate nodes need to be materialized.\\n\\n- **Projection pushdown**\uff1aBE only scans the columns that must be read when Scanning.\\n\\n- **Predicate pushdown**\uff1aPush down the filter conditions to the Scan node as much as possible under the premise of semantically correct.\\n\\n- **Partition, bucket cutting**\uff1aAccording to the information in the filter conditions, determine which partitions and buckets of tablets need to be scanned.\\n\\n- **Join Reorder**\uff1aFor Inner Join, Doris will adjust the order of the table according to the number of rows--put the large table in the front.\\n\\n- **Sort + Limit optimized to TopN**\uff1aFor the order by the limit statement, it will be converted into TopN operation nodes, which is convenient for unified processing.\\n\\n- **MaterializedView selection**: The best-materialized view will be selected according to the columns required by the query, the columns for filtering, sorting and Join, the number of rows, the number of columns, and other factors.\\n\\nFigure 9 shows an example of optimization. The optimization of Doris is carried out in generating relational algebra. Generating one will optimize one.\xb7 Projection pushdown: BE only scans the columns that must be read when Scanning.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_9_en.png)\\n\\n# 8 Generate Distributed Plan Phase\\n\\nAfter the single-machine PlanNode tree is generated, it needs to be split into a distributed PlanFragment tree (PlanFragment is used to represent an independent execution unit) according to the distributed environment. A table\'s data is distributed across multiple hosts could allow some computations to be parallelized.\\n\\nThe primary purpose of this step is to maximize parallelism and data localization. The primary strategy is to split the nodes that can be executed in parallel and create a separate PlanFragment. ExchangeNodes will replace the split nodes to receive data. Finally, a DataSinkNode will be added to the split node to transmit the calculated data to the ExchangeNode for further processing.\\n\\nThis step adopts a recursive method, traverses the entire PlanNode tree from bottom to top, and then creates a PlanFragment for each leaf node on the tree. If the parent node is encountered, splitting the child nodes that can be executed in parallel will be considered.\\n\\nFor query operations, the join operation is the most common.\\n\\n**Doris currently supports four join algorithms:** broadcast join, hash partition join, colocate join, and bucket shuffle join.\\n\\n**broadcast join**\uff1aSend the small table to each machine where the large table is located and perform a hash join operation. When the amount of data scanned from a table is small, the cost of broadcast join will be calculated, and the method with the smallest cost will be selected by calculating and comparing the cost of hash partitions.\\n\\n**hash partition join**\uff1aWhen the data scanned from the two tables are both large, hash partition join is generally used. It traverses all the data in the table, calculates the hash value of the key, then modulizes the number of clusters, and whichever machine is selected, the data will be sent to this machine for hash join operation.\\n\\n**colocate join**\uff1aIf the data distribution of the two tables is specified to be consistent when they are created, the colocate join algorithm will be used when the join key of the two tables is the same as the bucket key. Since the data distribution of the two tables is the same, the hash join operation is equivalent to a local process. It does not involve data transmission, which significantly improves query performance.\\n\\n**bucket shuffle join**\uff1aWhen the join key is a bucketing key, and only one partition is involved, the bucket shuffle join algorithm is preferred. Since bucketing itself represents a way of dividing data, it only needs to take the hash modulo of the number of buckets from the right table to the left table, so that only one copy of the data in the right table needs to be transmitted over the network, which greatly reduces the network of data transmission, as shown in Figure 10.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_10_en.png)\\n\\nFigure 11 shows the core process of creating a distributed logical plan with a single-machine logical plan with HashJoinNode.\\n\\n- For PlanNodes, PlanFragments are created bottom-up.\\n\\n- If it is a ScanNode, PlanFragment will be created directly, and the RootPlanNode of the PlanFragment is this ScanNode.\\n\\n- If it is a HashJoinNode, the broadcastCost will be calculated at first, which could provide a reference for selecting boracast join or hash partition join.\\n\\n- Join algorithm will be chosen according to different conditions.\\n\\n- If colocate joins are used, since joins are all local, no splitting is required. Set the left child node of HashJoinNode as the RootPlanNode of leftFragment, and the right child node as the RootPlanNode of rightFragment, share a PlanFragment with leftFragment, and delete rightFragment.\\n\\n- If bucket shuffle join is used, data from the right table needs to be sent to the left table. So first create an ExchangeNode, set the left child node of HashJoinNode as the RootPlanNode of leftFragment, the right child node as this ExchangeNode, share a PlanFragment with leftFragment, and specify the destination of rightFragment data to be sent to this ExchangeNode.\\n\\n- If broadcast join is used, the data from the right table needs to be sent to the left table. So first create an ExchangeNode, set the left child node of HashJoinNode as the RootPlanNode of leftFragment, the right child node as this ExchangeNode, share a PlanFragment with leftFragment, and specify the destination of rightFragment data to be sent to this ExchangeNode.\\n\\n- If hash partition join is used, the data in the left table and the right table must be split, and both left and right nodes need to be split out to create left ExchangeNode and right ExchangeNode respectively. HashJoinNode specifies the left and right nodes as left ExchangeNode and right ExchangeNode. Create a PlanFragment separately and specify RootPlanNode as this HashJoinNode. Finally, specify the data sending destination of leftFragment and rightFragment as left ExchangeNode and right ExchangeNode.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_11_en.png)\\n\\nFigure 12 is an example after the join operation of two tables is converted into a PlanFragment tree, there are 3 PlanFragments generated. The final output data passes through the ResultSinkNode node.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_12_en.png)\\n\\n# 9. Schedule phase\\n\\nThis step is to create a distributed physical plan based on the distributed logical plan. will solve the following questions:\\n\\n- Which BE executes which PlanFragment\\n\\n- Which replica to chooes for each Tablet to query\\n\\n- How to perform multi-instance concurrency\\n\\n**Figure 13 shows the core process for creating a distributed physical plan:**\\n\\n**a. Prepare phase**\uff1aCreate a FragmentExecParams structure for each PlanFragment to represent all the parameters required for PlanFragment execution; if a PlanFragment contains DataSinkNode, the destination PlanFragment for data transmission will be found, and specify the input of FragmentExecParams of the destination PlanFragment as FragmentExecParams of this PlanFragment.\\n\\n**b. computeScanRangeAssignment phase**\uff1aDifferent processing is performed for different types of joins.\\n\\n- computeScanRangeAssignmentByColocate: For colocate join processing, since the data distribution in the two table buckets of the join is the same, they are based on the bucket join operation, so here is to determine which host is selected for each bucket. When allocating buckets to hosts, try to ensure that the buckets allocated to each host are even.\\n\\n- computeScanRangeAssignmentByBucket: Processing for bucket shuffle join, which is only based on bucket operations, so here is to determine which host is selected for each bucket. When allocating buckets to hosts, it is also necessary to try to ensure that the buckets allocated to each host are even.\\n\\n- computeScanRangeAssignmentByScheduler: Process for other types of joins. Determines which replica of the tablet each scanNode reads. A scanNode will read multiple tablets, and each tablet has various copies. To distribute the scan operation on various machines as much as possible, improve concurrent performance, and reduce IO pressure, Doris uses the Round-Robin algorithm to distribute tablet scans to multiple machines as much as possible. For example, 100 tablets need to be scanned, each tablet has three copies, and ten machines could be used. When allocating, each machine is guaranteed to scan ten tablets.\\n\\n**c.computeFragmentExecParams phase**\uff1aThis stage determines which BE the PlanFragment is issued to for execution and how to handle instance concurrency. After the scan address of each tablet is determined, FragmentExecParams will generate multiple instances with the address as the dimension. If various addresses are contained in FragmentExecParams, various instances of FInstanceExecParam will be generated. If the concurrency is set, the execution instance of an address will be further split into multiple FInstanceExecParams. There will be some special processing for bucket shuffle join and colocate join, but the basic logic is the same. After FInstanceExecParam is created, a unique ID will be assigned to facilitate tracking information. If FragmentExecParams contains ExchangeNode, the number of senders will be counted to know how many senders\' data needs to be accepted. Finally, FragmentExecParams determines the destinations and fills in the destination address.\\n\\n**d. Create result receiver stage**\uff1aThe resulting receiver is where the final data needs to be output after the query is completed.\\n\\n**e. to thrift stage**\uff1aCreate RPC requests based on FInstanceExecParam of all PlanFragments, then send them to the BE side for execution. A complete SQL parsing process is completed.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_13_en.png)\\n\\nFigure 14 is a simple example. The PlanFrament in the figure contains a ScanNode. The ScanNode scans three tablets. Each tablet has two copies, and the cluster assumes that there are two hosts.\\n\\nThe computeScanRangeAssignment stage determines that replicas 1, 3, 5, 8, 10, and 12 need to be scanned, where replicas 1, 3, and 5 are located on host1, and replicas 8, 10, and 12 are located on host2.\\n\\nIf the global concurrency is set to 1, 2 instances of FInstanceExecParam are created and sent to host1 and host2 for execution. If the global concurrency is set to 3, 3 instances of FInstanceExecParam are created on this host1, and three instances of FInstanceExecParam are created on host2. Each instance scans one replica, equivalent to initiating 6 RPC requests.\\n\\n![](/images/blogs/principle-of-Doris-SQL-parsing/Figure_14_en.png)\\n\\n# 10 Summary\\nThis article first briefly introduces Doris and then introduces the general process of SQL parsing: lexical analysis, syntax analysis, generating logical plans, and generating physical plans. Then, it presents the overall architecture of DorisSQL parsing. In the end, the five processes:  Parse, Analyze, SinglePlan, DistributedPlan, and Schedule are explained in detail, and an in-depth explanation is given of the algorithm principle and code implementation.\\n\\nDoris complies with the standard methods of SQL parsing. Still, according to the underlying storage architecture and distributed characteristics, many optimizations have been made in SQL parsing to achieve maximum parallelism and minimize network transmission, reducing a lot of burden on the SQL execution level."},{"id":"/Flink-realtime-write","metadata":{"permalink":"/blog/Flink-realtime-write","source":"@site/blog/Flink-realtime-write.md","title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","description":"\x3c!--","date":"2022-07-29T00:00:00.000Z","formattedDate":"July 29, 2022","tags":[{"label":"Tech Sharing","permalink":"/blog/tags/tech-sharing"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","summary":"With the increasing demand for real-time analysis, the timeliness of data is becoming more and more important to the refined operation of enterprises. With the massive data, real-time data warehouse plays an irreplaceable role in effectively digging out valuable information, quickly obtaining data feedback, helping companies make faster decisions and better product iterations.","date":"2022-07-29","author":"Apache Doris","tags":["Tech Sharing"]},"prevItem":{"title":"Doris analysis: Doris SQL principle analysis","permalink":"/blog/principle-of-Doris-SQL-parsing"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1.1","permalink":"/blog/release-1.1.1"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n\\nWith the increasing demand for real-time analysis, the timeliness of data is becoming more and more important to the refined operation of enterprises. With the massive data, real-time data warehouse plays an irreplaceable role in effectively digging out valuable information, quickly obtaining data feedback, helping companies make faster decisions and better product iterations.\\n\\nIn this situation, Apache Doris stands out as a real-time MPP analytic database, which is high performance and easy to use, and supports various data import methods. Combined with Apache Flink, users can quickly import unstructured data from Kafka and CDC(Change Data Capture) from upstream database like MySQL. Apache Doris also provides sub-second analytic query capabilities, which can effectively satisfy the needs of several real-time scenarios: multi-dimensional analysis, dashboard and data serving etc.\\n# Challange\\n\\nUsually, there are many challenges to ensure high end-to-end concurrency and low latency for real-time data warehouses , such as:\\n\\n- How to ensure end-to-end data sync in second-level ?\\n\\n- How to quickly ensure data visibility ?\\n\\n- How to solve the problem of small files writing under high concurrency situation?\\n\\n- How to ensure end-to-end Exactly-Once?\\n\\nWithin the challenges above , we conducted an in-depth research on the business scenarios of users using Flink and Doris to build real-time data warehouses . After grasping the pain points of users, we made targeted optimizations in Doris version 1.1 and greatly improved the user experience  and improved the stability. The resource consumption of Doris has also been greatly optimized.\\n\\n# Optimization\\n\\n### Streamming Write\\n\\nThe initial practice of Flink Doris Connector is to cache the data into the memory batch after receiving data.The method of data writing is saving batches, and using parameters such as `batch.size` and `batch.interval` to control the timing of Stream Load writing at the same time.\\n\\nIt usually runs stably when the parameters are reasonable. Whatever the parameters are unreasonable, it would cause frequent Stream Load and compaction untimely, resulting in excessive version errors ( -235 ). On the other hand, when there is too much data, in order to reduce the writing frequency of Stream Load , the setting of `batch.size` too large may also cause OOM.\\n\\n**To solve this problem, we introduce streaming write:**\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/otliigutb8p9l1y6qyp6.png)\\n- After the Flink task starts, the Stream Load Http request will be asynchronously initiated.\\n\\n- When the data is received, it will be continuously transmitted to Doris through the Chunked transfer encoding of Http.\\n\\n- Http request will end at Checkpoint and complete the Stream Load writing . The next Stream Load request will be asynchronously initiated at the same time.\\n\\n- The data will continue to be received and the follow-up process is the same as above.\\n\\nThe pressure on the memory of the batch is avoided since the Chunked mechanism is used to transmit data. And the timing of writing is bound to the Checkpoint, which makes the timing of Stream Load controllable, and provides a basis for the following Exactly-Once semantics.\\n\\n### Exactly-Once\\n\\nExactly-Once means that data will not be reprocessed or lost, even machine or application failure. Flink supports the End-to-End\'s Exactly-Once scenario a long time ago, mainly through the two-phase commit protocol to realize the Exactly-Once semantics of the Sink operator.\\n\\nOn the basis of Flink\'s two-stage submission, with the help of Doris 1.0\'s Stream Load two-stage submission,Flink Doris Connector implements Exactly Once semantics. The specific principles are as follows:\\n\\n- When the Flink task is started, it will initiate a Stream Load PreCommit request. At this time, a transaction will be opened first, and data will be continuously sent to Doris through the Chunked mechanism of Http.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ole5tqi91jibzdg9vqep.png)\\n\\n- Http request will be completed when the data writing ends at Checkpoint , and set the transaction status to preCommitted. The data has been written to BE and is invisible to the user at this time.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/jiieu1eff6smunkr85s5.png)\\n\\n- A Commit request will be initiated after the Checkpoint, and the transaction status will be set to Committed. The data will become visible to the user after request.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/eaona8eslljmkpaa9324.png)\\n\\n- After the Flink application ends unexpectedly and restarts from Checkpoint, if the last transaction was in the preCommitted state, a rollback request will be initiated and the transaction state will be set to Aborted.\\n\\nBased on the above , Flink Doris Connector can be used to realize real-time data storage without loss or weight.\\n\\n### Second- Level Data Synchronization\\n\\nEnd-to-end second-level data sync and real-time visibility of data in high concurrent write scenarios require Doris to have the following capabilities:\\n\\n- **Transaction Processing Capability**\\n\\nFlink real-time writing interacts with Doris in the form of Stream Load 2pc, which requires Doris to have the corresponding transaction processing capabilities to ensure the basic ACID characteristics, and support Flink\'s second-level data sync in high concurrency scenarios.\\n\\n- **Rapid Aggregation Capability of Data Versions**\\n\\nOne import in Doris will generate one data version. In a high concurrent write scenario, an inevitable impact is that there are too many data versions, and the amount of data imported in a single time will not be too large. The continuous high-concurrency small file writing scenario extremely tests the real-time ability and Doris\' data merging performance, which is not friendly to Doris, and in turn affects the performance of the query. Doris has greatly enhanced the data compaction capability in version 1.1, which can quickly complete the aggregation of new data, avoiding -235 errors and query efficiency problems which are caused by too many versions of sharded data.\\n\\nFirst of all, in Doris 1.1 version, QuickCompaction was introduced, which can actively triggered Compaction when the data version increased. At the same time, by improving the ability to scan fragment meta information, fragments that need to be compacted can be quickly discovered and trigger Compaction. Through active triggering and passive scanning, the real-time problem of data merging is completely solved.\\n\\nFor high-frequency small file Cumulative Compaction, the scheduling and isolation of Compaction tasks is implemented to prevent the heavyweight Base Compaction from affecting the merging of new data.\\n\\nFinally, the strategy of merging small files is optimized by adopting gradient merge method. Each time the files participating in the merging belong to the same data magnitude,which can prevent versions with large differences in size from merging, and gradually merges hierarchically, reducing the number of times a single file is involved in merging, which can greatly save the CPU consumption of the system.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ez5qdcpgwjw60g9aacqd.png)\\n\\nDoris version 1.1 has made targeted optimizations for scenarios such as high concurrent import, second-level data sync, and real-time data visibility, which greatly increases the ease of use and stability of the Flink system and Doris system, saves the overall resources of the cluster.\\n\\n# Effect\\n\\n### General Flink High Concurrency Scenarios\\n\\nIn the general scenario of the survey, Flink is used to synchronize unstructured data in upstream Kafka. The data is written to Doris in real time by the Flink Doris Connector after ETL.\\n\\nThe customer scenario is extremely strict here. The upstream maintains a high frequency of 10w per second, and the data needs to be able to complete the upstream and downstream sync within 5s to achieve second-level data visibility. Flink is configured with 20 concurrency, and the Checkpoint interval is 5s. The performance of Doris version 1.1 is quite excellent.\\n\\nSpecifically reflected in the following aspects:\\n\\n- **Compaction Real-Time**\\n\\nData can be merged quickly, the number of tablet data versions is kept below 50, and the compaction score is stable. Compared with the previous -235 problem in high concurrent import scenario, the compaction efficiency is improved more than 10 times.\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/d6enyv1zj68o7myjypnl.png)\\n\\n-  **CPU Resource Consumption**\\n\\nDoris version 1.1 has optimized the strategy for compaction of small files. In high-concurrency import scenarios, CPU resource consumption is reduced by 25%.\\n\\n- **QPS Query Delay is Stable**\\n\\nBy reducing the CPU usage and the number of data versions, the overall order of data has been improved, and the delay of SQL queries will be reduced.\\n\\n### Second-Level Data Synchronization Scenario (Extreme High Pressure)\\n\\nIn single bet and single tablet with 30 concurrent limit stream load pressure test on the client side, data in real-time <1s, the comparison before and after compaction score optimization as below:\\n\\n![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/r01hn8hv6arzbdclknis.png)\\n# Recommendations\\n\\n### Real-Time Data Visualization Scenario\\n\\nFor strict latency requirements scenarios, such as second-level data synchronization, usually mean that a single import file is small, and it is recommended to reduce `cumulative_size_based_promotion_min_size_mbytes `. The default unit is 64 MB, and you can set it to 8 MB manually, which can greatly improve the compaction real-time performance. \\n\\n### High Concurrency Scenario\\n\\nFor high concurrent writing scenarios, you can reduce the frequency of Stream Load by increasing the checkpoint interval. For example, setting checkpoint to 5-10s can not only increase the throughput of Flink tasks, but also reduce the generation of small files and avoid causing compaction more pressure.\\n\\nIn addition, for scenarios that do not require high real-time data, such as minute-level data sync, the checkpoint interval can be increased, such as 5-10 minutes. And the Flink Doris connector can still ensure the integrity of data through the two-stage submission and checkpoint mechanism.\\n\\n# Future planning\\n\\n-  **Real-time Schema Change**\\n\\nWhen accessing data in real time through Flink CDC, the upstream business table will perform the schema change operation, it has to modify the schema manually in Doris and Flink tasks. In the end, the data of the new schema can be synchronized after restart the task . \\n\\nThis way requires human intervention, which will bring a great operation burden to users. In subsequent versions, real-time schema changes will support CDC scenarios, and the upstream schema changes will be synchronized to the downstream in real-time, which will comprehensively improve the efficiency of schema changes.\\n\\n-  **Doris Multi-table Writting**\\n\\nAt present, the Doris Sink operator only supports synchronizing a single table, so for the entire database, it still has to divide the flow manually at the Flink level and write to multiple Doris Sinks, which will increase the difficulty of developers. In subsequent versions, we will support a single Doris Sink to synchronize multiple tables, which greatly simplifies the user\'s operation.\\n\\n-  **Adaptive Compaction Parameter Tuning**\\n\\nAt present, the compaction strategy has many parameters, which can play a good role in most general scenarios, but these strategies still can\'t play an efficient role in some special scenarios. We will continue to optimize in subsequent versions, carry out adaptive compaction tuning for different scenarios, and keep improving data merging efficiency and real-time performance in various scenarios.\\n\\n-  **Single-Copy Compaction**\\n\\nThe current compaction strategy is that each BE is carried out separately. In subsequent versions, we will implement single-copy compaction, and realize compaction tasks by cloning snapshots, reduce system load while reducing about 2/3 compaction tasks of the cluster, leaving more system resources to the user side."},{"id":"/release-1.1.1","metadata":{"permalink":"/blog/release-1.1.1","source":"@site/blog/release-1.1.1.md","title":"Apache Doris announced the official release of version 1.1.1","description":"\x3c!--","date":"2022-07-29T00:00:00.000Z","formattedDate":"July 29, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1.1","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1.1 on July 29, 2022! This release is a hotfix version of 1.1.0","date":"2022-07-29","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"How Flink\'s real-time writes to Apache Doris ensure both high throughput and low latency","permalink":"/blog/Flink-realtime-write"},"nextItem":{"title":"Best Practice of Apache Doris in JD","permalink":"/blog/jd"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n## Features\\n\\n### Support ODBC Sink in Vectorized Engine.\\n\\nThis feature is enabled in non-vectorized engine but it is missed in vectorized engine in 1.1. So that we add back this feature in 1.1.1.\\n\\n### Simple Memtracker for Vectorized Engine.\\n\\nThere is no memtracker in BE for vectorized engine in 1.1, so that the memory is out of control and cause OOM. In 1.1.1, a simple memtracker is added to BE and could control the memory and cancel the query when memory exceeded.\\n\\n## Improvements\\n\\n### Cache decompressed data in page cache.\\n\\nSome data is compressed using bitshuffle and it costs a lot of time to decompress it during query. In 1.1.1, doris will decompress the data that encoded by bitshuffle to accelerate query and we find it could reduce 30% latency for some query in ssb-flat.\\n\\n## Bug Fix\\n\\n### Fix the problem that could not do rolling upgrade from 1.0.(Serious)\\n\\nThis issue was introduced in version 1.1 and may cause BE core when upgrade BE but not upgrade FE.\\n\\nIf you encounter this problem, you can try to fix it with [#10833](https://github.com/apache/doris/pull/10833).\\n\\n### Fix the problem that some query not fall back to non-vectorized engine, and BE will core.\\n\\nCurrently, vectorized engine could not deal with all sql queries and some queries (like left outer join) will use non-vectorized engine to run. But there are some cases not covered in 1.1. And it will cause be crash.\\n\\n### Compaction not work correctly and cause -235 Error.\\n\\nOne rowset multi segments in uniq key compaction, segments rows will be merged in generic_iterator but merged_rows not increased. Compaction will failed in check_correctness, and make a tablet with too much versions which lead to -235 load error.\\n\\n### Some segment fault cases during query.\\n\\n[#10961](https://github.com/apache/doris/pull/10961) \\n[#10954](https://github.com/apache/doris/pull/10954) \\n[#10962](https://github.com/apache/doris/pull/10962)\\n\\n# Thanks\\n\\nThanks to everyone who has contributed to this release:\\n\\n```\\n@jacktengg\\n@mrhhsg\\n@xinyiZzz\\n@yixiutt\\n@starocean999\\n@morrySnow\\n@morningman\\n@HappenLee\\n```"},{"id":"/jd","metadata":{"permalink":"/blog/jd","source":"@site/blog/jd.md","title":"Best Practice of Apache Doris in JD","description":"\x3c!--","date":"2022-07-20T00:00:00.000Z","formattedDate":"July 20, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Best Practice of Apache Doris in JD","summary":"This paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1.1","permalink":"/blog/release-1.1.1"},"nextItem":{"title":"Best Practice of Apache Doris in Meituan","permalink":"/blog/meituan"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# **Introduction\uff1a**\\n\\n\\n\\nApache Doris is an open source MPP analytical database product that not only can get query results in sub-second response time, effectively supporting real-time data analysis, but also supports huge data sets of more than 10PB. Compared with other industry-hot OLAP database systems, the distributed architecture of Apache is very simple. Itsupports elastic scaling and is easy to operate and maintain, saving a lot of labor and time costs. At present, the domestic community is very popular , and there are also many companies which have large scale uses, such as Meituan and Xiaomi,etc. \\n\\n\\n\\nThis paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.\\n\\n\\n\\nIn recent years, with the explosive growth of data volume and the emergence of the demand for online analysis of massive data, traditional relational databases such as MySQL and Oracle have encountered bottlenecks under large data volume, while databases such as Hive and Kylin lack timeliness. So Apache Doris, Apache Druid, ClickHouse and other real-time analytic databases begun to appear, not only to cope with the second-level queries of massive data, but also to meet the real-time and quasi-real-time analysis needs. Offline and real-time computing engines are in full bloom. But for different scenarios and facing different problems, no single engine is a panacea. We hope that this article can give you some inspiration on the application and practice of offline and real-time analytics in JD\'s customer service business, and we hope you will communicate more and give us valuable suggestions.\\n\\n# **JD Customer Service Business Form**\\n\\n\\n\\nAs the entrance to the group\'s services, JD Customer Service provides efficient and reliable protection for users and merchants. JD customer service is responsible for solving users\' problems in a timely manner and providing them with detailed and easy-to-understand instructions and explanations; in order to better understand users\' feedback and the status of products, it is necessary to monitor a series of indicators such as the number of inquiries, pick-up rates, complaints, etc. in real time, and discover problems in a timely manner through ring comparison and year-on-year comparison, in order to better adapt to users\' shopping styles, improve service quality and efficiency, and thus enhance the brand of JD influence.\\n\\n\\n\\n# **Easy OLAP Design**\\n\\n\\n\\n### **01 EasyOLAP Doris Data Import Links**\\n\\n\\n\\nEasyOLAP Doris data sources are mainly real-time Kafka and offline HDFS files. The import of real-time data relies on Routine Load; offline data is mainly imported using Broker Load and Stream Load.\\n\\n\\n\\n![1280X1280](/images/jd03.png)\\n\\n\\n\\nEasyOLAP Doris Data Import Links\\n\\n\\n\\n### **02 EasyOLAP Doris Full Link Monitor**\\n\\n\\n\\nThe EasyOLAP Doris project currently uses the Prometheus + Grafana framework for monitoring. The node_exporter is responsible for collecting machine-level metrics, and Doris automatically spits out FE and BE service-level metrics in Prometheus format. In addition, OLAP Exporter service is deployed to collect Routine Load related metrics, aiming to discover real-time data stream import at the first time and ensure real-time data timeliness.\\n\\n\\n\\n![EasyOLAP Doris monitoring link](/images/jd04.png)\\n\\n\\n\\nEasyOLAP Doris monitoring link\\n\\n![640](/images/jd01.png)\\n\\n### **03 EasyOLAP Doris Primary-Secondary Dual Stream Design**\\n\\n\\n\\nEasyOLAP Doris adopts a dual-write approach for the primary and secondary clusters in order to guarantee the service stability of Level 0 services during the promotion time.\\n\\n\\n\\n![03 EasyOLAP Doris Primary-Secondary Dual Stream Design](/images/jd02.png)\\n\\n\\n\\nEasyOLAP Doris Primary-Secondary Dual Stream Design\\n\\n\\n\\n### **04 EasyOLAP Doris Dynamic Partition Management**\\n\\n\\n\\nAfter analyzing the requirements, the JD OLAP team did some customization of Doris, which involved dynamic partition management. Although the community version already had the function of dynamic partitioning, the function could not retain partitions of a specified time. For the characteristics of JD Group, we have retained historical data of specified time, such as data during 618 and 11.11, which will not be deleted due to dynamic partitioning. The dynamic partition management feature can control the amount of data stored in the cluster, and it is easy to use by the business side without the need to manage partition information manually or with additional code.\\n\\n\\n\\n# **Doris Caching Mechanism**\\n\\n\\n\\n### **01 Demand Scenarios**\\n\\n\\n\\nCommitted to continuously improving user experience, JD Customer Service\'s data analysis pursues the ultimate timeliness. Offline data analysis scenario is write less read more, data is written once and read frequently many times; real-time data analysis scenario, part of the data is not updated historical partition, part of the data is in the updated partition. In most analysis applications, there are the following scenarios:\\n\\n- High concurrency scenario: Doris better support high concurrency, but too high QPS will cause cluster jitter, and a single node can not carry too high QPS;.\\n\\n- Complex queries: JD customer service real-time operation platform monitoring needs to display multi-dimensional complex indicators according to business scenarios, rich indicators display corresponding to a variety of different queries, and data sources from multiple tables . Although the response time of individual queries at milliseconds level , the overall response time may be at the second level.\\n\\n- Repeated queries: if there is no anti-refresh mechanism, repeatedly refreshing the page will lead to the submission of a large number of repeated queries due to delays or hand errors.\\n\\nFor the above scenario, there are solutions at the application layer \u2014\u2014 the query results are put into Redis and the cache is refreshed periodically or manually by the user, but there are some problems\uff1a\\n\\n- Data inconsistency: can not respond immediately to data updates, and the user may receive results with old data.\\n\\n- Low hit rate: if the data is highly real-time and the cache is frequently invalidated, the hit rate of the cache is low and the load on the system cannot be relieved.\\n\\nAdditional cost: introduction of external components increases system complexity and adds additional cost.\\n\\n\\n\\n### **02 Introduction to Caching Mechanism**\\n\\n\\n\\nThere are three different types of Cache in EasyOLAP Doris, respectively Result Cache, SQL Cache and Partition Cache, depending on the applicable scenario. All three types of caches can be switched on and off by MySQL client commands.\\n\\nThese three caching mechanisms can coexist: which can be turned on at the same time. When querying, the query parser first determines whether the Result Cache is enabled or not, and if the Result Cache is enabled, it first finds out whether the cache exists for the query from the Result Cache, and if the cache fails or does not exist, it directly takes the cached value and returns it to the client. The cache is placed in the memory of each FE node for fast reading.\\n\\nSQL Cache stores and gets the cache according to the signature of SQL, the ID of the partition of the queried table, and the latest version number of the partition. These three together serve as cache conditions. If one of these three conditions is changed, such as SQL statement change or partition version number change after data update, the cache will not be hit. In the case of multiple table joins, the partition update of one of the tables will also result in failure to hit the cache. SQL Cache is more suitable for T+1 update scenarios.\\n\\nPartition Cache is a more fine-grained caching mechanism. Partition cache mainly splits a query into read-only partition and updatable partition in parallel based on partition, read-only partition is cached, updatable partition is not cached, and the corresponding result set is generated n, and then the results of each split subquery are merged. Therefore, if the query N days of data, data update the most recent D days, each day is only a different date range but similar queries, you can use Partition Cache, only need to query D partitions can be, the other parts are from the cache, can effectively reduce the cluster load, shorten the query response time.\\n\\nWhen a query enters Doris, the system will first process the query statement and take it as the key, before executing the query statement, the query analyzer can automatically select the most suitable caching mechanism to ensure that the caching mechanism is used to shorten the query response time in the best case. Then, it checks whether the query result exists in the Cache, and if it does, it gets the data in the cache and returns it to the client; if it does not, it queries normally and stores the query result as Value and the query statement Key in the cache. SQL Cache is more suitable for T+1 scenarios and works well when partition updates are infrequent and SQL statements are repetitive Partition Cache is the least granular cache. When a query statement queries data for a time period, the query statement is split into multiple subqueries. It can shorten the query time and save cluster resources when the data is written to only one partition or partial partition.\\n\\nTo better observe the effectiveness of caching, metrics have been added to Doris\' service metrics, which are monitored visually through Prometheus and Grafana monitoring systems. The metrics include the number of hits for different types of Cache, the hit rate for different types of Cache, the memory size of the Cache, and other metrics.\\n\\n\\n\\n### **03 Caching Mechanism Effect**\\n\\n\\n\\nFor the JD Customer Service Doris main cluster, some services reached 100% CPU usage during 11.11 period without caching on; with Result Cache on, CPU usage was between 30% and 40%. The caching mechanism ensures that the business can get the query results quickly and protects the cluster resources well under high concurrency scenarios.\\n\\n\\n\\n# **Doris\' optimization during the 11.11 sale, 2020**\\n\\n\\n\\n### **01 Import Task Optimization**\\n\\n\\n\\nThe import of real-time data has always been a challenge. Among them, ensuring real-time data and importing stability is the most important. In order to observe the real-time data import situation more intuitively, JD OLAP team developed OLAP Exporter independently to collect real-time data import-related metrics, such as import speed, import backlog and suspended tasks. The import speed and import backlog can be used to determine the status of a real-time import task, and if find a trend of backlog, the sampling tool developed independently can be used to sample and analyze the real-time task. Real-time tasks have three main thresholds to control the submission of tasks, which are the maximum processing interval per batch, the maximum number of processing entries per batch and the maximum amount of data processed per batch, and a task will be submitted as soon as one of these thresholds is reached. By increasing the logs, we found that the task queue in FE was relatively busy, so the parameters were mainly adjusted to make the maximum number of processing entries per batch and the maximum amount of data processed per batch larger, and then the maximum processing interval per batch was adjusted to ensure that the data latency was within twice the maximum processing interval per batch according to the business requirements. Through the sampling tool, the analysis task ensures not only the real-time data, but also the stability of the import. In addition, we also set up alarms to detect abnormalities such as backlog of real-time import tasks and suspension of import tasks in a timely manner.\\n\\n\\n\\n### **02 Monitoring Metrics Optimization**\\n\\n\\n\\nThe monitoring metrics are divided into two main sections, a machine level metrics section and a business level metrics section. In the whole monitoring panel, detailed metrics bring comprehensive data and at the same time make it more difficult to get important metrics. So, to get a better view of important metrics for all clusters, a separate panel is created - 11.11 Important Metrics Summary Panel. The board contains metrics such as BE CPU usage, real-time task consumption backlog rows, TP99, QPS, and so on. The number of metrics is small, but the situation of all clusters can be observed, which can eliminate the trouble of frequent switching in monitoring.\\n\\n\\n\\n### **03 Peripheral Tools Support**\\n\\n\\n\\nIn addition to the sampling tools and OLAP Exporter mentioned above, the JD OLAP team has also developed a series maintenance tools for Doris.\\n\\n\\n\\n1. Import sampling tool: The import sampling tool not only collects the data imported in real time, but also supports adjusting the parameters of the real time import task, or generating creation statements (including the latest loci and other information) for task migration and other operations when the real time import task is paused.\\n\\n   \\n\\n2. Big query tool: Big queries not only cause jitter in cluster BE CPU usage, but also lead to longer response time for other queries. Before the Big Query tool, if you found jitter in cluster CPU, you needed to check the audit logs on all FEs and then do the statistics, which is not only time-consuming but also not intuitive. The Big Query tool is designed to solve the above problem. When the monitoring side finds that the cluster has jitter, you can use the Big Query tool and enter the cluster name and time point to get the total number of queries for different services at that time point, the number of queries with more than 5 seconds, 10 seconds, 20 seconds, the number of queries with huge scanning volume, etc. It is convenient for us to analyze the big queries from different dimensions. The details of the big queries will also be saved in the intermediate file, which can directly get the big queries of different businesses. The whole process only takes a few tens of seconds to a minute to locate the big query that is happening and get the corresponding query statements, which greatly saves time and operation and maintenance costs.\\n\\n   \\n\\n3. Downgrade and recovery tools: In order to ensure the stability of the Level 0 business during the 11.11 promotion, when the cluster pressure exceeds the safety level, it is necessary to downgrade other non-Level 0 businesses, and then restore them to the pre-downgrade settings with one click after the peak period. The degradation mainly involves reducing the maximum number of connections to the service, suspending non-level 0 real-time import tasks, and so on. This greatly increases the ease of operation and improves efficiency.\\n\\n   \\n\\n4. Cluster inspection tool: During 11.11 period, the health inspection of clusters is extremely important. Routine inspections include primary and secondary cluster consistency checks for dual-stream services. In order to ensure that the business can quickly switch to the other cluster when one cluster has problems, it is necessary to ensure that the library tables on both clusters are consistent and the data volume is not too different; check whether the number of copies of the library tables is 3 and whether there are unhealthy Tablet in the cluster; check the machine disk utilization, memory and other machine-level indicators, etc. Check the machine disk utilization, memory and other machine-level metrics, etc.\\n\\n   \\n\\n   # **Summary & Outlook**\\n\\n   \\n\\n   JD Customer Service was introduced to Doris in early 2020, and currently has one standalone cluster and one shared cluster, and is an experienced user of JD OLAP.\\n\\n   \\n\\n   In the business use, we also encountered problems such as task scheduling-related, import task configuration-related and query-related problems, which are driving the JD OLAP team to understand Doris more deeply. We plan to promote the use of materialized views to further improve the efficiency of queries; use Bitmap to support accurate de-duplication of UV and other metrics; use audit logs to make it easier to count large and slow queries; and solve the scheduling problem of real-time import tasks to make them more efficient and stable. In addition, we also plan to optimize table building, create high-quality Rollup or materialized views to improve the smoothness of the application, and accelerate more businesses to the OLAP platform to improve the impact of the application."},{"id":"/meituan","metadata":{"permalink":"/blog/meituan","source":"@site/blog/meituan.md","title":"Best Practice of Apache Doris in Meituan","description":"\x3c!--","date":"2022-07-20T00:00:00.000Z","formattedDate":"July 20, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Best Practice of Apache Doris in Meituan","summary":"Introduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"]},"prevItem":{"title":"Best Practice of Apache Doris in JD","permalink":"/blog/jd"},"nextItem":{"title":"Best Practice of Apache Doris in Xiaomi Group","permalink":"/blog/xiaomi"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Best Practice of Apache Doris in Meituan\\n\\nIntroduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.\\n\\n# real-time scene\\n\\nThere are many scenarios in which real-time data is delivered in Meituan, mainly including these following points:\\n\\n- Operational level: Such as real-time business changes, real-time marketing effects, daily business status and daily real-time business trend analysis, etc.\\n- Production level: such as whether the real-time system is reliable, whether the system is stable, real-time monitoring of the health of the system, etc.\\n- C-end users: For example, search recommendation sorting requires real-time understanding of users\' thoughts, behaviors and characteristics, and recommendation of more concerned content to users.\\n- Risk control: Food delivery and financial technology are used a lot. Real-time risk identification, anti-fraud, abnormal transactions, etc., are all scenarios where a large number of real-time data are applied\\n\\n# Real-time technology and architecture\\n\\n### 1.Real-time computing technology selection\\n\\nAt present, there are many open source real-time technologies, among which Storm, Spark Streaming and Flink are common. The specific selection depends on the business situation of different companies.\\n\\nMeituan Takeaway relies on the overall construction of meituan\'s basic data system. In terms of technology maturity, It used Storm a few years ago, which was irreplaceable in terms of performance stability, reliability and scalability. As Flink becomes more and more mature, it has surpassed Storm in terms of technical performance and framework design advantages. In terms of trends, just like Spark replacing MR, Storm will be gradually replaced by Flink. Of course, there will be a process of migrating from Storm to Flink. We currently have some old tasks still on Storm, and we are constantly promoting task migration.\\n\\nThe comparison between Storm and Flink can refer to the form above.\\n\\n### 2.Real-time Architecture\\n\\n#### \u2460 Lambda Architecture\\n\\nThe Lambda architecture is a relatively classic architecture. In the past, there were not many real-time scenarios, mainly offline. When a real-time scene is attached, the technical ecology is different due to the different timeliness of offline and real- time. The Lambda architecture is equivalent to attaching a real-time production link, which is integrated at the application level, and two-way production is independent of each other.This is also a logical approach to adopt in business applications.\\n\\nThere will be some problems in dual-channel production, such as double processing logic, double development and operation and maintenance, and resources will also become two resource links. Because of these problems,  Kappa architecture has been evolved.\\n\\n#### \u2461 Kappa Architecture\\n\\nThe Kappa architecture is relatively simple in terms of architecture design, unified in production, and a set of logic produces both offline and real time. However, there are relatively large limitations in practical application scenarios. There are few cases in the industry that directly use the Kappa architecture for production and implementation,  and the scene is relatively simple. These problems will also be encountered on our side, and we will also have some thoughts of our own, which will be discussed later.\\n\\n# Business Pain Points\\n\\nIn the take-away business, we also encountered some problems.\\n\\nIn the early stage of the business, in order to meet the business needs, the requirements are generally completed case by case after the requirements are obtained. The business has high real-time requirements. From the perspective of timeliness, there is no opportunity for middle-level precipitation. In the scenario, the business logic is generally directly embedded. This is a simple and effective method that can be imagined. This development mode is relatively common in the early stage of business development.\\n\\nAs shown in the figure above, after getting the data source, it will go through data cleaning, dimension expansion, business logic processing through Storm or Flink, and finally direct business output. Taking this link apart, the data source will repeatedly refer to the same data source, and the operations such as cleaning, filtering, and dimension expansion must be repeated. The only difference is that the code logic of the business is different. IIf there is less business, this model is acceptable, but when the subsequent business volume increases, there will be a situation where whoever develops will be responsible for operation and maintenance, the maintenance workload will increase, and the operations cannot be managed in a unified manner. Moreover, everyone is applying for resources, resulting in a rapid expansion of resource costs, and resources cannot be used intensively and effectively. Therefore, it is necessary to think about how to construct real-time data from the whole data source.\\n\\n# Data features and Application Scenario\\n\\nSo how to build a real-time data warehouse?\\n\\nFirst of all, we need to disassemble this task into what data, what scenarios, and what features these scenarios have in common. For takeaway business scenarios, there are two categories, log class and business category.\\n\\n- Log class: It is characterized by a large amount of data, semi-structured, and deeply nested.Log data has a great feature that once the log stream is formed, it will not change. It will collect all the logs of the platform by means of buried points, and then collect and distribute them uniformly. Just like a tree with really large roots.  The whole process of pushing to the front-end application is just like the process of a tree branching from the root to a branch (the decomposition process from 1 to n). If all businesses search for data from the root, although the path seems to be the shortest, because of the heavy burden,the data retrieval efficiency is low. Log data is generally used for production monitoring and user behavior analysis. The timeliness requirements are relatively high . Generally, the  time window will be 5 minutes or 10 minutes, or up to the current state. The main application is the real-time large screen and real-time features, such as behaviour can immediately perceive the need for waiting every time the user clicks.\\n\\n- Business category: The business class is mainly about business transaction data. Business systems are usually self-contained and distribute data down in the form of Binlog logs. All business systems are transactional, mainly using paradigm modeling methods, which have a structured characteristic and the main part can be seen clearly. However, due to the large number of data tables, multi-table associations are required to express the complete business. So it\'s an integrated machining process from n to 1 .\\n\\nSeveral difficulties faced by business real-time processing:\\n\\n- Diversity of business: Business processes are constantly changing from the beginning to the end, such as from ordering -> payment -> delivery. The business database is changed  on the original basis,and Binlog will generate a lot of changed logs. Business analysis is more focused on the end state, which leads to the problem of data retraction calculation, such as placing an order at 10 o\'clock and canceling it at 13 o\'clock, but hoping to subtract the canceled order at 10 o\'clock.\\n\\n- Business integration: Business analysis data usually cannot be expressed by a single subject, and often many tables are associated to obtain the desired information. When confluent alignment of data is performed in real-time streaming, it often requires large cache processing and is complicated.\\n\\n- The analysis is batch, and the processing process is streaming: for a single data, no analysis can be formed, so the analysis object must be batch, and the data processing is one by one.\\n\\nThe scenarios of log classes and business classes generally exist at the same time and are intertwined. Whether it is Lambda architecture or Kappa architecture, a single application will have some problems, so it is more meaningful to choose the architecture and practice according to the scenario.\\n\\n# Architecture Design of Real-time Data Warehouse\\n\\n### 1.Real-time Architecture: Exploration of Stream-Batch Combination\\n\\nBased on the above problems, we have our own thinking and ideas\uff0cit is to deal with different business scenarios through the combination of flow and batch.\\n\\nAs shown in the figure above, the data is collected from the log to the message queue, and then to the ETL process of the data stream. The construction of the basic data stream is unified. Afterwards, for log real-time features, real-time large-screen applications use real-time stream computing. Real-time OLAP batch processing is used for Binlog business analysis.\\n\\nWhat are the Pain Points of Stream Processing Analysis Business? For the paradigm business, both Storm and Flink require a large amount of external memory to achieve business alignment between data streams, which requires a lot of computing resources. Due to the limitation of external memory, the window limitation strategy must be carried out, and may eventually discard some data as a result. After calculation, it is generally stored in Redis as query support, and KV storage has many limitations in dealing with analytical query scenarios.\\n\\nHow to achieve real-time OLAP? Is there a real-time computing engine with its own storage, when the real-time data is entered,it can flexibly and freely calculate within a certain range, and has a certain data carrying capacity, and supports analysis of query responses at the same time? With the development of technology, the current MPP engine is developing very rapidly, and its performance is also improving rapidly, so there is a new possibility in this scenario, just like the Doris engine we use here.\\n\\nThis idea has been practiced in the industry and has become an important exploration direction. For example, Alibaba\'s real-time OLAP solution based on ADB, etc.\\n\\n### 2.Architecture Design of Real-time Data Warehouse\\n\\nFrom the perspective of the entire real-time data warehouse architecture, the first thing to consider is how to manage all real-time data, how to effectively integrate resources, and how to construct data.\\n\\nIn terms of methodology, the real-time and offline are very similar to each other. In the early stage of offline data warehouse, it is also case by case. Consider how to govern it when the scale of data increases to a certain amount. We all know that layering is a very effective way of data governing. So, on the issue of how to manage the real-time data warehouse, the first consideration is also the hierarchical processing logic, as follows:\\n\\n- Data source: At the data source level, offline and real-time data sources are consistent. They are mainly divided into log classes and business classes. Log classes include user logs, DB logs, and server logs.\\n\\n- Real-time detail layer: At the detail level, in order to solve the problem of repeated construction, a unified construction should be carried out.Using the offline data warehouse model to build a unified basic detailed data layer, managed according to the theme, the purpose of the detail layer is to provide directly available data downstream, so the basic layer should be processed uniformly, such as cleaning, filtering, and dimension expansion.\\n\\n- Aggregation layer: The summary layer can directly calculate the result through the concise operator of Flink or Storm. And form a summary of indicators, all indicators are processed at the summary layer, and everyone manages and constructs according to unified specifications, forming a reusable summary result.\\n\\nIn conclusion, from the perspective of the construction of the entire real-time data warehouse,first of all, the data construction needs to be layered, build the framework first, and set the specifications includs  what extent each layer is processed and how each layer is used.The definition of specifications facilitates standardized processing in production.Due to the need to ensure timeliness, don\'t design too many layers when designing.For scenarios with high real-time requirements, you can basically refer to the left side of the figure above. For batch processing requirements, you can import from the real-time detail layer to the real-time OLAP engine, and perform fast retraction calculations based on the OLAP engine\'s own calculation and query capabilities, as shown in the data flow on the right side of the figure above.\\n\\n# Real-time platform construction\\n\\nAfter the architecture is determined, the next consideration is how to build a platform.The construction of the real-time platform is completely attached to the real-time data warehouse management.\\n\\nFirst, abstract the functions and abstract them into components, so that standardized production can be achieved, and systematic guarantees can be further constructed. For the basic processing layer cleaning, filtering, confluence, dimension expansion, conversion, encryption, screening and other functions can be abstracted, and the base layer builds a directly usable data result stream in this componentized way. How to meet diverse needs and how to be compatible with users are the problems that we need to figure out. In this case it may occur problems with redundant processing. In terms of storage, real-time data does not have a history and will not consume too much storage. This redundancy is acceptable.The production efficiency can be improved by means of redundancy, which is an ideological application of changing space for time.\\n\\nThrough the processing of the base layer, all data is deposited in the IDL layer, and written to the base layer of the OLAP engine at the same time, and then the real-time summary layer is calculated. Based on Storm, Flink or Doris, multi-dimensional summary indicators are produced to form a unified summary layer for unified storage distribution.\\n\\nWhen these functions are available, system capabilities such as metadata management, indicator management, data security, SLA, and data quality will be gradually built.\\n\\n### 1.Real-time base layer functions\\n\\nThe construction of the real-time base layer needs to solve some problems.\\n\\nThe first is the problem of repeated reading of a stream. When a Binlog is called, it exists in the form of a DB package. Users may only use one of the tables. If everyone wants to use it, there may be a problem that everyone needs to access this stream. The solution can be deconstructed according to different businesses, restored to the basic data flow layer, made into a paradigm structure according to the needs of the business, and integrated with the theme construction according to the modeling method of the data warehouse.\\n\\nSecondly, we need to encapsulate components, such as basic layer cleaning, filtering, and dimension expansion . Users can write logic by a very simple expression. Trans part is more flexible. For example, converting from one value to another value, for this custom logic expression, we also open custom components, which can develop custom scripts through Java or Python for data processing.\\n\\n### 2.Real-time feature production capabilities\\n\\nFeature production can be expressed logically through SQL syntax, and the underlying logic is adapted, and transparently transmitted to the computing engine, shielding the user\'s dependence on the computing engine.Just like for offline scenarios, currently large companies rarely develop through code, unless there are some special cases, so they can basically be expressed in SQL.\\n\\nAt the functional level, the idea of indicator management is integrated. Atomic indicators, derived indicators, standard calculation apertures, dimension selection, window settings and other operations can be configured in a configurable way.In this way, the production logic can be uniformly parsed and packaged uniformly.\\n\\nAnother question,with the same source code a lot of SQL is written, and each submission will have a data stream which is a waste of resources.Our solution is to produce dynamic metrics through the same data stream, so that metrics can be added dynamically without stopping the service.\\n\\nSo, during the construction of the real-time platform, engineers should consider more about how to use resources more effectively and which links can use resources more economically.\\n\\n### 3.SLA construction\\n\\nSLA mainly solves two problems, one is about the end-to-end SLA, the other is  about the SLA of job productivity. We adopt the method of burying points + reporting.Because the real-time stream is relatively large, the burying point should be as simple as possible, do not bury too many things,can express the business information is enough.The output of each job is reported to the SLA monitoring platform in a unified manner, and the required information is reported at each job point through a unified interface, and finally the end-to-end SLA can be counted.\\n\\nIn real-time production, because the process is very long, it is impossible to control all links, but it can control the efficiency of its own operations, so job SLA is also essential.\\n\\n### 4.Real-time OLAP solution\\n\\nProblems:\\n\\n- Binlog business restoration is complex\uff1aThere are many changes in the business, and changes at a certain point in time are required. Therefore, sorting and data storage are required, which consumes a lot of memory and CPU resources.\\n\\n- Binlog business association is complex\uff1aIn stream computing, the relationship between streams and streams is very difficult to express business logic.\\n\\nsolutions\uff1a\\n\\nTo solve the problem through the OLAP engine with computing power, there is no need to logically map a data stream, and only the problem of real-time and stable data storage needs to be solved.\\n\\nWe use Doris as a high-performance OLAP engine here.Due to the need for derivative calculations between the results generated by the business data and the results, Doris can quickly restore the business by using the unique model or the aggregation model, and can also perform aggregation at the summary layer while restoring the business,and is also designed for reuse.The application layer can be physical or logical view.\\n\\nThis mode focuses on solving the business rollback calculation. For example, when the business state changes, the value needs to be changed at a certain point in history. The cost of using flow calculation in this scenario is very high. The OLAP mode can solve this problem very well.\\n\\n# Real-time use cases\\n\\nIn the end, we use a case to illustrate.For example, merchants want to offer discounts to users based on the number of historical orders placed by users. Merchants need to see how many orders they have placed in history. They must have historical T+1 data and real-time data today.This scenario is a typical Lambda architecture,You can design a partition table in Doris, one is the historical partition, and the other is the today partition. The historical partition can be produced offline. Today\'s indicators can be calculated in real time and written to today\'s partition. When querying, a simple summary.\\n\\nThis scenario seems relatively simple, but the difficulty lies in the fact that many simple problems will become complicated after the number of merchants increases.Therefore, in the future, we will use more business input to precipitate more business scenarios, abstract them to form a unified production plan and function, and support diversified business needs with minimized real-time computing resources, which is also what needs to be achieved in the future. \\n\\nThat\'s all for today, thank you.\\n\\n### about the author:\\n\\nZhu Liang, more than 5 years experience in data warehouse construction in traditional industries, 6 years experience in Internet data warehouse, technical direction involves offline, real-time data warehouse management, systematic capacity building, OLAP system and engine, big data related technologies, focusing on OLAP,and real-time technology frontier development trends.The business direction involves ad hoc query, operation analysis, strategy report product, user portrait, crowd recommendation, experimental evaluation, etc."},{"id":"/xiaomi","metadata":{"permalink":"/blog/xiaomi","source":"@site/blog/xiaomi.md","title":"Best Practice of Apache Doris in Xiaomi Group","description":"\x3c!--","date":"2022-07-20T00:00:00.000Z","formattedDate":"July 20, 2022","tags":[{"label":"Best Practice","permalink":"/blog/tags/best-practice"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Best Practice of Apache Doris in Xiaomi Group","summary":"In order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, **Apache Doris has been widely used in Xiaomi Group,** **such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.** This article will share the best practice of Apache Doris in Xiaomi Group.","date":"2022-07-20","author":"Apache Doris","tags":["Best Practice"]},"prevItem":{"title":"Best Practice of Apache Doris in Meituan","permalink":"/blog/meituan"},"nextItem":{"title":"Apache Doris announced the official release of version 1.1","permalink":"/blog/1.1 Release"}},"content":"\x3c!-- \\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n\\n# Background\\n\\nIn order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, **Apache Doris has been widely used in Xiaomi Group,** **such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.** This article will share the best practice of Apache Doris in Xiaomi Group. \\n\\n# Business Practice\\n\\nThe typical business practices of Apache Doris in Xiaomi are as follows:\\n\\n## 01 User Access\\n\\nData Factory is a one-stop data development platform developed by Xiaomi for data developers and data analysts. This platform supports data sources such as Doris, Hive, Kudu, Iceberg, ES, Talso, TiDB, MySQL, etc. It also supports computing engines such as Flink, Spark,  Presto,etc.\\n\\nInside Xiaomi, users need to access the Doris service through the data factory. Users need to register in the data factory and complete the approval for building the database. The Doris operation and maintenance classmates will connect according to the descriptions of the business scenarios and data usage expectations submitted by users in the data factory. After completing the access approval, users can use the Doris service to perform operations such as visual table creation and data import in the data factory.\\n\\n## 02 Data import\\n\\nIn Xiaomi\'s business, the two most common ways to import data into Doris are Stream Load and Broker Load. User data will be divided into real-time data and offline data, and users\' real-time and offline data will generally be written to Talos first (Talos is a distributed, high-throughput message queue developed by Xiaomi). The offline data from Talos will be sink to HDFS, and then imported to Doris through the data factory. Users can directly submit Broker Load tasks in the data factory to import large batches of data on HDFS into Doris, In addition, you can run the SparkSQL command in the data factory to query data from Hive, Import the data found in SparkSQL into Doris through Spark-doris-Connector, and encapsulate Stream Load at the bottom layer of Spark-doris-Connector. Real-time data from Talos is generally imported into Doris in two ways. One is to first perform ETL on the data through Flink, and then import small batches of data to Doris through.Flink- Doris-connector encapsulates the Stream Load at the bottom layer. Another way is to import small batches of data into Doris through Stream Load encapsulated by Spark Streaming at regular intervals.\\n\\n## 03 Data Query\\n\\nDoris users of Xiaomi generally analyze and query Doris and display the results through the ShuJing platform.ShuJing is a general-purpose BI analysis tool developed by Xiaomi. Users can query and visualize Doris through ShuJing  platform, and realize user behavior analysis (in order to meet the needs of business event analysis, retention analysis, funnel analysis, path analysis and other behavior analysis needs, We added corresponding UDF and UDAF ) and user profile analysis for Doris.\\n\\n## 04 Compaction Tuning\\n\\nFor Doris, each data import will generate a data version under the relevant data shard (Tablet) of the storage layer, and the Compaction mechanism will asynchronously merge the smaller data versions generated by the import (the detailed principle of the Compaction mechanism can be Refer to the previous article \\"Doris Compaction Mechanism Analysis\\").\\n\\nXiaomi has many high-frequency, high-concurrency, near-real-time import business scenarios, and a large number of small versions will be generated in a short period of time. If Compaction does not merge data versions in time, it will cause version accumulation.On the one hand, too many minor versions will increase the pressure on metadata, and on the other hand, too many versions will affect query performance.In Xiaomi\'s usage scenarios, many tables use the Unique and Aggregate data models, and the query performance is heavily dependent on whether Compaction can merge data versions in time.In our business scenario, the query performance was reduced by tens of times due to delayed version merging, thus affecting online services.When a Compaction happens, it consumes CPU, memory, and disk I/O resources. Too much compaction will take up too many machine resources, affect query performance, and may cause OOM.\\n\\n**In response to this problem of Compaction, we first start from the business side and guide users through the following aspects:**\\n\\n- Set reasonable partitions and buckets for tables to avoid generating too many data fragments.\\n\\n- Standardize the user\'s data import operation, reduce the frequency of data import, increase the amount of data imported in a single time, and reduce the pressure of Compaction.\\n\\n- Avoid using delete operations too much.The delete operation will generate a delete version under the relevant data shard in the storage layer.The Cumulative Compaction task will be truncated when the delete version is encountered. This task can only merge the data version after the Cumulative Point and before the delete version, move the Cumulative Point to the delete version, and hand over the delete version to the subsequent Base Compaction task. to process. If you use the delete operation too much, too many delete versions will be generated under the Tablet, which will cause the Cumulative Compaction task to slow down the progress of version merging. Using the delete operation does not actually delete the data from the disk, but records the deletion conditions in the delete version. When the data is queried, the deleted data will be filtered out by Merge-On-Read. Only the delete version is merged by the Base Compaction task. After that, the data to be deleted by the delete operation can be cleared from the disk as expired data with the Stale Rowset. If you need to delete the data of an entire partition, you can use the truncated partition operation instead of the delete operation.\\n\\n**Second, we tuned Compaction from the operation and maintenance side:**\\n\\n- According to different business scenarios, different Compaction parameters (Compaction strategy, number of threads, etc.) are configured for different clusters.\\n\\n- Appropriately lowers the priority of the Base Compaction task and increases the priority of the Cumulative Compaction task, because the Base Compaction task takes a long time to execute and has serious write amplification problems, while the Cumulative Compaction task executes faster and can quickly merge a large number of small versions.\\n\\n- Version backlog alarm, dynamic adjustment of Compaction parameters.When the Compaction Producer produces Compaction tasks, it will update the corresponding metric.It records the value of the largest Compaction Score on the BE node. You can check the trend of this indicator through Grafana to determine whether there is a version backlog. In addition, we have added a Version backlog alert.In order to facilitate the adjustment of Compaction parameters, we have optimized the code level to support dynamic adjustment of the Compaction strategy and the number of Compaction threads at runtime, avoiding the need to restart the process when adjusting the Compaction parameters.\\n\\n- Supports manual triggering of the Compaction task of the specified Table and data shards under the specified Partition, and improves the Compaction priority of the specified Table and data shards under the specified Partition.\\n\\n# Monitoring and Alarm Management\\n\\n## 01 Monitoring System\\n\\nPrometheus will regularly pull Metrics metrics from Doris\'s FE and BE and display them in the Grafana monitoring panel.The service metadata based on QingZhou Warehouse will be automatically registered in Zookeeper, and Prometheus will regularly pull the latest cluster metadata information from Zookeeper and display it dynamically in the Grafana monitoring panel.\uff08Qingzhou Data Warehouse is a data warehouse constructed by the Qingzhou platform based on the operation data of Xiaomi\'s full-scale big data service. It consists of 2 base tables and 30+ dimension tables.Covers the whole process data such as resources, server cmdb, cost, process status and so on when big data components are running\uff09We have also added statistics and display boards for common troubleshooting data such as Doris large query list, real-time write data volume, data import transaction numbers, etc. in Grafana.In Grafana, we also added statistics and display boards for common troubleshooting data such as the Doris big query list, the amount of real-time data written, and the number of data import transactions, so that alarms can be linked. When the cluster is abnormal, Doris\' operation and maintenance students can locate the cause of the cluster failure in the shortest time.\\n\\n## 02  Falcon \\n\\nFalcon is a monitoring and alarm system widely used inside Xiaomi.Because Doris provides a relatively complete metrics interface, which can easily provide monitoring functions based on Prometheus and Grafana, we only use Falcon\'s alarm function in the Doris service.For different levels of faults in Doris, we define alarms as three levels of P0, P1 and P2:\\n\\n- P2 alarm (alarm level is low): single node failure alarm.When a single node indicator or process status is abnormal, an alarm is generally issued as a P2 level.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.(Xiaomi Office is a privatized deployment product of ByteDance Feishu in Xiaomi, and its functions are similar to Feishu.)\\n\\n- P1 alarm (alarm level is higher):In a short period of time (within 3 minutes), the cluster will issue a P1 level alarm if there are short-term exceptions such as increased query delay and abnormal writing,etc.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.P1 level alarms require Oncall engineers to respond and provide feedback.\\n\\n- P0 alarm (alarm level is high):In a long period of time (more than 3 minutes), the cluster will issue a P0 level alarm if there are exceptions such as increased query delay and abnormal writing,etc.Alarm information is sent in the form of Xiaomi office messages and phone alarms.P0 level alarm requires Oncall engineers to respond within 1 minute and coordinate resources for failure recovery and review preparation.\\n\\n## 03  Cloud-Doris \\n\\ncloud-Doris is a data collection component developed by Xiaomi for the internal Doris service. Its main capability is to detect the availability of the Doris service and collect the cluster indicator data of internal concern.For example, Cloud-Doris can periodically simulate users reading and writing to the Doris system to detect the availability of services.If the cluster has abnormal availability, it will be alerted through Falcon.Collect user\'s read and write data, and then generate user bill.Collect information such as table-level data volume, unhealthy copies, and oversized Tablets, and send alarms to abnormal information through Falcon.\\n\\n## 04 QingZhou inspection\\n\\nFor chronic hidden dangers such as capacity, user growth, resource allocation, etc., we use the unified QingZhou big data service inspection platform for inspection and reporting.The inspection generally consists of two parts:Service-specific inspections and basic indicator inspections.Among them, the service-specific inspection refers to the indicators that are unique to each big data service and cannot be used universally.For Doris, it mainly includes: Quota, number of shard copies, number of single table columns, number of table partitions, etc.By increasing the inspection method, the chronic hidden dangers that are difficult to be alarmed in advance can be well avoided, which provides support for the failure-free major festivals.\\n\\n# Failure Recovery\\n\\nWhen an online cluster fails, the first principle should be to quickly restore services.If the cause of the failure is clear, handle it according to the specific cause and restore the service.If the cause of the failure is not clear, you should try restarting the process as soon as you keep the snapshot to restore the service.\\n\\n## 01 Access Failures Handling\\n\\nDoris uses Xiaomi LVS as the access layer, which is similar to the LB service of open source or public cloud, and provides layer 4 or layer 7 traffic load scheduling capability.After Doris binds a reasonable port,Generally speaking, if an abnormality occurs in a single FE node, it will be automatically kicked out, and the service can be restored without the user\'s perception, and an alarm will be issued for the abnormal node.Of course, for FE faults that cannot be processed in a short time, we will first adjust the weight of the faulty node to 0 or delete the abnormal node from LVS first to prevent unpredictable problems caused by process detection exceptions.\\n\\n## 02 Node Failure Handling\\n\\nFor FE node failures, if the cause of the failure cannot be quickly located, it is generally necessary to keep thread snapshots and memory snapshots and restart the process.\\n\\n```undefined\\njstack \u8fdb\u7a0bID >> \u5feb\u7167\u6587\u4ef6\u540d.jstack\\n```\\n\\nSave a memory snapshot of FE with the command:\\n\\n```undefined\\njmap -dump:live,format=b,file=\u5feb\u7167\u6587\u4ef6\u540d.heap \u8fdb\u7a0bID\\n```\\n\\nIn the case of version upgrade or some unexpected scenarios, the image of the FE node may have abnormal metadata, and the abnormal metadata may be synchronized to other FE, resulting in all FE not working.Once a failed image is discovered, the fastest recovery option is to use Recovery mode to stop FE elections and replace the failed image with the backup image.Of course, it is not easy to backup images all the time.Since this failure is common in cluster upgrades, we recommend adding simple local image backup logic to the cluster upgrade procedure.Ensure that a copy of the current and latest image data will be retained before each upgrade starts the FE process.For BE node failure, if the process crashes, a core file will be generated, and minos will automatically pull the process;If the task is stuck, you need to restart the process after retaining the thread snapshot with the following command:\\n\\n```undefined\\npstack \u8fdb\u7a0bID >> \u5feb\u7167\u6587\u4ef6\u540d.pstack\\n```\\n\\n# Concluding Remarks\\n\\nApache Doris has been widely used by Xiaomi since the first use of open source software Apache Doris by Xiaomi Group in September 2019.At present, it has served dozens of businesses of Xiaomi, with dozens of clusters and hundreds of nodes, and a set of data ecology with Apache Doris as the core has been formed within Xiaomi.In order to improve the efficiency of operation and maintenance, Xiaomi has also developed a complete set of automated management and operation and maintenance systems around Doris.With the increasing number of services, Doris also exposed some problems. For example, there was no better resource isolation mechanism in the past version, and services would affect each other. In addition, system monitoring needs to be further improved.With the rapid development of the community, more and more small partners have participated in the community construction, the vectorized engine has been transformed, the transformation of the query optimizer is in full swing, and Apache Doris is gradually maturing."},{"id":"/1.1 Release","metadata":{"permalink":"/blog/1.1 Release","source":"@site/blog/1.1 Release.md","title":"Apache Doris announced the official release of version 1.1","description":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.","date":"2022-07-14T00:00:00.000Z","formattedDate":"July 14, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"Apache Doris announced the official release of version 1.1","summary":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.","date":"2022-07-14","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Best Practice of Apache Doris in Xiaomi Group","permalink":"/blog/xiaomi"},"nextItem":{"title":"Announcing Open Source Realtime Analytical database Apache Doris as a Top-Level Project","permalink":"/blog/Annoucing"}},"content":"Dear community, we are pleased to announce that we have officially released Apache Doris 1.1 on July 14, 2022! This is the first release version after Apache Doris graduated from the Apache incubator and became an Apache Top-Level Project.\\n\\nIn version 1.1, we realized the full vectorization of the computing layer and storage layer, and officially enabled the vectorized execution engine as a stable function. All queries are executed by the vectorized execution engine by default, and the performance is 3-5 times higher than the previous version. It increases the ability to access the external tables of Apache Iceberg and supports federated query of data in Doris and Iceberg, and expands the analysis capabilities of Apache Doris on the data lake; on the basis of the original LZ4, the ZSTD compression algorithm is added , further improves the data compression rate; fixed many performance and stability problems in previous versions, greatly improving system stability. Downloading and using is recommended.\\n\\n## Upgrade Notes\\n\\n### The vectorized execution engine is enabled by default\\n\\nIn version 1.0, we introduced the vectorized execution engine as an experimental feature and Users need to manually enable it when executing queries by configuring the session variables through `set batch_size = 4096` and `set enable_vectorized_engine = true` .\\n\\nIn version 1.1, we officially fully enabled the vectorized execution engine as a stable function. The session variable `enable_vectorized_engine` is set to true by default. All queries are executed by default through the vectorized execution engine.\\n\\n### BE Binary File Renaming\\n\\nBE binary file has been renamed from palo_be to doris_be . Please pay attention to modifying the relevant scripts if you used to rely on process names for cluster management and other operations.\\n\\n### Segment storage format upgrade\\n\\nThe storage format of earlier versions of Apache Doris was Segment V1. In version 0.12, we had implemented Segment V2 as a new storage format, which introduced Bitmap indexes, memory tables, page cache, dictionary compression, delayed materialization and many other features. Starting from version 0.13, the default storage format for newly created tables is Segment V2, while maintaining compatibility with the Segment V1 format.\\n\\nIn order to ensure the maintainability of the code structure and reduce the additional learning and development costs caused by redundant historical codes, we have decided to no longer support the Segment v1 storage format from the next version. It is expected that this part of the code will be deleted in the Apache Doris 1.2 version, and all users who are still using the Segment V1 storage format must complete the data format conversion in version 1.1. Please refer to the following link for the operation manual:\\n\\n[https://doris.apache.org/zh-CN/docs/1.0/administrator-guide/segment-v2-usage](https://doris.apache.org/zh-CN/docs/1.0/administrator-guide/segment-v2-usage)\\n\\n### Normal Upgrade\\n\\nFor normal upgrade operations, you can perform rolling upgrades according to the cluster upgrade documentation on the official website.\\n\\n[https://doris.apache.org/zh-CN/docs/admin-manual/cluster-management/upgrade](https://doris.apache.org/zh-CN/docs/admin-manual/cluster-management/upgrade)\\n\\n## Features\\n\\n### Support random distribution of data [experimental]\\n\\nIn some scenarios (such as log data analysis), users may not be able to find a suitable bucket key to avoid data skew, so the system needs to provide additional distribution methods to solve the problem.\\n\\nTherefore, when creating a table you can set `DISTRIBUTED BY random BUCKET number`to use random distribution, the data will be randomly written to a single tablet when importing to reduce the data fanout during the loading process. And reduce resource overhead and improve system stability.\\n\\n### Support for creating Iceberg external tables[experimental]\\n\\nIceberg external tables provide Apache Doris with direct access to data stored in Iceberg. Through Iceberg external tables, federated queries on data stored in local storage and Iceberg can be implemented, which saves tedious data loading work, simplifies the system architecture for data analysis, and performs more complex analysis operations.\\n\\nIn version 1.1, Apache Doris supports creating Iceberg external tables and querying data, and supports automatic synchronization of all table schemas in the Iceberg database through the REFRESH command.\\n\\n### Added ZSTD compression algorithm\\n\\nAt present, the data compression method in Apache Doris is uniformly specified by the system, and the default is LZ4. For some scenarios that are sensitive to data storage costs, the original data compression ratio requirements cannot be met.\\n\\nIn version 1.1, users can set \\"compression\\"=\\"zstd\\" in the table properties to specify the compression method as ZSTD when creating a table. In the 25GB 110 million lines of text log test data, the highest compression rate is nearly 10 times, which is 53% higher than the original compression rate, and the speed of reading data from disk and decompressing it is increased by 30%.\\n\\n## Improvements\\n\\n### More comprehensive vectorization support\\n\\nIn version 1.1, we implemented full vectorization of the compute and storage layers, including:\\n\\nImplemented vectorization of all built-in functions\\n\\nThe storage layer implements vectorization and supports dictionary optimization for low-cardinality string columns\\n\\nOptimized and resolved numerous performance and stability issues with the vectorization engine.\\n\\nWe tested the performance of Apache Doris version 1.1 and version 0.15 on the SSB and TPC-H standard test datasets:\\n\\nOn all 13 SQLs in the SSB test data set, version 1.1 is better than version 0.15, and the overall performance is improved by about 3 times, which solves the problem of performance degradation in some scenarios in version 1.0;\\n\\nOn all 22 SQLs in the TPC-H test data set, version 1.1 is better than version 0.15, the overall performance is improved by about 4.5 times, and the performance of some scenarios is improved by more than ten times;\\n\\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/edb59781b0f74ff08821467f23a63bad~tplv-k3u1fbpfcp-zoom-1.image)\\n\\n<p align=\'center\'>SSB \u6d4b\u8bd5\u6570\u636e\u96c6</p>\\n\\n![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e34377054f4448b3b367789a391f2122~tplv-k3u1fbpfcp-zoom-1.image)\\n\\n<p align=\'center\'>TPC-H \u6d4b\u8bd5\u6570\u636e\u96c6</p>\\n\\n**Performance test report**\\n\\n[https://doris.apache.org/zh-CN/docs/benchmark/ssb](https://doris.apache.org/zh-CN/docs/benchmark/ssb)\\n\\n[https://doris.apache.org/zh-CN/docs/benchmark/tpch](https://doris.apache.org/zh-CN/docs/benchmark/tpch)\\n\\n### Compaction logic optimization and real-time guarantee\\n\\nIn Apache Doris, each commit will generate a data version. In high concurrent write scenarios, -235 errors are prone to occur due to too many data versions and untimely compaction, and query performance will also decrease accordingly.\\n\\nIn version 1.1, we introduced QuickCompaction, which will actively trigger compaction when the data version increases. At the same time, by improving the ability to scan fragment metadata, it can quickly find fragments with too many data versions and trigger compaction. Through active triggering and passive scanning, the real-time problem of data merging is completely solved.\\n\\nAt the same time, for high-frequency small file cumulative compaction, the scheduling and isolation of compaction tasks is implemented to prevent the heavyweight base compaction from affecting the merging of new data.\\n\\nFinally, for the merging of small files, the strategy of merging small files is optimized, and the method of gradient merging is adopted. Each time the files participating in the merging belong to the same data magnitude, it prevents versions with large differences in size from merging, and gradually merges hierarchically. , reducing the number of times a single file participates in merging, which can greatly save the CPU consumption of the system.\\n\\nWhen the data upstream maintains a write frequency of 10w per second (20 concurrent write tasks, 5000 rows per job, and checkpoint interval of 1s), version 1.1 behaves as follows:\\n\\n-   Quick data consolidation: Tablet version remains below 50 and compaction score is stable. Compared with the -235 problem that frequently occurred during high concurrent writing in the previous version, the compaction merge efficiency has been improved by more than 10 times.\\n\\n-   Significantly reduced CPU resource consumption: The strategy has been optimized for small file Compaction. In the above scenario of high concurrent writing, CPU resource consumption is reduced by 25%;\\n\\n-   Stable query time consumption: The overall orderliness of data is improved, and the fluctuation of query time consumption is greatly reduced. The query time consumption during high concurrent writing is the same as that of only querying, and the query performance is improved by 3-4 times compared with the previous version.\\n\\n### Read efficiency optimization for Parquet and ORC files\\n\\nBy adjusting arrow parameters, arrow\'s multi-threaded read capability is used to speed up Arrow\'s reading of each row_group, and it is modified to SPSC model to reduce the cost of waiting for the network through prefetching. After optimization, the performance of Parquet file import is improved by 4 to 5 times.\\n\\n### Safer metadata Checkpoint\\n\\nBy double-checking the image files generated after the metadata checkpoint and retaining the function of historical image files, the problem of metadata corruption caused by image file errors is solved.\\n\\n## Bugfix\\n\\n### Fix the problem that the data cannot be queried due to the missing data version.(Serious)\\n\\nThis issue was introduced in version 1.0 and may result in the loss of data versions for multiple replicas.\\n\\n### Fix the problem that the resource isolation is invalid for the resource usage limit of loading tasks (Moderate)\\n\\nIn 1.1, the broker load and routine load will use Backends with specified resource tags to do the load.\\n\\n### Use HTTP BRPC to transfer network data packets over 2GB (Moderate)\\n\\nIn the previous version, when the data transmitted between Backends through BRPC exceeded 2GB,\\nit may cause data transmission errors.\\n\\n## Others\\n\\n### Disabling Mini Load\\n\\nThe `/_load` interface is disabled by default, please use `the /_stream_load` interface uniformly.\\nOf course, you can re-enable it by turning off the FE configuration item `disable_mini_load`.\\n\\nThe Mini Load interface will be completely removed in version 1.2.\\n\\n### Completely disable the SegmentV1 storage format\\n\\nData in SegmentV1 format is no longer allowed to be created. Existing data can continue to be accessed normally.\\nYou can use the `ADMIN SHOW TABLET STORAGE FORMAT` statement to check whether the data in SegmentV1 format\\nstill exists in the cluster. And convert to SegmentV2 through the data conversion command\\n\\nAccess to SegmentV1 data will no longer be supported in version 1.2.\\n\\n### Limit the maximum length of String type\\n\\nIn previous versions, String types were allowed a maximum length of 2GB.\\nIn version 1.1, we will limit the maximum length of the string type to 1MB. Strings longer than this length cannot be written anymore.\\nAt the same time, using the String type as a partitioning or bucketing column of a table is no longer supported.\\n\\nThe String type that has been written can be accessed normally.\\n\\n### Fix fastjson related vulnerabilities\\n\\nUpdate to Canal version to fix fastjson security vulnerability.\\n\\n### Added `ADMIN DIAGNOSE TABLET` command\\n\\nUsed to quickly diagnose problems with the specified tablet.\\n\\n## Download to Use\\n\\n### Download Link\\n\\n[hhttps://doris.apache.org/download](https://doris.apache.org/download)\\n\\n### Feedback\\n\\nIf you encounter any problems with use, please feel free to contact us through GitHub discussion forum or Dev e-mail group anytime.\\n\\nGitHub Forum: [https://github.com/apache/doris/discussions](https://github.com/apache/doris/discussions)\\n\\nMailing list: [dev@doris.apache.org](dev@doris.apache.org)\\n\\n## Thanks\\n\\nThanks to everyone who has contributed to this release:\\n\\n```\\n\\n@adonis0147\\n\\n@airborne12\\n\\n@amosbird\\n\\n@aopangzi\\n\\n@arthuryangcs\\n\\n@awakeljw\\n\\n@BePPPower\\n\\n@BiteTheDDDDt\\n\\n@bridgeDream\\n\\n@caiconghui\\n\\n@cambyzju\\n\\n@ccoffline\\n\\n@chenlinzhong\\n\\n@daikon12\\n\\n@DarvenDuan\\n\\n@dataalive\\n\\n@dataroaring\\n\\n@deardeng\\n\\n@Doris-Extras\\n\\n@emerkfu\\n\\n@EmmyMiao87\\n\\n@englefly\\n\\n@Gabriel39\\n\\n@GoGoWen\\n\\n@gtchaos\\n\\n@HappenLee\\n\\n@hello-stephen\\n\\n@Henry2SS\\n\\n@hewei-nju\\n\\n@hf200012\\n\\n@jacktengg\\n\\n@jackwener\\n\\n@Jibing-Li\\n\\n@JNSimba\\n\\n@kangshisen\\n\\n@Kikyou1997\\n\\n@kylinmac\\n\\n@Lchangliang\\n\\n@leo65535\\n\\n@liaoxin01\\n\\n@liutang123\\n\\n@lovingfeel\\n\\n@luozenglin\\n\\n@luwei16\\n\\n@luzhijing\\n\\n@mklzl\\n\\n@morningman\\n\\n@morrySnow\\n\\n@nextdreamblue\\n\\n@Nivane\\n\\n@pengxiangyu\\n\\n@qidaye\\n\\n@qzsee\\n\\n@SaintBacchus\\n\\n@SleepyBear96\\n\\n@smallhibiscus\\n\\n@spaces-X\\n\\n@stalary\\n\\n@starocean999\\n\\n@steadyBoy\\n\\n@SWJTU-ZhangLei\\n\\n@Tanya-W\\n\\n@tarepanda1024\\n\\n@tianhui5\\n\\n@Userwhite\\n\\n@wangbo\\n\\n@wangyf0555\\n\\n@weizuo93\\n\\n@whutpencil\\n\\n@wsjz\\n\\n@wunan1210\\n\\n@xiaokang\\n\\n@xinyiZzz\\n\\n@xlwh\\n\\n@xy720\\n\\n@yangzhg\\n\\n@Yankee24\\n\\n@yiguolei\\n\\n@yinzhijian\\n\\n@yixiutt\\n\\n@zbtzbtzbt\\n\\n@zenoyang\\n\\n@zhangstar333\\n\\n@zhangyifan27\\n\\n@zhannngchen\\n\\n@zhengshengjun\\n\\n@zhengshiJ\\n\\n@zingdle\\n\\n@zuochunwei\\n\\n@zy-kkk\\n```"},{"id":"/Annoucing","metadata":{"permalink":"/blog/Annoucing","source":"@site/blog/Annoucing.md","title":"Announcing Open Source Realtime Analytical database Apache Doris as a Top-Level Project","description":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.","date":"2022-06-16T00:00:00.000Z","formattedDate":"June 16, 2022","tags":[{"label":"Top News","permalink":"/blog/tags/top-news"}],"truncated":false,"authors":[{"name":"morningman"}],"frontMatter":{"title":"Announcing Open Source Realtime Analytical database Apache Doris as a Top-Level Project","summary":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.","date":"2022-06-16","author":"morningman","tags":["Top News"]},"prevItem":{"title":"Apache Doris announced the official release of version 1.1","permalink":"/blog/1.1 Release"},"nextItem":{"title":"[Release Note] Apache Doris(Incubating) 1.0.0 Release","permalink":"/blog/release-note-1.0.0"}},"content":"Apache Doris is a modern, high-performance and real-time analytical database based on MPP. It is well known for its high-performance and easy-to-use. It can return query results under massive data within only sub-seconds. It can support not only high concurrent point query scenarios, but also complex analysis scenarios with high throughput. Based on this, Apache Doris can be well applied in many business fields, such as multi-dimensional reporting, user portrait, ad-hoc query, real-time dashboard and so on.\\n\\nApache Doris was first born in the Palo Project within Baidu\'s advertising report business and officially opened source in 2017. It was donated by Baidu to Apache foundation for incubation in July 2018, and then incubated and operated by members of the podling project management committee\uff08PPMC\uff09under the guidance of Apache incubator mentors.\\n\\nWe are very proud that Doris graduated from Apache incubator successfully. It is an important milestone. In the whole incubating period, with the guidance of Apache Way and the help of incubator mentors, we learned how to develop our project and community in Apache Way, and have achieved great growth in this process.\\n\\nAt present, Apache Doris community has gathered more than 300 contributors from nearly 100 enterprises in different industries, and the number of active contributors per month is close to 100. During the incubation period, Apache Doris released a total of 8 major versions and completed many major functions, including storage engine upgrade, vectorization execution engine and so on, and released 1.0 version. It is the strength of these open source contributors that makes Apache Doris achieve today\'s results.\\n\\nAt the same time, Apache Doris now has a wide range of users in China and even around the world. Up to now, Apache Doris has been applied in the production environment of more than 500 enterprises around the world. Among the top 50 Internet companies in China by market value or valuation, more than 80% are long-term users of Apache Doris, including Baidu, Meituan, Xiaomi, JD, ByteDance, Tencent, Kwai, Netease, Sina, 360 and other well-known companies. It also has rich applications in some traditional industries, such as finance, energy, manufacturing, telecommunications and other fields.\\n\\nYou can quickly build a simple, easy-to-use and powerful data analysis platform based on Apache Doris, which is very easy to start, and the learning cost is very low. In addition, the distributed architecture of Apache Doris is very simple, which can greatly reduce the workload of system operation and maintenance. This is also the key factor for more and more users to choose Apache Doris.\\n\\nAs a mature analytical database project, Apache Doris has the following advantages:\\n\\n-   Excellent performance: it is equipped with an efficient column storage engine, which not only reduces the amount of data scanning, but also implements an ultra-high data compression ratio. At the same time, Doris also provides a rich index structure to speed up data reading and filtering. Using the partition and bucket pruning function, Doris can support ultra-high concurrency of online service business, and a single node can support up to thousands of QPS. Further, Apache Doris combines the vectorization execution engine to give full play to the modern CPU parallel computing power, supplemented by intelligent materialized view technology to accelerate pre-aggregation, and can simultaneously carry out planning based and cost based query optimization through the query optimizer. Through the above methods, Doris can reach ultimate query performance.\\n\\n-   Easy to use: it supports ANSI SQL syntax, including single table aggregation, sorting, filtering and multi table join, sub query, etc. it also supports complex SQL syntax such as window function and grouping set. At the same time, users can expand system functions through UDF, UDAF and other user-defined functions. In addition, Apache Doris is also compatible with MySQL protocol. Users can access Doris through various client tools and support seamless connection with BI tools.\\n\\n-   Streamlined architecture: the system has only two modules \u2014\u2014 frontend (FE) and backend (BE). The FE node is responsible for the access of user requests, the analysis of query plans, metadata storage and cluster management, and the BE node is responsible for the implementation of data storage and query plans. It is a complete distributed database management system. Users can run the Apache Doris cluster without installing any third-party management and control components, and the deployment and upgrade process are very simple. At the same time, any module can support horizontal expansion, and the cluster can be expanded up to hundreds of nodes, supporting the storage of more than 10PB of ultra large scale data.\\n\\n-   Scalability and reliability: it supports the storage of multiple replicas of data. The cluster is able to self-healing. Its own distributed management framework can automatically manage the distribution, repair and balance of data replicas. When the replicas are damaged, the system can automatically perceive and repair them. When a node is expanded, it can be completed with only one SQL command, and the data replicas will be automatically rebalanced among nodes without manual intervention or operation. Whether it is capacity expansion, capacity reduction, single node failure or upgrading, the system does not need to stop running, and can normally provide stable and reliable online services.\\n\\n-   Ecological enrichment: It provides rich data synchronisation methods, supports fast loading of data from localhost, Hadoop, Flink, Spark, Kafka, SeaTunnel and other systems, and can also directly access data in MySQL, PostgreSQL, Oracle, S3, Hive, Iceberg, Elasticsearch and other systems without data replication. At the same time, the data stored in Doris can also be read by Spark and Flink, and can be output to the upstream data application for display and analysis.\\n\\nGraduation is not the ultimate goal, it is the starting point of a new journey. In the past, our goal of launching Doris was to provide more people with better data analysis tools and solve their data analysis problems. Becoming an Apache top-level project is not only an affirmation of the hard work of all contributors to the Apache Doris community in the past, but also means that we have established a strong, prosperous and sustainable open source community under the guidance of Apache Way.In the future, we will continue to operate the community in the Way of Apache. I believe we will attract more excellent open source contributors to participate in the community, and the community will further grow with the help of all contributors.\\n\\nApache Doris will carry out more challenging and meaningful work in the future, including new query optimizer, support for Lakehouse integration, and architecture evolution for cloud infrastructure. More open source technology enthusiasts are welcome to join the Apache Doris community and grow together.\\n\\nOnce again, we sincerely thank all contributors who participated in the construction of Apache Doris community and all users who use Apache Doris and constantly put forward improvement suggestions. At the same time, we also thank our incubator mentors, IPMC members and friends in various open source project communities who have continuously encouraged, supported and helped us all the way.\\n\\n**Apache Doris GitHub\uff1a**\\n\\n[https://github.com/apache/doris](https://github.com/apache/doris)\\n\\n**Apache Doris website:**\\n\\n[http://doris.apache.org](http://doris.apache.org)\\n\\n**Please contact us via:**\\n\\n[dev@doris.apache.org.](dev@doris.apache.org.)\\n\\n**See How to subscribe:**\\n\\n[https://doris.apache.org/community/subscribe-mail-list](https://doris.apache.org/community/subscribe-mail-list/)"},{"id":"/release-note-1.0.0","metadata":{"permalink":"/blog/release-note-1.0.0","source":"@site/blog/release-note-1.0.0.md","title":"[Release Note] Apache Doris(Incubating) 1.0.0 Release","description":"\x3c!--","date":"2022-04-18T00:00:00.000Z","formattedDate":"April 18, 2022","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"[Release Note] Apache Doris(Incubating) 1.0.0 Release","summary":"[Release Note] Apache Doris(Incubating) 1.0.0 Release","date":"2022-04-18","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"Announcing Open Source Realtime Analytical database Apache Doris as a Top-Level Project","permalink":"/blog/Annoucing"},"nextItem":{"title":"[Release Note] Apache Doris(Incubating) 0.15.0 Release","permalink":"/blog/release-note-0.15.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris(Incubating) 1.0.0 Release\\n\\nDear community friends, after several months, we are happy to announce that Apache Doris (incubating) has officially released the 1.0 Release version on April 18, 2022! **This is the first 1-bit version of Apache Doris since it was incubated by the Apache Foundation, and it is also the version with the largest refactoring of the core code of Apache Doris so far****! **With **114 Contributors** committing **over 660 optimizations and fixes** for Apache Doris, thank you to everyone who makes Apache Doris even better!\\n\\nIn version 1.0, we introduced important functions such as vectorized execution engine, Hive external table, Lateral View syntax and Table Function table function, Z-Order data index, Apache SeaTunnel plug-in, etc., and added support for synchronous update and deletion of data in Flink CDC. Support, optimize many problems in the process of data import and query, and comprehensively enhance the query performance, ease of use, stability and other special effects of Apache Doris. Welcome to download and use! Click \\"**Read the original text**\\" at the end of the article to go directly to the download address.\\n\\nEvery day that has not been published, there are countless contributors behind it, who dare not stop for half a minute. Here we would like to especially thank the small partners from SIG (Special Interest Group) such as **vectorized execution engine, query optimizer, and visual operation and maintenance platform**. Since the establishment of the Apache Doris Community SIG group in August 2021, data from more than ten companies including Baidu, Meituan, Xiaomi, JD, Shuhai, ByteDance, Tencent, NetEase, Alibaba, PingCAP, Nebula Graph, etc. Ten contributors** joined the SIG as the first members, and for the first time completed the development of such major functions as the vectorized execution engine, query optimizer, and Doris Manager visual monitoring operation and maintenance platform in the form of open source collaboration of special groups. **During more than half a year, conducting technical research and sharing dozens of times, holding nearly 100 remote meetings, accumulatively submitting hundreds of Commits, involving more than 100,000 lines of code**, it is precisely because of their contributions , only the 1.0 version came out, let us once again express our most sincere thanks for their hard work!\\n\\nAt the same time, the number of Apache Doris contributors has exceeded 300, the number of monthly active contributors has exceeded 60, and the average weekly number of Commits submitted in recent weeks has also exceeded 80. The scale and activity of developers gathered by the community There has been a huge improvement. We are very much looking forward to having more small partners participate in the community contribution, and work with us to build Apache Doris into the world\'s top analytical database. We also hope that all small partners can reap valuable growth with us. If you would like to participate in the community, please contact us via the developer email dev@doris.apache.org.\\n\\nWe welcome you to contact us with any questions during the use process through GitHub Discussion or Dev mail group, and we look forward to your participation in community discussions and construction.\\n\\n\\n## Important update \\n\\n### Vectorized Execution Engine [Experimental]\\n\\nIn the past, the SQL execution engine of Apache Doris was designed based on the row-based memory format and the traditional volcano model. There was unnecessary overhead in performing SQL operator and function operations, which led to the limited efficiency of the Apache Doris execution engine, which did not Adapt to the architecture of modern CPUs. The goal of the vectorized execution engine is to replace the current row-based SQL execution engine of Apache Doris, fully release the computing power of modern CPUs, break through the performance limitations on the SQL execution engine, and exert extreme performance.\\n\\nBased on the characteristics of modern CPUs and the execution characteristics of the volcano model, the vectorized execution engine redesigned the SQL execution engine in the columnar storage system:\\n\\n- Reorganized the data structure of memory, replaced Tuple with Column, improved Cache affinity, branch prediction and prefetch memory friendliness during calculation\\n- Type judgment is performed in batches. In this batch, the type determined during type judgment is used, and the virtual function cost of type judgment of each line is allocated to the batch level.\\n- Through batch-level type judgment, virtual function calls are eliminated, allowing the compiler to have the opportunity for function inlining and SIMD optimization\\n\\nThis greatly improves the efficiency of the CPU when executing SQL and improves the performance of SQL queries.\\n\\nIn Apache Doris version 1.0, enabling the vectorized execution engine with set batch_size = 4096 and set enable_vectorized_engine = true can significantly improve query performance in most cases. Under the SSB and OnTime standard test datasets, the overall performance of the two scenarios of multi-table association and wide-column query is improved by 3 times and 2.6 times respectively.\\n\\n![](/images/blogs/1.0/1.0.0-1.png)\\n\\n![](/images/blogs/1.0/1.0.0-2.png)\\n\\n### Lateral View Grammar [Experimental]\\n\\nThrough Lateral View syntax, we can use Table Function table functions such as explode_bitmap, explode_split, explode_jaon_array, etc., to expand bitmap, String or Json Array from one column into multiple rows, so that the expanded data can be further processed (such as Filter, Join, etc.) .\\n\\n### Hive External Table [Experimental]\\n\\nHive External Table provides users with the ability to directly access Hive tables through Doris. External tables save the tedious data import work, and can use Doris\'s own OLAP capabilities to solve data analysis problems of Hive tables. The current version supports connecting Hive data sources to Doris, and supports federated queries through data in Doris and Hive data sources for more complex analysis operations.\\n\\n### Support Z-Order data sorting format\\n\\nApache Doris data is sorted and stored according to the prefix column, so when the prefix query condition is included, fast data search can be performed on the sorted data, but if the query condition is not a prefix column, the data sorting feature cannot be used for fast data search. The above problems can be solved by Z-Order Indexing. In version 1.0, we have added the Z-Order data sorting format, which can play a good filtering effect in the scenario of kanban multi-column query and accelerate the filtering performance of non-prefix column conditions. .\\n\\n### Support for Apache SeaTunnel (Incubating) plugin\\n\\nApache SeaTunnel is a high-performance distributed data integration framework built on Apache Spark and Apache Flink. In the 1.0 version of Apache Doris, we have added the SaeTunnel plugin, users can use Apache SeaTunnel for synchronization and ETL between multiple data sources.\\n\\n### New Function\\n\\nMore bitmap functions are supported, see the function manual for details:\\n\\n- bitmap_max\\n- bitmap_and_not\\n- bitmap_and_not_count\\n- bitmap_has_all\\n- bitmap_and_count\\n- bitmap_or_count\\n- bitmap_xor_count\\n- bitmap_subset_limit\\n- sub_bitmap\\n\\nSupport national secret algorithm SM3/SM4;\\n\\n\\n\\n> **Note**: The functions marked [Experimental] above are experimental functions. We will continue to optimize and iterate on the above functions in subsequent versions, and further improve them in subsequent versions. If you have any questions or comments during use, please feel free to contact us\\n\\n### Important Optimization\\n\\n### Features Optimization\\n\\n* Reduced the number of segment files generated when importing in large batches to reduce Compaction pressure.\\n* Transfer data through BRPC\'s attachment function to reduce serialization and deserialization overhead during query.\\n* Support to directly return binary data of HLL/BITMAP type for external analysis of business.\\n* Optimize and reduce the probability of OVERCROWDED and NOT_CONNECTED errors in BRPC, and enhance system stability.\\n* Enhance the fault tolerance of import.\\n* Support to update and delete data synchronously through Flink CDC.\\n* Support adaptive Runtime Filter.\\n* Significantly reduce the memory footprint of insert into operations\\n\\n\\n### Usability Improvements\\n\\n* Routine Load supports displaying the current offset delay number and other status.\\n* Added statistics on peak memory usage of queries in FE audit log.\\n* Added missing version information to Compaction URL results to facilitate troubleshooting.\\n* Support marking BE as non-queryable or non-importable to quickly screen problem nodes.\\n\\n### Important Bug Fixes\\n\\n* Fixed several query errors.\\n* Fixed some scheduling logic issues in Broker Load.\\n* Fix the problem that the metadata cannot be loaded due to the STREAM keyword.\\n* Fixed Decommission not executing correctly.\\n* Fix the problem that -102 error may occur when operating Schema Change operation in some cases.\\n* Fix the problem that using String type may cause BE to crash in some cases.\\n\\n### Other\\n\\n* Added Minidump function; easy to locate when problems occur\\n\\n## Changelog\\n\\nFor detailed Release Note, please check the link:\\n\\nhttps://github.com/apache/incubator-doris/issues/8549\\n\\n## Thanks  \\n\\nThe release of Apache Doris(incubating) 1.0 Release version is inseparable from the support of all community users. I would like to express my gratitude to all community contributors who participated in version design, development, testing and discussion. They are:\\n\\n```\\n@924060929\\n@adonis0147\\n@Aiden-Dong\\n@aihai\\n@airborne12\\n@Alibaba-HZY\\n@amosbird\\n@arthuryangcs\\n@awakeljw\\n@bingzxy\\n@BiteTheDDDDt\\n@blackstar-baba\\n@caiconghui\\n@CalvinKirs\\n@cambyzju\\n@caoliang-web\\n@ccoffline\\n@chaplinthink\\n@chovy-3012\\n@ChPi\\n@DarvenDuan\\n@dataalive\\n@dataroaring\\n@dh-cloud\\n@dohongdayi\\n@dongweizhao\\n@drgnchan\\n@e0c9\\n@EmmyMiao87\\n@englefly\\n@eyesmoons\\n@freemandealer\\n@Gabriel39\\n@gaodayue\\n@GoGoWen\\n@Gongruixiao\\n@gwdgithubnom\\n@HappenLee\\n@Henry2SS\\n@hf200012\\n@htyoung\\n@jacktengg\\n@jackwener\\n@JNSimba\\n@Keysluomo\\n@kezhenxu94\\n@killxdcj\\n@lihuigang\\n@littleeleventhwolf\\n@liutang123\\n@liuzhuang2017\\n@lonre\\n@lovingfeel\\n@luozenglin\\n@luzhijing\\n@MeiontheTop\\n@mh-boy\\n@morningman\\n@mrhhsg\\n@Myasuka\\n@nimuyuhan\\n@obobj\\n@pengxiangyu\\n@qidaye\\n@qzsee\\n@renzhimin7\\n@Royce33\\n@SleepyBear96\\n@smallhibiscus\\n@sodamnsure\\n@spaces-X\\n@sparklezzz\\n@stalary\\n@steadyBoy\\n@tarepanda1024\\n@THUMarkLau\\n@tianhui5\\n@tinkerrrr\\n@ucasfl\\n@Userwhite\\n@vinson0526\\n@wangbo\\n@wangshuo128\\n@wangyf0555\\n@weajun\\n@weizuo93\\n@whutpencil\\n@WindyGao\\n@wunan1210\\n@xiaokang\\n@xiaokangguo\\n@xiedeyantu\\n@xinghuayu007\\n@xingtanzjr\\n@xinyiZzz\\n@xtr1993\\n@xu20160924\\n@xuliuzhe\\n@xuzifu666\\n@xy720\\n@yangzhg\\n@yiguolei\\n@yinzhijian\\n@yjant\\n@zbtzbtzbt\\n@zenoyang\\n@zh0122\\n@zhangstar333\\n@zhannngchen\\n@zhengshengjun\\n@zhengshiJ\\n@ZhikaiZuo\\n@ztgoto\\n@zuochunwei\\n```"},{"id":"/release-note-0.15.0","metadata":{"permalink":"/blog/release-note-0.15.0","source":"@site/blog/release-note-0.15.0.md","title":"[Release Note] Apache Doris(Incubating) 0.15.0 Release","description":"\x3c!--","date":"2021-11-29T00:00:00.000Z","formattedDate":"November 29, 2021","tags":[{"label":"Release Notes","permalink":"/blog/tags/release-notes"}],"truncated":false,"authors":[{"name":"Apache Doris"}],"frontMatter":{"title":"[Release Note] Apache Doris(Incubating) 0.15.0 Release","summary":"[Release Note] Apache Doris(Incubating) 0.15.0 Release","date":"2021-11-29","author":"Apache Doris","tags":["Release Notes"]},"prevItem":{"title":"[Release Note] Apache Doris(Incubating) 1.0.0 Release","permalink":"/blog/release-note-1.0.0"}},"content":"\x3c!--\\nLicensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\"License\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\n  http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\"AS IS\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\\n--\x3e\\n\\n# Apache Doris(Incubating) 0.15.0 Release\\n\\nDear Community, After months of polishing, we are pleased to announce the release of Apache Doris(Incubating) on November 29, 2021! Nearly 700 optimizations and fixes have been submitted by 99 contributors to Apache Doris, and we\'d like to express our sincere gratitude to all of them!\\n\\nIn the 0.15.0 Release, we have added many new features to optimize Apache Doris\'s query performance, ease of use, and stability: a new resource division and isolation feature that allows users to divide BE nodes in a cluster into resource groups by means of resource tags, enabling unified management of online and offline services and resource isolation; the addition of Runtime Filter and Join Reorder functions have been added to significantly improve the query efficiency of multi-table Join scenarios, with a 2-10 times performance improvement under the Star Schema Benchmark test data set; new import method Binlog Load enables Doris to incrementally synchronize the CDC of data update operations in MySQL; support for String column type The new import method, Binlog Load, allows Doris to incrementally synchronize the CDC of MySQL for data update operations; supports String column type with a maximum length of 2GB; supports List partitioning to create partitions by enumerating values; supports Update statements on the Unique Key model; Spark-Doris-Connector supports data writing to Doris ... ...and many more important features, welcome to download and use.\\n\\nWe welcome you to contact us via GitHub Discussion or the Dev email group if you have any questions during use, and we look forward to your participation in community discussions and building.\\n\\n## High Lights\\n\\n### Resource Segregation and Isolation\\n\\nYou can divide BE nodes in a Doris cluster into resource groups by using resource tags, allowing you to manage online and offline operations and isolate resources at the node level.\\nYou can also control the resource overhead of individual queries by limiting the CPU and memory overhead and complexity of individual query tasks, thus reducing the resource hogging problem between different queries.\\n\\n### Performance Optimization\\n\\n* The Runtime Filter feature can significantly improve query efficiency in most Join scenarios by using the Join Key column condition of the right table in the Join algorithm to filter the data in the left table. For example, you can get 2-10 times performance improvement under Star Schema Benchmark (TPCH\'s streamlined test set).\\n\\n* The Join Reorder feature can automatically help adjust the order of joins in SQL by using a cost model to help achieve optimal join efficiency.\\nIt can be enabled via the session variable `set enable_cost_based_join_reorder=true`.\\n\\n### New features\\n\\n* Support synchronizing MySQL binlog data directly to Canal Server.\\n* Support String column type, support up to 2GB.\\n* Support List partitioning, you can create partitions for enumerated values.\\n* Support transactional Insert statement function. You can import data in bulk by begin ; insert ; insert;, ... You can import data in bulk by begin ; insert ; insert ;, ... ;.\\n* Support Update statement function on Unique Key model. You can execute Update Set where statement on Unique Key model table.\\n* Support SQL blocking list function. You can block some SQL execution by regular, hash value matching, etc.\\n* Support LDAP login authentication.\\n\\n### Extended Features\\n\\n* Support Flink-Doris-Connector.\\n* Support for DataX doriswriter plugin.\\n* Spark-Doris-Connector support for data writing to Doris.\\n\\n## Feature Optimization \\n\\n### Query\\n\\n* Support for computing all constant expressions in the SQL query planning phase using BE\'s functional computing power.\\n\\n### Import\\n\\n* Support for specifying multi-byte row separators or invisible separators when importing text format files.\\n* Supports importing compressed format files via Stream Load.\\n* Stream Load supports importing Json data in multi-line format.\\n\\n### Export\\n\\n* Support Export export function to specify where filter. Supports exporting files with multi-byte row separators. Support export to local files.\\n* Export export function supports exporting only specified columns.\\n* Supports exporting the result set to local disk via outfile statement and writing the exported marker file after exporting.\\n\\n### Ease of use\\n\\n* Dynamic partitioning function supports creating and keeping specified historical partitions, and supports automatic hot and cold data migration settings.\\n* Supports displaying queries, imported schedules and Profiles using a visual tree structure at the command line.\\n* Support to record and view Stream Load operation logs.\\n* When consuming Kafka data via Routine Load, you can specify the time point for consumption.\\n* Supports exporting Routine Load creation statements by show create routine load function.\\n* Support to start and stop all Routine Load jobs with one click by pause/resume all routine load command.\\n* Supports modifying the Broker List and Topic of Routine Load by alter routine load statement.\\n* Support create table as select function.\\n* Support modify column comments and table comments by alter table command.\\n* show tablet status to add table creation time and data update time.\\n* Support show data skew command to check the data volume distribution of a table to troubleshoot data skewing problems.\\n* Support show/clean trash command to check the disk occupation of BE file recycle bin and clear it actively.\\n* Support show view statement to show which views a table is referenced by.\\n\\n### New functions\\n\\n* `bitmap_min`, `bit_length`\\n* `yearweek`, `week`, `makedate`\\n* `percentile` exact percentile function\\n* `json_array`, `json_object`, `json_quote`\\n* Support for creating custom public keys for the `AES_ENCRYPT` and `AES_DECRYPT` functions.\\n* Support for creating function aliases to combine multiple functions by `create alias function`.\\n\\n### Other\\n\\n* Support for accessing the ES exterior of the SSL connection protocol.\\n* Support specifying the number of hotspot partitions in the dynamic partition property, which will be stored in SSD disks.\\n* Support importing Json format data via Broker Load.\\n* Supports accessing HDFS directly through libhdfs3 library for data import and export without the Broker process.\\n* select into outfile function supports exporting Parquet file format and parallel export.\\n* ODBC external table support for SQLServer. \\n\\n## \u81f4\u8c22  \\n\\nThe release of Apache Doris (incubating) 0.15.0 Release is made possible by the support of all community users. We would like to thank all the community contributors who participated in the design, development, testing, and discussion of the release, namely.\\n\\n* [@924060929](https://github.com/924060929)\\n* [@acelyc111](https://github.com/acelyc111)\\n* [@Aimiyoo](https://github.com/Aimiyoo)\\n* [@amosbird](https://github.com/amosbird)\\n* [@arthur-zhang](https://github.com/arthur-zhang)\\n* [@azurenake](https://github.com/azurenake)\\n* [@BiteTheDDDDt](https://github.com/BiteTheDDDDt)\\n* [@caiconghui](https://github.com/caiconghui)\\n* [@caneGuy](https://github.com/caneGuy)\\n* [@caoliang-web](https://github.com/caoliang-web)\\n* [@ccoffline](https://github.com/ccoffline)\\n* [@chaplinthink](https://github.com/chaplinthink)\\n* [@chovy-3012](https://github.com/chovy-3012)\\n* [@ChPi](https://github.com/ChPi)\\n* [@copperybean](https://github.com/copperybean)\\n* [@crazyleeyang](https://github.com/crazyleeyang)\\n* [@dh-cloud](https://github.com/dh-cloud)\\n* [@DinoZhang](https://github.com/DinoZhang)\\n* [@dixingxing0](https://github.com/dixingxing0)\\n* [@dohongdayi](https://github.com/dohongdayi)\\n* [@e0c9](https://github.com/e0c9)\\n* [@EmmyMiao87](https://github.com/EmmyMiao87)\\n* [@eyesmoons](https://github.com/eyesmoons)\\n* [@francisoliverlee](https://github.com/francisoliverlee)\\n* [@Gabriel39](https://github.com/Gabriel39)\\n* [@gaodayue](https://github.com/gaodayue)\\n* [@GoGoWen](https://github.com/GoGoWen)\\n* [@HappenLee](https://github.com/HappenLee)\\n* [@harveyyue](https://github.com/harveyyue)\\n* [@Henry2SS](https://github.com/Henry2SS)\\n* [@hf200012](https://github.com/hf200012)\\n* [@huangmengbin](https://github.com/huangmengbin)\\n* [@huozhanfeng](https://github.com/huozhanfeng)\\n* [@huzk8](https://github.com/huzk8)\\n* [@hxianshun](https://github.com/hxianshun)\\n* [@ikaruga4600](https://github.com/ikaruga4600)\\n* [@JameyWoo](https://github.com/JameyWoo)\\n* [@Jennifer88huang](https://github.com/Jennifer88huang)\\n* [@JinLiOnline](https://github.com/JinLiOnline)\\n* [@jinyuanlu](https://github.com/jinyuanlu)\\n* [@JNSimba](https://github.com/JNSimba)\\n* [@killxdcj](https://github.com/killxdcj)\\n* [@kuncle](https://github.com/kuncle)\\n* [@liutang123](https://github.com/liutang123)\\n* [@luozenglin](https://github.com/luozenglin)\\n* [@luzhijing](https://github.com/luzhijing)\\n* [@MarsXDM](https://github.com/MarsXDM)\\n* [@mh-boy](https://github.com/mh-boy)\\n* [@mk8310](https://github.com/mk8310)\\n* [@morningman](https://github.com/morningman)\\n* [@Myasuka](https://github.com/Myasuka)\\n* [@nimuyuhan](https://github.com/nimuyuhan)\\n* [@pan3793](https://github.com/pan3793)\\n* [@PatrickNicholas](https://github.com/PatrickNicholas)\\n* [@pengxiangyu](https://github.com/pengxiangyu)\\n* [@pierre94](https://github.com/pierre94)\\n* [@qidaye](https://github.com/qidaye)\\n* [@qzsee](https://github.com/qzsee)\\n* [@shiyi23](https://github.com/shiyi23)\\n* [@smallhibiscus](https://github.com/smallhibiscus)\\n* [@songenjie](https://github.com/songenjie)\\n* [@spaces-X](https://github.com/spaces-X)\\n* [@stalary](https://github.com/stalary)\\n* [@stdpain](https://github.com/stdpain)\\n* [@Stephen-Robin](https://github.com/Stephen-Robin)\\n* [@Sunt-ing](https://github.com/Sunt-ing)\\n* [@Taaang](https://github.com/Taaang)\\n* [@tarepanda1024](https://github.com/tarepanda1024)\\n* [@tianhui5](https://github.com/tianhui5)\\n* [@tinkerrrr](https://github.com/tinkerrrr)\\n* [@TobKed](https://github.com/TobKed)\\n* [@ucasfl](https://github.com/ucasfl)\\n* [@Userwhite](https://github.com/Userwhite)\\n* [@vinson0526](https://github.com/vinson0526)\\n* [@wangbo](https://github.com/wangbo)\\n* [@wangliansong](https://github.com/wangliansong)\\n* [@wangshuo128](https://github.com/wangshuo128)\\n* [@weajun](https://github.com/weajun)\\n* [@weihongkai2008](https://github.com/weihongkai2008)\\n* [@weizuo93](https://github.com/weizuo93)\\n* [@WindyGao](https://github.com/WindyGao)\\n* [@wunan1210](https://github.com/wunan1210)\\n* [@wuyunfeng](https://github.com/wuyunfeng)\\n* [@xhmz](https://github.com/xhmz)\\n* [@xiaokangguo](https://github.com/xiaokangguo)\\n* [@xiaoxiaopan118](https://github.com/xiaoxiaopan118)\\n* [@xinghuayu007](https://github.com/xinghuayu007)\\n* [@xinyiZzz](https://github.com/xinyiZzz)\\n* [@xuliuzhe](https://github.com/xuliuzhe)\\n* [@xxiao2018](https://github.com/xxiao2018)\\n* [@xy720](https://github.com/xy720)\\n* [@yangzhg](https://github.com/yangzhg)\\n* [@yx91490](https://github.com/yx91490)\\n* [@zbtzbtzbt](https://github.com/zbtzbtzbt)\\n* [@zenoyang](https://github.com/zenoyang)\\n* [@zh0122](https://github.com/zh0122)\\n* [@zhangboya1](https://github.com/zhangboya1)\\n* [@zhangstar333](https://github.com/zhangstar333)\\n* [@zuochunwei](https://github.com/zuochunwei)"}]}')}}]);