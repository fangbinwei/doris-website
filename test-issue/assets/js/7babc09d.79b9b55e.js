"use strict";(self.webpackChunkselectdb_portal=self.webpackChunkselectdb_portal||[]).push([[99627],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>h});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),s=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=s(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",k={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,m=p(e,["components","mdxType","originalType","parentName"]),c=s(a),d=r,h=c["".concat(l,".").concat(d)]||c[d]||k[d]||i;return a?n.createElement(h,o(o({ref:t},m),{},{components:a})):n.createElement(h,o({ref:t},m))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var p={};for(var l in t)hasOwnProperty.call(t,l)&&(p[l]=t[l]);p.originalType=e,p[c]="string"==typeof e?e:r,o[1]=p;for(var s=2;s<i;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},4082:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>p,toc:()=>s});var n=a(87462),r=(a(67294),a(3905));const i={title:"ROUTINE LOAD",language:"en"},o=void 0,p={unversionedId:"sql-reference/sql-statements/Data-Manipulation/ROUTINE-LOAD",id:"version-0.15/sql-reference/sql-statements/Data-Manipulation/ROUTINE-LOAD",title:"ROUTINE LOAD",description:"\x3c!--",source:"@site/versioned_docs/version-0.15/sql-reference/sql-statements/Data-Manipulation/ROUTINE-LOAD.md",sourceDirName:"sql-reference/sql-statements/Data-Manipulation",slug:"/sql-reference/sql-statements/Data-Manipulation/ROUTINE-LOAD",permalink:"/docs/0.15/sql-reference/sql-statements/Data-Manipulation/ROUTINE-LOAD",draft:!1,tags:[],version:"0.15",frontMatter:{title:"ROUTINE LOAD",language:"en"},sidebar:"docs",previous:{title:"RESUME SYNC JOB",permalink:"/docs/0.15/sql-reference/sql-statements/Data-Manipulation/RESUME-SYNC-JOB"},next:{title:"SHOW ALTER",permalink:"/docs/0.15/sql-reference/sql-statements/Data-Manipulation/SHOW-ALTER"}},l={},s=[{value:"description",id:"description",level:2},{value:"example",id:"example",level:2},{value:"keyword",id:"keyword",level:2}],m={toc:s};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"routine-load"},"ROUTINE LOAD"),(0,r.kt)("h2",{id:"description"},"description"),(0,r.kt)("p",null,"Routine Load function allows users to submit a resident load task, and continuously load data into Doris by continuously reading data from the specified data source. Currently, only text data format (CSV) data is loaded from Kafka by means of no authentication or SSL authentication."),(0,r.kt)("p",null,"Syntax:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE ROUTINE LOAD [db.]job_name ON tbl_name\n[merge_type]\n[load_properties]\n[job_properties]\nFROM data_source\n[data_source_properties]\n")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"[db.]","job_name"),(0,r.kt)("p",{parentName:"li"},"The name of the load job, in the same database, only one job can run with the same name.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"tbl_name"),(0,r.kt)("p",{parentName:"li"},"Specifies the name of the table that needs to be loaded.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"merge_type"),(0,r.kt)("p",{parentName:"li"},"The type of data merging supports three types: APPEND, DELETE, and MERGE. APPEND is the default value, which means that all this batch of data needs to be appended to the existing data. DELETE means to delete all rows with the same key as this batch of data. MERGE semantics Need to be used in conjunction with the delete condition, which means that the data that meets the delete on condition is processed according to DELETE semantics and the rest is processed according to APPEND semantics")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"load_properties"),(0,r.kt)("p",{parentName:"li"},"Used to describe the load data. grammar:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"[column_separator],\n[columns_mapping],\n[where_predicates],\n[delete_on_predicates]\n[partitions],\n[preceding_predicates]\n")),(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"column_separator:"),(0,r.kt)("p",{parentName:"li"},"Specify column separators, such as:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},'COLUMNS TERMINATED BY ","')),(0,r.kt)("p",{parentName:"li"},"The default is: ",(0,r.kt)("inlineCode",{parentName:"p"},"\\t"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"columns_mapping:"),(0,r.kt)("p",{parentName:"li"},"Specifies the mapping of columns in the source data and defines how the derived columns are generated."),(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Map column:"),(0,r.kt)("p",{parentName:"li"},"Specify in order, which columns in the source data correspond to which columns in the destination table. For columns that you want to skip, you can specify a column name that does not exist."),(0,r.kt)("p",{parentName:"li"},"Suppose the destination table has three columns k1, k2, v1. The source data has 4 columns, of which columns 1, 2, and 4 correspond to k2, k1, and v1, respectively. Write as follows:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"COLUMNS (k2, k1, xxx, v1)")),(0,r.kt)("p",{parentName:"li"},"Where xxx is a column that does not exist and is used to skip the third column in the source data.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Derived columns:"),(0,r.kt)("p",{parentName:"li"},"A column represented in the form of col_name = expr, which we call a derived column. That is, the value of the corresponding column in the destination table is calculated by expr."),(0,r.kt)("p",{parentName:"li"},"Derived columns are usually arranged after the mapped column. Although this is not mandatory, Doris always parses the mapped columns first and then parses the derived columns."),(0,r.kt)("p",{parentName:"li"},"Following an example, assume that the destination table also has column 4, v2, which is generated by the sum of k1 and k2. You can write as follows:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"COLUMNS (k2, k1, xxx, v1, v2 = k1 + k2);"))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"where_predicates"),(0,r.kt)("p",{parentName:"li"},"Used to specify filter criteria to filter out unwanted columns. Filter columns can be either mapped columns or derived columns."),(0,r.kt)("p",{parentName:"li"},"For example, if we only want to load a column with k1 greater than 100 and k2 equal to 1000, we would write as follows:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"WHERE k1 > 100 and k2 = 1000"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"partitions"),(0,r.kt)("p",{parentName:"li"},"Specifies which partitions of the load destination table. If not specified, it will be automatically loaded into the corresponding partition."),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"PARTITION(p1, p2, p3)"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"delete_on_predicates:"),(0,r.kt)("p",{parentName:"li"},"Only used when merge type is MERGE")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"preceding_predicates"),(0,r.kt)("p",{parentName:"li"},"Used to filter original data. The original data is the data without column mapping and transformation. The user can filter the data before conversion, select the desired data, and then perform the conversion.")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"job_properties"),(0,r.kt)("p",{parentName:"li"},"A generic parameter that specifies a routine load job."),(0,r.kt)("p",{parentName:"li"},"syntax:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'PROPERTIES (\n    "key1" = "val1",\n    "key2" = "val2"\n)\n')),(0,r.kt)("p",{parentName:"li"},"Currently we support the following parameters:"),(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"desired_concurrent_number")),(0,r.kt)("p",{parentName:"li"},"The degree of concurrency desired. A routine load job is split into multiple subtasks. This parameter specifies how many tasks can be executed simultaneously in a job. Must be greater than 0. The default is 3."),(0,r.kt)("p",{parentName:"li"},"This concurrency is not the actual concurrency. The actual concurrency will be considered by the number of nodes in the cluster, the load, and the data source."),(0,r.kt)("p",{parentName:"li"},"example:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},'"desired_concurrent_number" = "3"'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"max_batch_interval/max_batch_rows/max_batch_size")),(0,r.kt)("p",{parentName:"li"},"These three parameters represent:"),(0,r.kt)("p",{parentName:"li"},"1) The maximum execution time of each subtask, in seconds. The range is 5 to 60. The default is 10."),(0,r.kt)("p",{parentName:"li"},"2) The maximum number of rows read per subtask. Must be greater than or equal to 200,000. The default is 200000."),(0,r.kt)("p",{parentName:"li"},"3) The maximum number of bytes read per subtask. The unit is byte and the range is 100MB to 1GB. The default is 100MB."),(0,r.kt)("p",{parentName:"li"},"These three parameters are used to control the execution time and throughput of a subtask. When either one reaches the threshold, the task ends."),(0,r.kt)("p",{parentName:"li"},"example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'"max_batch_interval" = "20",\n"max_batch_rows" = "300000",\n"max_batch_size" = "209715200"\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"max_error_number")),(0,r.kt)("p",{parentName:"li"},"The maximum number of error lines allowed in the sampling window. Must be greater than or equal to 0. The default is 0, which means that no error lines are allowed."),(0,r.kt)("p",{parentName:"li"},"The sampling window is max_batch_rows * 10. That is, if the number of error lines is greater than max_error_number in the sampling window, the routine job will be suspended, and manual intervention is required to check the data quality problem."),(0,r.kt)("p",{parentName:"li"},"Lines that are filtered by the where condition are not counted as error lines.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"strict_mode")),(0,r.kt)("p",{parentName:"li"},'Whether to enable strict mode, the default is disabled. If turned on, the column type transformation of non-null raw data is filtered if the result is NULL. Specified as "strict_mode" = "true"')),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"timezone")),(0,r.kt)("p",{parentName:"li"},"Specifies the time zone in which the job will be loaded. The default by using session variable's timezone. This parameter affects all function results related to the time zone involved in the load.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"format")),(0,r.kt)("p",{parentName:"li"},"Specifies the format of the imported data. Support csv and json, the default is csv.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"jsonpaths")),(0,r.kt)("p",{parentName:"li"},"There are two ways to import json: simple mode and matched mode. If jsonpath is set, it will be the matched mode import, otherwise it will be the simple mode import, please refer to the example for details.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"strip_outer_array"),"\nBoolean type, true to indicate that json data starts with an array object and flattens objects in the array object, default value is false.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"json_root"),'\njson_root is a valid JSONPATH string that specifies the root node of the JSON Document. The default value is "".')),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"send_batch_parallelism"),"\nInteger, Used to set the default parallelism for sending batch, if the value for parallelism exceed ",(0,r.kt)("inlineCode",{parentName:"p"},"max_send_batch_parallelism_per_job")," in BE config, then the coordinator BE will use the value of ",(0,r.kt)("inlineCode",{parentName:"p"},"max_send_batch_parallelism_per_job"),".")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"data_source"),(0,r.kt)("p",{parentName:"li"},"The type of data source. Current support:"),(0,r.kt)("p",{parentName:"li"},"KAFKA")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"data_source_properties")),(0,r.kt)("p",{parentName:"li"},"Specify information about the data source."),(0,r.kt)("p",{parentName:"li"},"syntax:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'(\n    "key1" = "val1",\n    "key2" = "val2"\n)\n')),(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"KAFKA data source"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"Kafka_broker_list")),(0,r.kt)("p",{parentName:"li"},"Kafka's broker connection information. The format is ip:host. Multiple brokers are separated by commas."),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},'"kafka_broker_list" = "broker1:9092,broker2:9092"'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"kafka_topic")),(0,r.kt)("p",{parentName:"li"},"Specify the topic of Kafka to subscribe to."),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},'"kafka_topic" = "my_topic"'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"kafka_partitions/kafka_offsets")),(0,r.kt)("p",{parentName:"li"},"Specify the kafka partition to be subscribed to, and the corresponding star offset for each partition."),(0,r.kt)("p",{parentName:"li"},"Offset can specify a specific offset from 0 or greater, or:"),(0,r.kt)("p",{parentName:"li"},"1) OFFSET_BEGINNING: Subscribe from the location where the data is available."),(0,r.kt)("p",{parentName:"li"},"2) OFFSET_END: Subscribe from the end."),(0,r.kt)("p",{parentName:"li"},'3) Timestamp, the format must be like: "2021-05-11 10:00:00", the system will automatically locate the offset of the first message greater than or equal to the timestamp.\nNote that the offset of the timestamp format cannot be mixed with the number type, only one of them can be selected.'),(0,r.kt)("p",{parentName:"li"},"If not specified, all partitions under topic are subscribed by default fromSET_END."),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'"kafka_partitions" = "0,1,2,3",\n"kafka_offsets" = "101,0,OFFSET_BEGINNING,OFFSET_END"\n\n"kafka_partitions" = "0,1",\n"kafka_offsets" = "2021-05-11 10:00:00, 2021-05-11 11:00:00"\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"property"),(0,r.kt)("p",{parentName:"li"},"Specify custom kafka parameters."),(0,r.kt)("p",{parentName:"li"},'The function is equivalent to the "--property" parameter in the kafka shel'),(0,r.kt)("p",{parentName:"li"},'When the value of the parameter is a file, you need to add the keyword: "FILE" before the value.'),(0,r.kt)("p",{parentName:"li"},'For information on how to create a file, see "HELP CREATE FILE;"'),(0,r.kt)("p",{parentName:"li"},"For more supported custom parameters, see the configuration items on the nt side in the official CONFIGURATION documentation for librdkafka."),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'"property.client.id" = "12345",\n"property.ssl.ca.location" = "FILE:ca.pem"\n')),(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"When connecting to Kafka using SSL, you need to specify the following parameters:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'"property.security.protocol" = "ssl",\n"property.ssl.ca.location" = "FILE:ca.pem",\n"property.ssl.certificate.location" = "FILE:client.pem",\n"property.ssl.key.location" = "FILE:client.key",\n"property.ssl.key.password" = "abcdefg"\n')),(0,r.kt)("p",{parentName:"li"},"among them:"),(0,r.kt)("p",{parentName:"li"},'"property.security.protocol" and "property.ssl.ca.location" are required to indicate the connection method is SSL and the location of the CA certificate.'),(0,r.kt)("p",{parentName:"li"},"If the client authentication is enabled on the Kafka server, you also need to set:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'"property.ssl.certificate.location"\n"property.ssl.key.location"\n"property.ssl.key.password"\n')),(0,r.kt)("p",{parentName:"li"},"Used to specify the public key of the client, the private key, and the word of the private key.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Specify the default starting offset for kafka partition"),(0,r.kt)("p",{parentName:"li"},"If kafka_partitions/kafka_offsets is not specified, all partitions are unanmed by default, and you can specify kafka_default_offsets to specify the star offset. The default is OFFSET_END, which starts at the end of the subscription."),(0,r.kt)("p",{parentName:"li"},"Values:"),(0,r.kt)("p",{parentName:"li"},"1) OFFSET_BEGINNING: Subscribe from the location where the data is available."),(0,r.kt)("p",{parentName:"li"},"2) OFFSET_END: Subscribe from the end."),(0,r.kt)("p",{parentName:"li"},"3) Timestamp, the format is the same as kafka_offsets"),(0,r.kt)("p",{parentName:"li"},"Example:"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},'"property.kafka_default_offsets" = "OFFSET_BEGINNING"'),"\n",(0,r.kt)("inlineCode",{parentName:"p"},'"property.kafka_default_offsets" = "2021-05-11 10:00:00"'))))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"load data format sample"),(0,r.kt)("p",{parentName:"li"},"Integer class (TINYINT/SMALLINT/INT/BIGINT/LARGEINT): 1, 1000, 1234"),(0,r.kt)("p",{parentName:"li"},"Floating point class (FLOAT/DOUBLE/DECIMAL): 1.1, 0.23, .356"),(0,r.kt)("p",{parentName:"li"},"Date class (DATE/DATETIME): 2017-10-03, 2017-06-13 12:34:03."),(0,r.kt)("p",{parentName:"li"},"String class (CHAR/VARCHAR) (without quotes): I am a student, a"),(0,r.kt)("p",{parentName:"li"},"NULL value: \\N"))),(0,r.kt)("h2",{id:"example"},"example"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a Kafka routine load task named test1 for the example_tbl of example_db. Specify group.id and client.id, and automatically consume all partitions by default, with subscriptions starting at the end (OFFSET_END)"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100)\nPROPERTIES\n(\n    "desired_concurrent_number"="3",\n    "max_batch_interval" = "20",\n    "max_batch_rows" = "300000",\n    "max_batch_size" = "209715200",\n    "strict_mode" = "false"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "property.group.id" = "xxx",\n    "property.client.id" = "xxx"\n);\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a Kafka routine load task named test1 for the example_tbl of example_db. The load task is in strict mode."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100),\nWHERE k1 > 100 and k2 like "%doris%"\nPROPERTIES\n(\n\xa0\xa0\xa0\xa0"desired_concurrent_number"="3",\n\xa0\xa0\xa0\xa0"max_batch_interval" = "20",\n\xa0\xa0\xa0\xa0"max_batch_rows" = "300000",\n\xa0\xa0\xa0\xa0"max_batch_size" = "209715200",\n\xa0\xa0\xa0\xa0"strict_mode" = "false"\n)\nFROM KAFKA\n(\n\xa0\xa0\xa0\xa0"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n\xa0\xa0\xa0\xa0"kafka_topic" = "my_topic",\n\xa0\xa0\xa0\xa0"kafka_partitions" = "0,1,2,3",\n\xa0\xa0\xa0\xa0"kafka_offsets" = "101,0,0,200"\n);\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"load data from Kafka clusters via SSL authentication. Also set the client.id parameter. The load task is in non-strict mode and the time zone is Africa/Abidjan"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(k1, k2, k3, v1, v2, v3 = k1 * 100),\nWHERE k1 > 100 and k2 like "%doris%"\nPROPERTIES\n(\n\xa0\xa0\xa0\xa0"desired_concurrent_number"="3",\n\xa0\xa0\xa0\xa0"max_batch_interval" = "20",\n\xa0\xa0\xa0\xa0"max_batch_rows" = "300000",\n\xa0\xa0\xa0\xa0"max_batch_size" = "209715200",\n\xa0\xa0\xa0\xa0"strict_mode" = "false",\n\xa0\xa0\xa0\xa0"timezone" = "Africa/Abidjan"\n)\nFROM KAFKA\n(\n\xa0\xa0\xa0\xa0"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n\xa0\xa0\xa0\xa0"kafka_topic" = "my_topic",\n\xa0\xa0\xa0\xa0"property.security.protocol" = "ssl",\n\xa0\xa0\xa0\xa0"property.ssl.ca.location" = "FILE:ca.pem",\n\xa0\xa0\xa0\xa0"property.ssl.certificate.location" = "FILE:client.pem",\n\xa0\xa0\xa0\xa0"property.ssl.key.location" = "FILE:client.key",\n\xa0\xa0\xa0\xa0"property.ssl.key.password" = "abcdefg",\n\xa0\xa0\xa0\xa0"property.client.id" = "my_client_id"\n);\n'))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a Kafka routine load task named test1 for the example_tbl of example_db. The load data is a simple json."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'  CREATE ROUTINE LOAD example_db.test_json_label_1 ON table1\n  COLUMNS(category,price,author)\n  PROPERTIES\n    (\n    "desired_concurrent_number"="3",\n    "max_batch_interval" = "20",\n    "max_batch_rows" = "300000",\n    "max_batch_size" = "209715200",\n    "strict_mode" = "false",\n    "format" = "json"\n    )\n    FROM KAFKA\n    (\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "kafka_partitions" = "0,1,2",\n    "kafka_offsets" = "0,0,0"\n    );\n')),(0,r.kt)("p",{parentName:"li"},'  It support two kinds data style:\n1\uff09{"category":"a9jadhx","author":"test","price":895}\n2\uff09','[\n{"category":"a9jadhx","author":"test","price":895},\n{"category":"axdfa1","author":"EvelynWaugh","price":1299}\n]')),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Matched load json by jsonpaths."),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},'CREATE TABLE `example_tbl` (\n  `category` varchar(24) NULL COMMENT "",\n  `author` varchar(24) NULL COMMENT "",\n  `timestamp` bigint(20) NULL COMMENT "",\n  `dt` int(11) NULL COMMENT "",\n  `price` double REPLACE\n) ENGINE=OLAP\nAGGREGATE KEY(`category`,`author`,`timestamp`,`dt`)\nCOMMENT "OLAP"\nPARTITION BY RANGE(`dt`)\n(PARTITION p0 VALUES [("-2147483648"), ("20200509")),\nPARTITION p20200509 VALUES [("20200509"), ("20200510")),\nPARTITION p20200510 VALUES [("20200510"), ("20200511")),\nPARTITION p20200511 VALUES [("20200511"), ("20200512")))\nDISTRIBUTED BY HASH(`category`,`author`,`timestamp`) BUCKETS 4\nPROPERTIES (\n"replication_num" = "1"\n);\n\nCREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(category, author, price, timestamp, dt=from_unixtime(timestamp, \'%Y%m%d\'))\nPROPERTIES\n(\n    "desired_concurrent_number"="3",\n    "max_batch_interval" = "20",\n    "max_batch_rows" = "300000",\n    "max_batch_size" = "209715200",\n    "strict_mode" = "false",\n    "format" = "json",\n    "jsonpaths" = "[\\"$.category\\",\\"$.author\\",\\"$.price\\",\\"$.timestamp\\"]",\n    "strip_outer_array" = "true"\n)\nFROM KAFKA\n(\n    "kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n    "kafka_topic" = "my_topic",\n    "kafka_partitions" = "0,1,2",\n    "kafka_offsets" = "0,0,0"\n);\n')),(0,r.kt)("p",{parentName:"li"}," For example json data:\n",'[\n{"category":"11","title":"SayingsoftheCentury","price":895,"timestamp":1589191587},\n{"category":"22","author":"2avc","price":895,"timestamp":1589191487},\n{"category":"33","author":"3avc","title":"SayingsoftheCentury","timestamp":1589191387}\n]'),(0,r.kt)("p",{parentName:"li"},"Tips:\n1\uff09If the json data starts as an array and each object in the array is a record, you need to set the strip_outer_array to true to represent the flat array.\n2\uff09If the json data starts with an array, and each object in the array is a record, our ROOT node is actually an object in the array when we set jsonpath.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},'User specifies the json_root node\nCREATE ROUTINE LOAD example_db.test1 ON example_tbl\nCOLUMNS(category, author, price, timestamp, dt=from_unixtime(timestamp, \'%Y%m%d\'))\nPROPERTIES\n(\n"desired_concurrent_number"="3",\n"max_batch_interval" = "20",\n"max_batch_rows" = "300000",\n"max_batch_size" = "209715200",\n"strict_mode" = "false",\n"format" = "json",\n"jsonpaths" = "','[\\"$.category\\",\\"$.author\\",\\"$.price\\",\\"$.timestamp\\"]','",\n"strip_outer_array" = "true",\n"json_root" = "$.RECORDS"\n)\nFROM KAFKA\n(\n"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n"kafka_topic" = "my_topic",\n"kafka_partitions" = "0,1,2",\n"kafka_offsets" = "0,0,0"\n);\nFor example json data:\n{\n"RECORDS":','[\n{"category":"11","title":"SayingsoftheCentury","price":895,"timestamp":1589191587},\n{"category":"22","author":"2avc","price":895,"timestamp":1589191487},\n{"category":"33","author":"3avc","title":"SayingsoftheCentury","timestamp":1589191387}\n]',"\n}"),(0,r.kt)("ol",{parentName:"li",start:7},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create a Kafka routine load task named test1 for the example_tbl of example_db. delete all data key columns match v3 >100 key columns."),(0,r.kt)("p",{parentName:"li"},' CREATE ROUTINE LOAD example_db.test1 ON example_tbl\nWITH MERGE\nCOLUMNS(k1, k2, k3, v1, v2, v3),\nWHERE k1 > 100 and k2 like "%doris%",\nDELETE ON v3 >100\nPROPERTIES\n(\n"desired_concurrent_number"="3",\n"max_batch_interval" = "20",\n"max_batch_rows" = "300000",\n"max_batch_size" = "209715200",\n"strict_mode" = "false"\n)\nFROM KAFKA\n(\n"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n"kafka_topic" = "my_topic",\n"kafka_partitions" = "0,1,2,3",\n"kafka_offsets" = "101,0,0,200"\n);')),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Filter original data"),(0,r.kt)("p",{parentName:"li"},' CREATE ROUTINE LOAD example_db.test_job ON example_tbl\nCOLUMNS TERMINATED BY ",",\nCOLUMNS(k1,k2,source_sequence,v1,v2),\nPRECEDING FILTER k1 > 2\nPROPERTIES\n(\n"desired_concurrent_number"="3",\n"max_batch_interval" = "30",\n"max_batch_rows" = "300000",\n"max_batch_size" = "209715200"\n) FROM KAFKA\n(\n"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n"kafka_topic" = "my_topic",\n"kafka_partitions" = "0,1,2,3",\n"kafka_offsets" = "101,0,0,200"\n);')),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Start consumption from the specified point in time"),(0,r.kt)("p",{parentName:"li"},'CREATE ROUTINE LOAD example_db.test_job ON example_tbl\nPROPERTIES\n(\n"desired_concurrent_number"="3",\n"max_batch_interval" = "30",\n"max_batch_rows" = "300000",\n"max_batch_size" = "209715200"\n) FROM KAFKA\n(\n"kafka_broker_list" = "broker1:9092,broker2:9092,broker3:9092",\n"kafka_topic" = "my_topic",\n"property.kafka_default_offsets" = "2021-10-10 11:00:00"\n);'))))),(0,r.kt)("h2",{id:"keyword"},"keyword"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE, ROUTINE, LOAD\n")))}c.isMDXComponent=!0}}]);